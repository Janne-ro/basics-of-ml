{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6chybAVFJW2"
      },
      "source": [
        "# Different architectures on MNIST classification\n",
        "\n",
        "This notebook is based on the exercies provided in [Understanding Deep Learning (UDL)]((https://udlbook.github.io/udlbook/)) by David Prince. Specifically exercises 8.1 - MNIST-1D performance and  10.2 - Convolution for MNIST-1D. In these exercises the task was to first use a FNN and then CNN to classify MNIST-1D data and evalaute them using cross-entropy loss and %error. In accordance to the exercises in the book I also discussed my results.\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifVjS4cTOqKz",
        "outputId": "13b404a5-6615-4027-f303-bb9229ba1f9e"
      },
      "outputs": [],
      "source": [
        "# Install MNIST 1D repository\n",
        "#%pip install git+https://github.com/greydanus/mnist1d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qyE7G1StPIqO"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import matplotlib.pyplot as plt\n",
        "import mnist1d\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fully connected, feedforward neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLxf7dJfPaqw",
        "outputId": "a5c047ef-b790-4dee-8f18-147a836b9aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded data from ./mnist1d_data.pkl\n",
            "Examples in training set: 4000\n",
            "Examples in test set: 1000\n",
            "Length of each example: 40\n"
          ]
        }
      ],
      "source": [
        "args = mnist1d.data.get_dataset_args()\n",
        "data = mnist1d.data.get_dataset(args, path='./mnist1d_data.pkl', download=False, regenerate=False)\n",
        "\n",
        "# The training and test input and outputs are in\n",
        "# data['x'], data['y'], data['x_test'], and data['y_test']\n",
        "print(\"Examples in training set: {}\".format(len(data['y'])))\n",
        "print(\"Examples in test set: {}\".format(len(data['y_test'])))\n",
        "print(\"Length of each example: {}\".format(data['x'].shape[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxaB5vc0uevl",
        "outputId": "38e29ffd-6d74-43ae-9b42-049cd773f9ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=40, out_features=285, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=285, out_features=135, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=135, out_features=60, bias=True)\n",
              "  (5): ReLU()\n",
              "  (6): Linear(in_features=60, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "D_i = 40    # Input dimensions\n",
        "D_1 = 285   # Hidden dimensions\n",
        "D_2 = 135\n",
        "D_3 = 60\n",
        "D_o = 10    # Output dimensions\n",
        "\n",
        "model_f = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_i, D_1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(D_1, D_2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(D_2, D_3),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(D_3, D_o))\n",
        "\n",
        "def weights_init(layer_in):\n",
        "  if isinstance(layer_in, nn.Linear):\n",
        "    nn.init.kaiming_normal_(layer_in.weight)\n",
        "    layer_in.bias.data.fill_(0.0)\n",
        "\n",
        "model_f\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original Hyperparameters\n",
        "Momentum = 0.9, Learning Rate = 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rX6N3VyyQTY",
        "outputId": "943892ac-b6e0-481c-ef7e-006da4426e36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch     0, train loss 1.659192, train error 66.07,  test loss 1.712127, test error 68.10\n",
            "Epoch     1, train loss 1.388585, train error 52.22,  test loss 1.506275, test error 63.20\n",
            "Epoch     2, train loss 1.184083, train error 43.78,  test loss 1.347137, test error 53.50\n",
            "Epoch     3, train loss 0.993600, train error 35.60,  test loss 1.262166, test error 48.50\n",
            "Epoch     4, train loss 0.924714, train error 34.95,  test loss 1.272286, test error 51.20\n",
            "Epoch     5, train loss 0.679277, train error 24.35,  test loss 1.084958, test error 41.20\n",
            "Epoch     6, train loss 0.582480, train error 20.18,  test loss 1.101449, test error 40.10\n",
            "Epoch     7, train loss 0.474950, train error 16.18,  test loss 1.100022, test error 39.00\n",
            "Epoch     8, train loss 0.437255, train error 15.78,  test loss 1.171029, test error 40.80\n",
            "Epoch     9, train loss 0.415848, train error 14.43,  test loss 1.208716, test error 37.80\n",
            "Epoch    10, train loss 0.217658, train error 6.43,  test loss 1.065547, test error 34.20\n",
            "Epoch    11, train loss 0.146553, train error 2.93,  test loss 1.078868, test error 34.40\n",
            "Epoch    12, train loss 0.109650, train error 1.93,  test loss 1.091914, test error 34.00\n",
            "Epoch    13, train loss 0.090691, train error 1.20,  test loss 1.187310, test error 34.60\n",
            "Epoch    14, train loss 0.063860, train error 0.53,  test loss 1.176001, test error 33.80\n",
            "Epoch    15, train loss 0.049917, train error 0.32,  test loss 1.266546, test error 34.30\n",
            "Epoch    16, train loss 0.041016, train error 0.22,  test loss 1.308155, test error 34.20\n",
            "Epoch    17, train loss 0.030851, train error 0.07,  test loss 1.370854, test error 34.60\n",
            "Epoch    18, train loss 0.027203, train error 0.00,  test loss 1.391220, test error 35.70\n",
            "Epoch    19, train loss 0.019651, train error 0.00,  test loss 1.426376, test error 34.00\n",
            "Epoch    20, train loss 0.016775, train error 0.00,  test loss 1.449075, test error 34.60\n",
            "Epoch    21, train loss 0.015407, train error 0.00,  test loss 1.457219, test error 34.40\n",
            "Epoch    22, train loss 0.014294, train error 0.00,  test loss 1.472160, test error 34.40\n",
            "Epoch    23, train loss 0.013356, train error 0.00,  test loss 1.485232, test error 34.50\n",
            "Epoch    24, train loss 0.012570, train error 0.00,  test loss 1.499613, test error 34.70\n",
            "Epoch    25, train loss 0.011925, train error 0.00,  test loss 1.509276, test error 34.40\n",
            "Epoch    26, train loss 0.011268, train error 0.00,  test loss 1.522517, test error 34.40\n",
            "Epoch    27, train loss 0.010683, train error 0.00,  test loss 1.546621, test error 34.90\n",
            "Epoch    28, train loss 0.010100, train error 0.00,  test loss 1.547952, test error 34.70\n",
            "Epoch    29, train loss 0.009590, train error 0.00,  test loss 1.561045, test error 34.70\n",
            "Epoch    30, train loss 0.009330, train error 0.00,  test loss 1.564726, test error 34.80\n",
            "Epoch    31, train loss 0.009115, train error 0.00,  test loss 1.569735, test error 34.80\n",
            "Epoch    32, train loss 0.008936, train error 0.00,  test loss 1.578334, test error 35.00\n",
            "Epoch    33, train loss 0.008713, train error 0.00,  test loss 1.581437, test error 34.90\n",
            "Epoch    34, train loss 0.008549, train error 0.00,  test loss 1.586729, test error 35.00\n",
            "Epoch    35, train loss 0.008348, train error 0.00,  test loss 1.590828, test error 34.90\n",
            "Epoch    36, train loss 0.008182, train error 0.00,  test loss 1.592650, test error 34.70\n",
            "Epoch    37, train loss 0.008030, train error 0.00,  test loss 1.598512, test error 35.10\n",
            "Epoch    38, train loss 0.007856, train error 0.00,  test loss 1.605092, test error 34.90\n",
            "Epoch    39, train loss 0.007700, train error 0.00,  test loss 1.608600, test error 35.00\n",
            "Epoch    40, train loss 0.007621, train error 0.00,  test loss 1.611736, test error 35.10\n",
            "Epoch    41, train loss 0.007547, train error 0.00,  test loss 1.613959, test error 35.00\n",
            "Epoch    42, train loss 0.007475, train error 0.00,  test loss 1.616311, test error 35.20\n",
            "Epoch    43, train loss 0.007403, train error 0.00,  test loss 1.619252, test error 35.10\n",
            "Epoch    44, train loss 0.007334, train error 0.00,  test loss 1.620487, test error 35.20\n",
            "Epoch    45, train loss 0.007265, train error 0.00,  test loss 1.623167, test error 35.10\n",
            "Epoch    46, train loss 0.007200, train error 0.00,  test loss 1.625251, test error 34.90\n",
            "Epoch    47, train loss 0.007134, train error 0.00,  test loss 1.626642, test error 35.20\n",
            "Epoch    48, train loss 0.007067, train error 0.00,  test loss 1.629693, test error 35.10\n",
            "Epoch    49, train loss 0.007005, train error 0.00,  test loss 1.630158, test error 35.10\n",
            "Epoch    50, train loss 0.006971, train error 0.00,  test loss 1.632181, test error 35.10\n",
            "Epoch    51, train loss 0.006938, train error 0.00,  test loss 1.633210, test error 35.00\n",
            "Epoch    52, train loss 0.006908, train error 0.00,  test loss 1.634328, test error 35.00\n",
            "Epoch    53, train loss 0.006877, train error 0.00,  test loss 1.635468, test error 35.10\n",
            "Epoch    54, train loss 0.006846, train error 0.00,  test loss 1.636537, test error 35.10\n",
            "Epoch    55, train loss 0.006816, train error 0.00,  test loss 1.637355, test error 35.10\n",
            "Epoch    56, train loss 0.006786, train error 0.00,  test loss 1.638558, test error 35.10\n",
            "Epoch    57, train loss 0.006756, train error 0.00,  test loss 1.639425, test error 35.10\n",
            "Epoch    58, train loss 0.006727, train error 0.00,  test loss 1.640611, test error 35.00\n",
            "Epoch    59, train loss 0.006697, train error 0.00,  test loss 1.641377, test error 35.10\n",
            "Epoch    60, train loss 0.006682, train error 0.00,  test loss 1.642233, test error 35.10\n",
            "Epoch    61, train loss 0.006668, train error 0.00,  test loss 1.642313, test error 35.10\n",
            "Epoch    62, train loss 0.006653, train error 0.00,  test loss 1.642699, test error 35.10\n",
            "Epoch    63, train loss 0.006639, train error 0.00,  test loss 1.643314, test error 35.10\n",
            "Epoch    64, train loss 0.006625, train error 0.00,  test loss 1.644006, test error 35.10\n",
            "Epoch    65, train loss 0.006610, train error 0.00,  test loss 1.644408, test error 35.10\n",
            "Epoch    66, train loss 0.006596, train error 0.00,  test loss 1.645026, test error 35.10\n",
            "Epoch    67, train loss 0.006582, train error 0.00,  test loss 1.645531, test error 35.10\n",
            "Epoch    68, train loss 0.006568, train error 0.00,  test loss 1.645873, test error 35.10\n",
            "Epoch    69, train loss 0.006554, train error 0.00,  test loss 1.646382, test error 35.10\n",
            "Epoch    70, train loss 0.006547, train error 0.00,  test loss 1.646696, test error 35.10\n",
            "Epoch    71, train loss 0.006540, train error 0.00,  test loss 1.646841, test error 35.10\n",
            "Epoch    72, train loss 0.006533, train error 0.00,  test loss 1.647119, test error 35.10\n",
            "Epoch    73, train loss 0.006526, train error 0.00,  test loss 1.647321, test error 35.10\n",
            "Epoch    74, train loss 0.006519, train error 0.00,  test loss 1.647519, test error 35.10\n",
            "Epoch    75, train loss 0.006512, train error 0.00,  test loss 1.647818, test error 35.10\n",
            "Epoch    76, train loss 0.006505, train error 0.00,  test loss 1.648004, test error 35.10\n",
            "Epoch    77, train loss 0.006498, train error 0.00,  test loss 1.648285, test error 35.10\n",
            "Epoch    78, train loss 0.006491, train error 0.00,  test loss 1.648570, test error 35.10\n",
            "Epoch    79, train loss 0.006484, train error 0.00,  test loss 1.648828, test error 35.10\n",
            "Epoch    80, train loss 0.006481, train error 0.00,  test loss 1.648946, test error 35.10\n",
            "Epoch    81, train loss 0.006477, train error 0.00,  test loss 1.649068, test error 35.10\n",
            "Epoch    82, train loss 0.006474, train error 0.00,  test loss 1.649178, test error 35.10\n",
            "Epoch    83, train loss 0.006471, train error 0.00,  test loss 1.649299, test error 35.10\n",
            "Epoch    84, train loss 0.006467, train error 0.00,  test loss 1.649469, test error 35.10\n",
            "Epoch    85, train loss 0.006464, train error 0.00,  test loss 1.649573, test error 35.10\n",
            "Epoch    86, train loss 0.006460, train error 0.00,  test loss 1.649752, test error 35.10\n",
            "Epoch    87, train loss 0.006457, train error 0.00,  test loss 1.649775, test error 35.10\n",
            "Epoch    88, train loss 0.006454, train error 0.00,  test loss 1.649914, test error 35.10\n",
            "Epoch    89, train loss 0.006450, train error 0.00,  test loss 1.650056, test error 35.10\n",
            "Epoch    90, train loss 0.006448, train error 0.00,  test loss 1.650124, test error 35.10\n",
            "Epoch    91, train loss 0.006447, train error 0.00,  test loss 1.650210, test error 35.10\n",
            "Epoch    92, train loss 0.006445, train error 0.00,  test loss 1.650248, test error 35.10\n",
            "Epoch    93, train loss 0.006443, train error 0.00,  test loss 1.650335, test error 35.10\n",
            "Epoch    94, train loss 0.006442, train error 0.00,  test loss 1.650370, test error 35.10\n",
            "Epoch    95, train loss 0.006440, train error 0.00,  test loss 1.650443, test error 35.10\n",
            "Epoch    96, train loss 0.006438, train error 0.00,  test loss 1.650487, test error 35.10\n",
            "Epoch    97, train loss 0.006437, train error 0.00,  test loss 1.650551, test error 35.10\n",
            "Epoch    98, train loss 0.006435, train error 0.00,  test loss 1.650631, test error 35.10\n",
            "Epoch    99, train loss 0.006433, train error 0.00,  test loss 1.650702, test error 35.10\n",
            "Epoch   100, train loss 0.006432, train error 0.00,  test loss 1.650720, test error 35.10\n",
            "Epoch   101, train loss 0.006431, train error 0.00,  test loss 1.650748, test error 35.10\n",
            "Epoch   102, train loss 0.006431, train error 0.00,  test loss 1.650783, test error 35.10\n",
            "Epoch   103, train loss 0.006430, train error 0.00,  test loss 1.650810, test error 35.10\n",
            "Epoch   104, train loss 0.006429, train error 0.00,  test loss 1.650839, test error 35.10\n",
            "Epoch   105, train loss 0.006428, train error 0.00,  test loss 1.650878, test error 35.10\n",
            "Epoch   106, train loss 0.006427, train error 0.00,  test loss 1.650874, test error 35.10\n",
            "Epoch   107, train loss 0.006426, train error 0.00,  test loss 1.650936, test error 35.10\n",
            "Epoch   108, train loss 0.006426, train error 0.00,  test loss 1.650958, test error 35.10\n",
            "Epoch   109, train loss 0.006425, train error 0.00,  test loss 1.650980, test error 35.10\n",
            "Epoch   110, train loss 0.006424, train error 0.00,  test loss 1.651001, test error 35.10\n",
            "Epoch   111, train loss 0.006424, train error 0.00,  test loss 1.651020, test error 35.10\n",
            "Epoch   112, train loss 0.006423, train error 0.00,  test loss 1.651033, test error 35.10\n",
            "Epoch   113, train loss 0.006423, train error 0.00,  test loss 1.651048, test error 35.10\n",
            "Epoch   114, train loss 0.006423, train error 0.00,  test loss 1.651063, test error 35.10\n",
            "Epoch   115, train loss 0.006422, train error 0.00,  test loss 1.651087, test error 35.10\n",
            "Epoch   116, train loss 0.006422, train error 0.00,  test loss 1.651095, test error 35.10\n",
            "Epoch   117, train loss 0.006421, train error 0.00,  test loss 1.651114, test error 35.10\n",
            "Epoch   118, train loss 0.006421, train error 0.00,  test loss 1.651123, test error 35.10\n",
            "Epoch   119, train loss 0.006421, train error 0.00,  test loss 1.651138, test error 35.10\n",
            "Epoch   120, train loss 0.006420, train error 0.00,  test loss 1.651141, test error 35.10\n",
            "Epoch   121, train loss 0.006420, train error 0.00,  test loss 1.651156, test error 35.10\n",
            "Epoch   122, train loss 0.006420, train error 0.00,  test loss 1.651163, test error 35.10\n",
            "Epoch   123, train loss 0.006420, train error 0.00,  test loss 1.651172, test error 35.10\n",
            "Epoch   124, train loss 0.006420, train error 0.00,  test loss 1.651176, test error 35.10\n",
            "Epoch   125, train loss 0.006419, train error 0.00,  test loss 1.651187, test error 35.10\n",
            "Epoch   126, train loss 0.006419, train error 0.00,  test loss 1.651194, test error 35.10\n",
            "Epoch   127, train loss 0.006419, train error 0.00,  test loss 1.651199, test error 35.10\n",
            "Epoch   128, train loss 0.006419, train error 0.00,  test loss 1.651210, test error 35.10\n",
            "Epoch   129, train loss 0.006419, train error 0.00,  test loss 1.651212, test error 35.10\n",
            "Epoch   130, train loss 0.006419, train error 0.00,  test loss 1.651216, test error 35.10\n",
            "Epoch   131, train loss 0.006418, train error 0.00,  test loss 1.651219, test error 35.10\n",
            "Epoch   132, train loss 0.006418, train error 0.00,  test loss 1.651224, test error 35.10\n",
            "Epoch   133, train loss 0.006418, train error 0.00,  test loss 1.651226, test error 35.10\n",
            "Epoch   134, train loss 0.006418, train error 0.00,  test loss 1.651228, test error 35.10\n",
            "Epoch   135, train loss 0.006418, train error 0.00,  test loss 1.651231, test error 35.10\n",
            "Epoch   136, train loss 0.006418, train error 0.00,  test loss 1.651235, test error 35.10\n",
            "Epoch   137, train loss 0.006418, train error 0.00,  test loss 1.651236, test error 35.10\n",
            "Epoch   138, train loss 0.006418, train error 0.00,  test loss 1.651240, test error 35.10\n",
            "Epoch   139, train loss 0.006418, train error 0.00,  test loss 1.651245, test error 35.10\n",
            "Epoch   140, train loss 0.006418, train error 0.00,  test loss 1.651246, test error 35.10\n",
            "Epoch   141, train loss 0.006418, train error 0.00,  test loss 1.651246, test error 35.10\n",
            "Epoch   142, train loss 0.006418, train error 0.00,  test loss 1.651246, test error 35.10\n",
            "Epoch   143, train loss 0.006418, train error 0.00,  test loss 1.651247, test error 35.10\n",
            "Epoch   144, train loss 0.006418, train error 0.00,  test loss 1.651248, test error 35.10\n",
            "Epoch   145, train loss 0.006418, train error 0.00,  test loss 1.651249, test error 35.10\n",
            "Epoch   146, train loss 0.006418, train error 0.00,  test loss 1.651249, test error 35.10\n",
            "Epoch   147, train loss 0.006418, train error 0.00,  test loss 1.651250, test error 35.10\n",
            "Epoch   148, train loss 0.006418, train error 0.00,  test loss 1.651251, test error 35.10\n",
            "Epoch   149, train loss 0.006418, train error 0.00,  test loss 1.651251, test error 35.10\n",
            "Epoch   150, train loss 0.006418, train error 0.00,  test loss 1.651251, test error 35.10\n",
            "Epoch   151, train loss 0.006418, train error 0.00,  test loss 1.651252, test error 35.10\n",
            "Epoch   152, train loss 0.006418, train error 0.00,  test loss 1.651252, test error 35.10\n",
            "Epoch   153, train loss 0.006418, train error 0.00,  test loss 1.651252, test error 35.10\n",
            "Epoch   154, train loss 0.006418, train error 0.00,  test loss 1.651252, test error 35.10\n",
            "Epoch   155, train loss 0.006418, train error 0.00,  test loss 1.651252, test error 35.10\n",
            "Epoch   156, train loss 0.006418, train error 0.00,  test loss 1.651252, test error 35.10\n",
            "Epoch   157, train loss 0.006418, train error 0.00,  test loss 1.651252, test error 35.10\n",
            "Epoch   158, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   159, train loss 0.006418, train error 0.00,  test loss 1.651252, test error 35.10\n",
            "Epoch   160, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   161, train loss 0.006418, train error 0.00,  test loss 1.651252, test error 35.10\n",
            "Epoch   162, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   163, train loss 0.006418, train error 0.00,  test loss 1.651252, test error 35.10\n",
            "Epoch   164, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   165, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   166, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   167, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   168, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   169, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   170, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   171, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   172, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   173, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   174, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   175, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   176, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   177, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   178, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   179, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   180, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   181, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   182, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   183, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   184, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   185, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   186, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   187, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   188, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   189, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   190, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   191, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   192, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   193, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   194, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   195, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   196, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   197, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   198, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   199, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   200, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   201, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   202, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   203, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   204, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   205, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   206, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   207, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   208, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   209, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   210, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   211, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   212, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   213, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   214, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   215, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   216, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   217, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   218, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   219, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   220, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   221, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   222, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   223, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   224, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   225, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   226, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   227, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   228, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   229, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   230, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   231, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   232, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   233, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   234, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   235, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   236, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   237, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   238, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   239, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   240, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   241, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   242, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   243, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   244, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   245, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   246, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   247, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   248, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   249, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   250, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   251, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   252, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   253, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   254, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   255, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   256, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   257, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   258, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   259, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   260, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   261, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   262, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   263, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   264, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   265, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   266, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   267, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   268, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   269, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   270, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   271, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   272, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   273, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   274, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   275, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   276, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   277, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   278, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   279, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   280, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   281, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   282, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   283, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   284, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   285, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   286, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   287, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   288, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   289, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   290, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   291, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   292, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   293, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   294, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   295, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   296, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   297, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   298, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   299, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   300, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   301, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   302, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   303, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   304, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   305, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   306, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   307, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   308, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   309, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   310, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   311, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   312, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   313, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   314, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   315, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   316, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   317, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   318, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   319, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   320, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   321, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   322, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   323, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   324, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   325, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   326, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   327, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   328, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   329, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   330, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   331, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   332, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   333, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   334, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   335, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   336, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   337, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   338, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   339, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   340, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   341, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   342, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   343, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   344, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   345, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   346, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   347, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   348, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   349, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   350, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   351, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   352, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   353, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   354, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   355, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   356, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   357, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   358, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   359, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   360, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   361, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   362, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   363, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   364, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   365, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   366, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   367, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   368, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   369, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   370, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   371, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   372, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   373, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   374, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   375, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   376, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   377, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   378, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   379, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   380, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   381, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   382, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   383, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   384, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   385, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   386, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   387, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   388, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   389, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   390, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   391, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   392, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   393, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   394, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   395, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   396, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   397, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   398, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   399, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   400, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   401, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   402, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   403, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   404, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   405, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   406, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   407, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   408, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   409, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   410, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   411, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   412, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   413, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   414, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   415, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   416, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   417, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   418, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   419, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   420, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   421, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   422, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   423, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   424, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   425, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   426, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   427, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   428, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   429, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   430, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   431, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   432, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   433, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   434, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   435, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   436, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   437, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   438, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   439, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   440, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   441, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   442, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   443, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   444, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   445, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   446, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   447, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   448, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   449, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   450, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   451, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   452, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   453, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   454, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   455, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   456, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   457, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   458, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   459, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   460, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   461, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   462, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   463, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   464, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   465, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   466, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   467, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   468, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   469, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   470, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   471, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   472, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   473, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   474, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   475, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   476, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   477, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   478, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   479, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   480, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   481, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   482, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   483, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   484, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   485, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   486, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   487, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   488, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   489, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   490, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   491, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   492, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   493, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   494, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   495, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   496, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   497, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   498, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   499, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   500, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   501, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   502, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   503, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   504, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   505, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   506, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   507, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   508, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   509, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   510, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   511, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   512, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   513, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   514, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   515, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   516, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   517, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   518, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   519, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   520, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   521, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   522, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   523, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   524, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   525, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   526, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   527, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   528, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   529, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   530, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   531, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   532, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   533, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   534, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   535, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   536, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   537, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   538, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   539, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   540, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   541, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   542, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   543, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   544, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   545, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   546, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   547, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   548, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   549, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   550, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   551, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   552, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   553, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   554, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   555, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   556, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   557, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   558, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   559, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   560, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   561, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   562, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   563, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   564, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   565, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   566, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   567, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   568, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   569, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   570, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   571, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   572, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   573, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   574, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   575, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   576, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   577, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   578, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   579, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   580, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   581, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   582, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   583, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   584, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   585, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   586, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   587, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   588, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   589, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   590, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   591, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   592, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   593, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   594, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   595, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   596, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   597, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   598, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   599, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   600, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   601, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   602, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   603, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   604, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   605, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   606, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   607, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   608, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   609, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   610, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   611, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   612, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   613, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   614, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   615, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   616, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   617, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   618, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   619, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   620, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   621, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   622, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   623, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   624, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   625, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   626, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   627, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   628, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   629, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   630, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   631, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   632, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   633, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   634, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   635, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   636, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   637, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   638, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   639, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   640, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   641, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   642, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   643, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   644, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   645, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   646, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   647, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   648, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   649, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   650, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   651, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   652, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   653, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   654, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   655, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   656, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   657, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   658, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   659, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   660, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   661, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   662, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   663, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   664, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   665, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   666, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   667, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   668, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   669, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   670, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   671, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   672, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   673, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   674, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   675, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   676, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   677, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   678, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   679, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   680, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   681, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   682, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   683, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   684, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   685, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   686, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   687, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   688, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   689, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   690, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   691, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   692, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   693, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   694, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   695, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   696, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   697, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   698, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   699, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   700, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   701, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   702, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   703, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   704, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   705, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   706, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   707, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   708, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   709, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   710, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   711, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   712, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   713, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   714, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   715, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   716, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   717, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   718, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   719, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   720, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   721, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   722, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   723, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   724, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   725, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   726, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   727, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   728, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   729, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   730, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   731, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   732, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   733, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   734, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   735, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   736, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   737, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   738, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   739, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   740, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   741, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   742, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   743, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   744, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   745, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   746, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   747, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   748, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   749, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   750, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   751, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   752, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   753, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   754, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   755, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   756, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   757, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   758, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   759, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   760, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   761, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   762, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   763, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   764, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   765, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   766, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   767, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   768, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   769, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   770, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   771, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   772, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   773, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   774, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   775, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   776, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   777, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   778, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   779, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   780, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   781, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   782, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   783, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   784, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   785, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   786, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   787, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   788, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   789, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   790, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   791, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   792, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   793, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   794, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   795, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   796, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   797, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   798, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   799, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   800, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   801, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   802, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   803, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   804, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   805, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   806, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   807, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   808, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   809, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   810, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   811, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   812, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   813, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   814, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   815, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   816, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   817, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   818, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   819, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   820, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   821, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   822, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   823, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   824, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   825, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   826, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   827, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   828, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   829, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   830, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   831, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   832, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   833, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   834, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   835, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   836, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   837, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   838, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   839, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   840, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   841, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   842, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   843, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   844, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   845, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   846, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   847, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   848, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   849, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   850, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   851, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   852, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   853, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   854, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   855, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   856, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   857, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   858, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   859, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   860, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   861, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   862, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   863, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   864, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   865, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   866, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   867, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   868, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   869, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   870, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   871, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   872, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   873, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   874, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   875, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   876, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   877, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   878, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   879, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   880, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   881, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   882, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   883, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   884, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   885, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   886, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   887, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   888, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   889, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   890, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   891, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   892, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   893, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   894, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   895, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   896, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   897, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   898, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   899, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   900, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   901, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   902, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   903, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   904, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   905, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   906, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   907, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   908, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   909, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   910, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   911, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   912, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   913, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   914, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   915, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   916, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   917, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   918, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   919, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   920, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   921, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   922, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   923, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   924, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   925, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   926, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   927, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   928, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   929, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   930, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   931, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   932, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   933, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   934, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   935, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   936, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   937, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   938, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   939, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   940, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   941, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   942, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   943, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   944, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   945, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   946, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   947, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   948, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   949, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   950, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   951, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   952, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   953, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   954, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   955, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   956, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   957, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   958, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   959, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   960, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   961, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   962, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   963, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   964, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   965, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   966, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   967, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   968, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   969, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   970, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   971, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   972, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   973, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   974, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   975, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   976, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   977, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   978, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   979, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   980, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   981, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   982, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   983, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   984, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   985, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   986, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   987, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   988, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   989, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   990, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   991, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   992, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   993, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   994, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   995, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   996, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   997, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   998, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch   999, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1000, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1001, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1002, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1003, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1004, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1005, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1006, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1007, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1008, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1009, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1010, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1011, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1012, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1013, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1014, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1015, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1016, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1017, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1018, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1019, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1020, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1021, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1022, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1023, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1024, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1025, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1026, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1027, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1028, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1029, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1030, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1031, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1032, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1033, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1034, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1035, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1036, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1037, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1038, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1039, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1040, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1041, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1042, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1043, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1044, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1045, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1046, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1047, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1048, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1049, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1050, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1051, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1052, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1053, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1054, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1055, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1056, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1057, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1058, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1059, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1060, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1061, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1062, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1063, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1064, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1065, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1066, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1067, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1068, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1069, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1070, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1071, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1072, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1073, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1074, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1075, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1076, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1077, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1078, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1079, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1080, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1081, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1082, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1083, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1084, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1085, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1086, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1087, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1088, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1089, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1090, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1091, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1092, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1093, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1094, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1095, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1096, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1097, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1098, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1099, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1100, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1101, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1102, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1103, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1104, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1105, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1106, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1107, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1108, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1109, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1110, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1111, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1112, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1113, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1114, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1115, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1116, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1117, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1118, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1119, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1120, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1121, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1122, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1123, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1124, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1125, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1126, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1127, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1128, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1129, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1130, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1131, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1132, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1133, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1134, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1135, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1136, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1137, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1138, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1139, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1140, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1141, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1142, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1143, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1144, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1145, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1146, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1147, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1148, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1149, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1150, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1151, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1152, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1153, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1154, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1155, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1156, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1157, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1158, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1159, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1160, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1161, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1162, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1163, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1164, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1165, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1166, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1167, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1168, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1169, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1170, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1171, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1172, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1173, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1174, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1175, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1176, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1177, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1178, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1179, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1180, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1181, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1182, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1183, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1184, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1185, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1186, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1187, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1188, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1189, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1190, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1191, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1192, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1193, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1194, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1195, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1196, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1197, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1198, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1199, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1200, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1201, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1202, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1203, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1204, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1205, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1206, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1207, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1208, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1209, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1210, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1211, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1212, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1213, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1214, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1215, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1216, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1217, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1218, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1219, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1220, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1221, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1222, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1223, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1224, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1225, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1226, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1227, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1228, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1229, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1230, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1231, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1232, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1233, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1234, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1235, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1236, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1237, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1238, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1239, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1240, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1241, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1242, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1243, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1244, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1245, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1246, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1247, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1248, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1249, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1250, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1251, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1252, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1253, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1254, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1255, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1256, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1257, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1258, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1259, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1260, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1261, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1262, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1263, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1264, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1265, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1266, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1267, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1268, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1269, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1270, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1271, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1272, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1273, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1274, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1275, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1276, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1277, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1278, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1279, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1280, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1281, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1282, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1283, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1284, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1285, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1286, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1287, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1288, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1289, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1290, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1291, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1292, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1293, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1294, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1295, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1296, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1297, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1298, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1299, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1300, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1301, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1302, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1303, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1304, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1305, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1306, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1307, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1308, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1309, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1310, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1311, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1312, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1313, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1314, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1315, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1316, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1317, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1318, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1319, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1320, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1321, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1322, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1323, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1324, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1325, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1326, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1327, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1328, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1329, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1330, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1331, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1332, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1333, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1334, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1335, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1336, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1337, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1338, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1339, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1340, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1341, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1342, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1343, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1344, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1345, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1346, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1347, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1348, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1349, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1350, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1351, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1352, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1353, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1354, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1355, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1356, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1357, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1358, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1359, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1360, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1361, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1362, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1363, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1364, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1365, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1366, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1367, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1368, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1369, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1370, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1371, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1372, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1373, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1374, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1375, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1376, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1377, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1378, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1379, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1380, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1381, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1382, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1383, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1384, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1385, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1386, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1387, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1388, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1389, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1390, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1391, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1392, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1393, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1394, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1395, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1396, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1397, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1398, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1399, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1400, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1401, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1402, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1403, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1404, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1405, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1406, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1407, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1408, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1409, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1410, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1411, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1412, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1413, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1414, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1415, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1416, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1417, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1418, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1419, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1420, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1421, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1422, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1423, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1424, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1425, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1426, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1427, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1428, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1429, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1430, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1431, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1432, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1433, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1434, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1435, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1436, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1437, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1438, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1439, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1440, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1441, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1442, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1443, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1444, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1445, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1446, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1447, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1448, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1449, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1450, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1451, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1452, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1453, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1454, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1455, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1456, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1457, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1458, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1459, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1460, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1461, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1462, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1463, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1464, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1465, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1466, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1467, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1468, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1469, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1470, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1471, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1472, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1473, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1474, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1475, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1476, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1477, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1478, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1479, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1480, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1481, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1482, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1483, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1484, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1485, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1486, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1487, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1488, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1489, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1490, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1491, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1492, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1493, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1494, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1495, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1496, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1497, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1498, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1499, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1500, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1501, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1502, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1503, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1504, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1505, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1506, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1507, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1508, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1509, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1510, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1511, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1512, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1513, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1514, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1515, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1516, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1517, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1518, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1519, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1520, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1521, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1522, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1523, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1524, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1525, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1526, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1527, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1528, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1529, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1530, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1531, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1532, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1533, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1534, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1535, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1536, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1537, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1538, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1539, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1540, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1541, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1542, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1543, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1544, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1545, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1546, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1547, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1548, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1549, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1550, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1551, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1552, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1553, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1554, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1555, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1556, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1557, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1558, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1559, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1560, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1561, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1562, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1563, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1564, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1565, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1566, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1567, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1568, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1569, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1570, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1571, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1572, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1573, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1574, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1575, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1576, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1577, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1578, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1579, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1580, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1581, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1582, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1583, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1584, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1585, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1586, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1587, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1588, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1589, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1590, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1591, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1592, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1593, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1594, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1595, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1596, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1597, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1598, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1599, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1600, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1601, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1602, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1603, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1604, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1605, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1606, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1607, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1608, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1609, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1610, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1611, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1612, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1613, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1614, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1615, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1616, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1617, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1618, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1619, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1620, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1621, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1622, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1623, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1624, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1625, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1626, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1627, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1628, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1629, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1630, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1631, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1632, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1633, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1634, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1635, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1636, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1637, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1638, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1639, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1640, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1641, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1642, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1643, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1644, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1645, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1646, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1647, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1648, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1649, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1650, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1651, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1652, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1653, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1654, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1655, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1656, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1657, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1658, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1659, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1660, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1661, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1662, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1663, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1664, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1665, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1666, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1667, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1668, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1669, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1670, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1671, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1672, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1673, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1674, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1675, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1676, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1677, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1678, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1679, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1680, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1681, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1682, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1683, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1684, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1685, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1686, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1687, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1688, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1689, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1690, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1691, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1692, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1693, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1694, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1695, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1696, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1697, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1698, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1699, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1700, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1701, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1702, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1703, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1704, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1705, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1706, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1707, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1708, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1709, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1710, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1711, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1712, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1713, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1714, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1715, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1716, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1717, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1718, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1719, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1720, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1721, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1722, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1723, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1724, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1725, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1726, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1727, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1728, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1729, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1730, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1731, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1732, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1733, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1734, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1735, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1736, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1737, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1738, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1739, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1740, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1741, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1742, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1743, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1744, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1745, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1746, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1747, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1748, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1749, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1750, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1751, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1752, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1753, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1754, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1755, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1756, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1757, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1758, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1759, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1760, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1761, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1762, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1763, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1764, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1765, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1766, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1767, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1768, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1769, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1770, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1771, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1772, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1773, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1774, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1775, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1776, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1777, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1778, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1779, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1780, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1781, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1782, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1783, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1784, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1785, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1786, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1787, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1788, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1789, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1790, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1791, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1792, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1793, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1794, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1795, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1796, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1797, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1798, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1799, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1800, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1801, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1802, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1803, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1804, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1805, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1806, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1807, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1808, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1809, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1810, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1811, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1812, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1813, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1814, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1815, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1816, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1817, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1818, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1819, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1820, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1821, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1822, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1823, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1824, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1825, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1826, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1827, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1828, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1829, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1830, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1831, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1832, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1833, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1834, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1835, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1836, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1837, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1838, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1839, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1840, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1841, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1842, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1843, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1844, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1845, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1846, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1847, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1848, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1849, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1850, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1851, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1852, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1853, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1854, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1855, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1856, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1857, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1858, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1859, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1860, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1861, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1862, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1863, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1864, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1865, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1866, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1867, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1868, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1869, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1870, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1871, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1872, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1873, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1874, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1875, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1876, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1877, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1878, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1879, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1880, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1881, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1882, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1883, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1884, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1885, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1886, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1887, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1888, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1889, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1890, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1891, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1892, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1893, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1894, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1895, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1896, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1897, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1898, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1899, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1900, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1901, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1902, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1903, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1904, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1905, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1906, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1907, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1908, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1909, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1910, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1911, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1912, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1913, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1914, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1915, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1916, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1917, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1918, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1919, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1920, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1921, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1922, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1923, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1924, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1925, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1926, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1927, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1928, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1929, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1930, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1931, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1932, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1933, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1934, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1935, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1936, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1937, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1938, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1939, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1940, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1941, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1942, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1943, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1944, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1945, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1946, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1947, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1948, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1949, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1950, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1951, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1952, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1953, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1954, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1955, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1956, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1957, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1958, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1959, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1960, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1961, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1962, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1963, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1964, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1965, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1966, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1967, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1968, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1969, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1970, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1971, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1972, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1973, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1974, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1975, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1976, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1977, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1978, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1979, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1980, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1981, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1982, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1983, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1984, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1985, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1986, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1987, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1988, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1989, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1990, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1991, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1992, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1993, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1994, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1995, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1996, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1997, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1998, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  1999, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2000, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2001, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2002, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2003, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2004, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2005, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2006, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2007, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2008, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2009, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2010, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2011, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2012, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2013, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2014, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2015, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2016, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2017, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2018, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2019, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2020, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2021, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2022, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2023, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2024, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2025, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2026, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2027, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2028, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2029, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2030, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2031, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2032, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2033, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2034, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2035, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2036, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2037, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2038, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2039, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2040, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2041, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2042, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2043, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2044, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2045, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2046, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2047, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2048, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2049, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2050, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2051, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2052, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2053, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2054, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2055, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2056, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2057, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2058, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2059, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2060, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2061, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2062, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2063, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2064, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2065, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2066, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2067, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2068, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2069, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2070, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2071, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2072, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2073, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2074, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2075, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2076, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2077, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2078, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2079, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2080, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2081, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2082, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2083, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2084, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2085, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2086, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2087, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2088, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2089, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2090, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2091, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2092, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2093, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2094, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2095, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2096, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2097, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2098, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2099, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2100, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2101, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2102, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2103, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2104, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2105, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2106, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2107, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2108, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2109, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2110, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2111, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2112, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2113, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2114, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2115, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2116, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2117, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2118, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2119, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2120, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2121, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2122, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2123, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2124, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2125, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2126, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2127, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2128, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2129, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2130, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2131, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2132, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2133, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2134, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2135, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2136, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2137, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2138, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2139, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2140, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2141, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2142, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2143, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2144, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2145, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2146, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2147, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2148, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2149, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2150, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2151, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2152, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2153, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2154, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2155, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2156, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2157, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2158, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2159, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2160, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2161, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2162, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2163, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2164, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2165, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2166, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2167, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2168, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2169, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2170, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2171, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2172, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2173, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2174, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2175, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2176, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2177, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2178, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2179, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2180, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2181, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2182, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2183, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2184, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2185, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2186, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2187, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2188, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2189, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2190, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2191, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2192, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2193, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2194, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2195, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2196, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2197, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2198, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2199, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2200, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2201, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2202, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2203, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2204, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2205, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2206, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2207, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2208, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2209, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2210, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2211, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2212, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2213, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2214, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2215, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2216, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2217, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2218, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2219, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2220, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2221, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2222, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2223, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2224, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2225, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2226, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2227, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2228, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2229, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2230, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2231, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2232, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2233, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2234, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2235, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2236, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2237, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2238, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2239, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2240, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2241, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2242, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2243, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2244, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2245, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2246, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2247, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2248, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2249, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2250, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2251, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2252, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2253, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2254, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2255, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2256, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2257, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2258, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2259, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2260, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2261, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2262, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2263, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2264, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2265, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2266, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2267, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2268, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2269, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2270, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2271, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2272, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2273, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2274, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2275, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2276, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2277, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2278, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2279, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2280, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2281, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2282, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2283, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2284, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2285, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2286, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2287, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2288, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2289, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2290, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2291, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2292, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2293, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2294, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2295, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2296, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2297, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2298, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2299, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2300, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2301, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2302, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2303, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2304, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2305, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2306, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2307, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2308, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2309, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2310, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2311, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2312, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2313, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2314, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2315, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2316, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2317, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2318, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2319, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2320, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2321, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2322, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2323, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2324, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2325, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2326, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2327, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2328, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2329, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2330, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2331, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2332, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2333, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2334, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2335, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2336, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2337, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2338, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2339, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2340, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2341, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2342, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2343, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2344, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2345, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2346, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2347, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2348, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2349, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2350, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2351, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2352, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2353, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2354, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2355, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2356, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2357, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2358, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2359, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2360, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2361, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2362, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2363, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2364, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2365, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2366, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2367, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2368, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2369, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2370, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2371, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2372, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2373, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2374, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2375, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2376, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2377, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2378, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2379, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2380, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2381, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2382, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2383, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2384, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2385, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2386, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2387, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2388, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2389, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2390, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2391, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2392, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2393, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2394, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2395, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2396, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2397, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2398, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2399, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2400, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2401, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2402, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2403, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2404, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2405, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2406, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2407, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2408, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2409, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2410, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2411, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2412, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2413, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2414, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2415, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2416, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2417, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2418, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2419, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2420, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2421, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2422, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2423, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2424, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2425, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2426, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2427, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2428, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2429, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2430, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2431, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2432, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2433, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2434, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2435, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2436, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2437, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2438, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2439, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2440, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2441, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2442, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2443, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2444, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2445, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2446, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2447, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2448, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2449, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2450, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2451, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2452, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2453, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2454, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2455, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2456, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2457, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2458, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2459, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2460, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2461, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2462, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2463, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2464, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2465, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2466, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2467, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2468, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2469, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2470, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2471, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2472, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2473, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2474, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2475, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2476, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2477, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2478, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2479, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2480, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2481, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2482, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2483, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2484, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2485, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2486, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2487, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2488, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2489, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2490, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2491, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2492, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2493, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2494, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2495, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2496, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2497, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2498, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n",
            "Epoch  2499, train loss 0.006418, train error 0.00,  test loss 1.651253, test error 35.10\n"
          ]
        }
      ],
      "source": [
        "model = copy.deepcopy(model_f)\n",
        "\n",
        "# choose cross entropy loss function (equation 5.24)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "# construct SGD optimizer and initialize learning rate and momentum\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
        "# object that decreases learning rate by half every 10 epochs\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "x_train_f = torch.tensor(data['x'].astype('float32'))\n",
        "y_train_f = torch.tensor(data['y'].transpose().astype('int64'))\n",
        "x_test_f= torch.tensor(data['x_test'].astype('float32'))\n",
        "y_test_f = torch.tensor(data['y_test'].astype('int64'))\n",
        "\n",
        "# load the data into a class that creates the batches\n",
        "data_loader = DataLoader(TensorDataset(x_train_f,y_train_f), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
        "\n",
        "# Initialize model weights\n",
        "model.apply(weights_init)\n",
        "\n",
        "# loop over the dataset n_epoch times\n",
        "# steps per epoch = total examples / batch size = 4000 / 100 = 40\n",
        "# number of epochs = total steps / steps per epoch = 100000 / 40 = 2500\n",
        "n_epoch = 2500\n",
        "# store the loss and the % correct at each epoch\n",
        "losses_train_f = np.zeros((n_epoch))\n",
        "errors_train_f = np.zeros((n_epoch))\n",
        "losses_test_f = np.zeros((n_epoch))\n",
        "errors_test_f = np.zeros((n_epoch))\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # loop over batches\n",
        "  for i, batch in enumerate(data_loader):\n",
        "    # retrieve inputs and labels for this batch\n",
        "    x_batch, y_batch = batch\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass -- calculate model output\n",
        "    pred = model(x_batch)\n",
        "    # compute the loss\n",
        "    loss = loss_function(pred, y_batch)\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    # SGD update\n",
        "    optimizer.step()\n",
        "\n",
        "  # Run whole dataset to get statistics -- normally wouldn't do this\n",
        "  pred_train = model(x_train_f)\n",
        "  pred_test = model(x_test_f)\n",
        "  _, predicted_train_class = torch.max(pred_train.data, 1)\n",
        "  _, predicted_test_class = torch.max(pred_test.data, 1)\n",
        "  errors_train_f[epoch] = 100 - 100 * (predicted_train_class == y_train_f).float().sum() / len(y_train_f)\n",
        "  errors_test_f[epoch]= 100 - 100 * (predicted_test_class == y_test_f).float().sum() / len(y_test_f)\n",
        "  losses_train_f[epoch] = loss_function(pred_train, y_train_f).item()\n",
        "  losses_test_f[epoch]= loss_function(pred_test, y_test_f).item()\n",
        "  print(f'Epoch {epoch:5d}, train loss {losses_train_f[epoch]:.6f}, train error {errors_train_f[epoch]:3.2f},  test loss {losses_test_f[epoch]:.6f}, test error {errors_test_f[epoch]:3.2f}')\n",
        "\n",
        "  # tell scheduler to consider updating learning rate\n",
        "  scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min Train Error 0.00, Min Test Error 33.80\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min Train Loss 0.01, Min Test Loss 1.07\n"
          ]
        }
      ],
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(errors_train_f,'r-',label='train')\n",
        "ax.plot(errors_test_f,'b-',label='test')\n",
        "ax.set_ylim(0,100); ax.set_xlim(0,n_epoch)\n",
        "ax.set_xlabel('Epoch'); ax.set_ylabel('Error')\n",
        "ax.set_title('Train Error %3.2f, Test Error %3.2f'%(errors_train_f[-1],errors_test_f[-1]))\n",
        "ax.legend()\n",
        "plt.show()\n",
        "print(f'Min Train Error %3.2f, Min Test Error %3.2f'%(min(errors_train_f),min(errors_test_f)))\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(losses_train_f,'r-',label='train')\n",
        "ax.plot(losses_test_f,'b-',label='test')\n",
        "ax.set_xlim(0,n_epoch)\n",
        "ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
        "ax.set_title('Train Loss %3.2f, Test Loss %3.2f'%(losses_train_f[-1],losses_test_f[-1]))\n",
        "ax.legend()\n",
        "plt.show()\n",
        "print(f'Min Train Loss %3.2f, Min Test Loss %3.2f'%(min(losses_train_f),min(losses_test_f)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I ran the scheduler for 2500 epochs, however the model converged somewhere between 20 and 50 epochs. Therefore from this point I will only run the model for 100 epochs and 50 epochs for hyperparameter tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch     0, train loss 1.655176, train error 65.28,  test loss 1.750293, test error 70.10\n",
            "Epoch     1, train loss 1.408231, train error 57.08,  test loss 1.548821, test error 63.00\n",
            "Epoch     2, train loss 1.144810, train error 43.53,  test loss 1.402209, test error 53.20\n",
            "Epoch     3, train loss 0.915599, train error 32.50,  test loss 1.218288, test error 48.10\n",
            "Epoch     4, train loss 0.841359, train error 31.43,  test loss 1.218439, test error 47.00\n",
            "Epoch     5, train loss 0.681563, train error 25.93,  test loss 1.217005, test error 47.10\n",
            "Epoch     6, train loss 0.570250, train error 20.12,  test loss 1.101154, test error 40.20\n",
            "Epoch     7, train loss 0.467799, train error 15.72,  test loss 1.110009, test error 41.20\n",
            "Epoch     8, train loss 0.405158, train error 13.28,  test loss 1.066874, test error 39.20\n",
            "Epoch     9, train loss 0.322806, train error 11.68,  test loss 1.252633, test error 40.30\n",
            "Epoch    10, train loss 0.163379, train error 4.22,  test loss 1.091150, test error 35.00\n",
            "Epoch    11, train loss 0.118290, train error 2.30,  test loss 1.102304, test error 34.60\n",
            "Epoch    12, train loss 0.095102, train error 1.65,  test loss 1.161841, test error 34.20\n",
            "Epoch    13, train loss 0.069087, train error 0.85,  test loss 1.204925, test error 35.60\n",
            "Epoch    14, train loss 0.052483, train error 0.43,  test loss 1.254964, test error 36.20\n",
            "Epoch    15, train loss 0.037750, train error 0.12,  test loss 1.276838, test error 33.50\n",
            "Epoch    16, train loss 0.027649, train error 0.05,  test loss 1.330667, test error 34.00\n",
            "Epoch    17, train loss 0.024342, train error 0.03,  test loss 1.347977, test error 34.30\n",
            "Epoch    18, train loss 0.018070, train error 0.00,  test loss 1.385627, test error 34.00\n",
            "Epoch    19, train loss 0.015145, train error 0.00,  test loss 1.422249, test error 34.40\n",
            "Epoch    20, train loss 0.013691, train error 0.00,  test loss 1.416372, test error 33.20\n",
            "Epoch    21, train loss 0.012786, train error 0.00,  test loss 1.439738, test error 33.70\n",
            "Epoch    22, train loss 0.011948, train error 0.00,  test loss 1.447026, test error 33.80\n",
            "Epoch    23, train loss 0.011314, train error 0.00,  test loss 1.457658, test error 33.60\n",
            "Epoch    24, train loss 0.010654, train error 0.00,  test loss 1.471034, test error 33.70\n",
            "Epoch    25, train loss 0.010116, train error 0.00,  test loss 1.482073, test error 33.80\n",
            "Epoch    26, train loss 0.009570, train error 0.00,  test loss 1.492770, test error 33.30\n",
            "Epoch    27, train loss 0.009133, train error 0.00,  test loss 1.499414, test error 33.40\n",
            "Epoch    28, train loss 0.008745, train error 0.00,  test loss 1.508469, test error 33.20\n",
            "Epoch    29, train loss 0.008374, train error 0.00,  test loss 1.520501, test error 33.50\n",
            "Epoch    30, train loss 0.008131, train error 0.00,  test loss 1.523472, test error 33.20\n",
            "Epoch    31, train loss 0.007948, train error 0.00,  test loss 1.529989, test error 33.10\n",
            "Epoch    32, train loss 0.007784, train error 0.00,  test loss 1.534298, test error 33.30\n",
            "Epoch    33, train loss 0.007624, train error 0.00,  test loss 1.537038, test error 33.30\n",
            "Epoch    34, train loss 0.007476, train error 0.00,  test loss 1.541904, test error 33.40\n",
            "Epoch    35, train loss 0.007327, train error 0.00,  test loss 1.547104, test error 33.50\n",
            "Epoch    36, train loss 0.007189, train error 0.00,  test loss 1.549841, test error 33.50\n",
            "Epoch    37, train loss 0.007049, train error 0.00,  test loss 1.554684, test error 33.60\n",
            "Epoch    38, train loss 0.006917, train error 0.00,  test loss 1.558648, test error 33.60\n",
            "Epoch    39, train loss 0.006789, train error 0.00,  test loss 1.562724, test error 33.60\n",
            "Epoch    40, train loss 0.006724, train error 0.00,  test loss 1.563665, test error 33.40\n",
            "Epoch    41, train loss 0.006662, train error 0.00,  test loss 1.565473, test error 33.30\n",
            "Epoch    42, train loss 0.006603, train error 0.00,  test loss 1.568403, test error 33.40\n",
            "Epoch    43, train loss 0.006546, train error 0.00,  test loss 1.569942, test error 33.40\n",
            "Epoch    44, train loss 0.006488, train error 0.00,  test loss 1.572542, test error 33.40\n",
            "Epoch    45, train loss 0.006432, train error 0.00,  test loss 1.573780, test error 33.40\n",
            "Epoch    46, train loss 0.006374, train error 0.00,  test loss 1.574878, test error 33.40\n",
            "Epoch    47, train loss 0.006318, train error 0.00,  test loss 1.577774, test error 33.40\n",
            "Epoch    48, train loss 0.006264, train error 0.00,  test loss 1.579197, test error 33.40\n",
            "Epoch    49, train loss 0.006212, train error 0.00,  test loss 1.580672, test error 33.40\n",
            "Epoch    50, train loss 0.006183, train error 0.00,  test loss 1.581971, test error 33.40\n",
            "Epoch    51, train loss 0.006157, train error 0.00,  test loss 1.582822, test error 33.40\n",
            "Epoch    52, train loss 0.006131, train error 0.00,  test loss 1.583409, test error 33.40\n",
            "Epoch    53, train loss 0.006105, train error 0.00,  test loss 1.584783, test error 33.40\n",
            "Epoch    54, train loss 0.006080, train error 0.00,  test loss 1.585548, test error 33.40\n",
            "Epoch    55, train loss 0.006055, train error 0.00,  test loss 1.586653, test error 33.40\n",
            "Epoch    56, train loss 0.006029, train error 0.00,  test loss 1.586986, test error 33.40\n",
            "Epoch    57, train loss 0.006005, train error 0.00,  test loss 1.588366, test error 33.50\n",
            "Epoch    58, train loss 0.005980, train error 0.00,  test loss 1.588933, test error 33.40\n",
            "Epoch    59, train loss 0.005956, train error 0.00,  test loss 1.589748, test error 33.50\n",
            "Epoch    60, train loss 0.005943, train error 0.00,  test loss 1.590078, test error 33.40\n",
            "Epoch    61, train loss 0.005931, train error 0.00,  test loss 1.590812, test error 33.50\n",
            "Epoch    62, train loss 0.005919, train error 0.00,  test loss 1.591361, test error 33.50\n",
            "Epoch    63, train loss 0.005907, train error 0.00,  test loss 1.591685, test error 33.40\n",
            "Epoch    64, train loss 0.005895, train error 0.00,  test loss 1.592052, test error 33.50\n",
            "Epoch    65, train loss 0.005883, train error 0.00,  test loss 1.592442, test error 33.50\n",
            "Epoch    66, train loss 0.005871, train error 0.00,  test loss 1.592795, test error 33.50\n",
            "Epoch    67, train loss 0.005859, train error 0.00,  test loss 1.593174, test error 33.50\n",
            "Epoch    68, train loss 0.005847, train error 0.00,  test loss 1.593891, test error 33.50\n",
            "Epoch    69, train loss 0.005835, train error 0.00,  test loss 1.594164, test error 33.50\n",
            "Epoch    70, train loss 0.005829, train error 0.00,  test loss 1.594408, test error 33.50\n",
            "Epoch    71, train loss 0.005823, train error 0.00,  test loss 1.594581, test error 33.50\n",
            "Epoch    72, train loss 0.005817, train error 0.00,  test loss 1.594809, test error 33.50\n",
            "Epoch    73, train loss 0.005812, train error 0.00,  test loss 1.595025, test error 33.50\n",
            "Epoch    74, train loss 0.005806, train error 0.00,  test loss 1.595229, test error 33.50\n",
            "Epoch    75, train loss 0.005800, train error 0.00,  test loss 1.595486, test error 33.50\n",
            "Epoch    76, train loss 0.005794, train error 0.00,  test loss 1.595735, test error 33.50\n",
            "Epoch    77, train loss 0.005788, train error 0.00,  test loss 1.595906, test error 33.50\n",
            "Epoch    78, train loss 0.005782, train error 0.00,  test loss 1.596015, test error 33.50\n",
            "Epoch    79, train loss 0.005777, train error 0.00,  test loss 1.596349, test error 33.50\n",
            "Epoch    80, train loss 0.005774, train error 0.00,  test loss 1.596460, test error 33.50\n",
            "Epoch    81, train loss 0.005771, train error 0.00,  test loss 1.596515, test error 33.50\n",
            "Epoch    82, train loss 0.005768, train error 0.00,  test loss 1.596681, test error 33.50\n",
            "Epoch    83, train loss 0.005765, train error 0.00,  test loss 1.596731, test error 33.50\n",
            "Epoch    84, train loss 0.005762, train error 0.00,  test loss 1.596824, test error 33.50\n",
            "Epoch    85, train loss 0.005759, train error 0.00,  test loss 1.596928, test error 33.50\n",
            "Epoch    86, train loss 0.005756, train error 0.00,  test loss 1.597050, test error 33.50\n",
            "Epoch    87, train loss 0.005754, train error 0.00,  test loss 1.597148, test error 33.50\n",
            "Epoch    88, train loss 0.005751, train error 0.00,  test loss 1.597260, test error 33.50\n",
            "Epoch    89, train loss 0.005748, train error 0.00,  test loss 1.597383, test error 33.50\n",
            "Epoch    90, train loss 0.005746, train error 0.00,  test loss 1.597428, test error 33.50\n",
            "Epoch    91, train loss 0.005745, train error 0.00,  test loss 1.597498, test error 33.50\n",
            "Epoch    92, train loss 0.005744, train error 0.00,  test loss 1.597519, test error 33.50\n",
            "Epoch    93, train loss 0.005742, train error 0.00,  test loss 1.597569, test error 33.50\n",
            "Epoch    94, train loss 0.005741, train error 0.00,  test loss 1.597645, test error 33.50\n",
            "Epoch    95, train loss 0.005739, train error 0.00,  test loss 1.597684, test error 33.50\n",
            "Epoch    96, train loss 0.005738, train error 0.00,  test loss 1.597766, test error 33.50\n",
            "Epoch    97, train loss 0.005736, train error 0.00,  test loss 1.597813, test error 33.50\n",
            "Epoch    98, train loss 0.005735, train error 0.00,  test loss 1.597878, test error 33.50\n",
            "Epoch    99, train loss 0.005734, train error 0.00,  test loss 1.597904, test error 33.50\n"
          ]
        }
      ],
      "source": [
        "model = copy.deepcopy(model_f)\n",
        "\n",
        "# choose cross entropy loss function (equation 5.24)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "# construct SGD optimizer and initialize learning rate and momentum\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9)\n",
        "# object that decreases learning rate by half every 10 epochs\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "x_train_f = torch.tensor(data['x'].astype('float32'))\n",
        "y_train_f = torch.tensor(data['y'].transpose().astype('int64'))\n",
        "x_test_f= torch.tensor(data['x_test'].astype('float32'))\n",
        "y_test_f = torch.tensor(data['y_test'].astype('int64'))\n",
        "\n",
        "# load the data into a class that creates the batches\n",
        "data_loader = DataLoader(TensorDataset(x_train_f,y_train_f), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
        "\n",
        "# Initialize model weights\n",
        "model.apply(weights_init)\n",
        "\n",
        "# loop over the dataset n_epoch times\n",
        "n_epoch = 100\n",
        "# store the loss and the % correct at each epoch\n",
        "losses_train_f = np.zeros((n_epoch))\n",
        "errors_train_f = np.zeros((n_epoch))\n",
        "losses_test_f = np.zeros((n_epoch))\n",
        "errors_test_f = np.zeros((n_epoch))\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # loop over batches\n",
        "  for i, batch in enumerate(data_loader):\n",
        "    # retrieve inputs and labels for this batch\n",
        "    x_batch, y_batch = batch\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass -- calculate model output\n",
        "    pred = model(x_batch)\n",
        "    # compute the loss\n",
        "    loss = loss_function(pred, y_batch)\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    # SGD update\n",
        "    optimizer.step()\n",
        "\n",
        "  # Run whole dataset to get statistics -- normally wouldn't do this\n",
        "  pred_train = model(x_train_f)\n",
        "  pred_test = model(x_test_f)\n",
        "  _, predicted_train_class = torch.max(pred_train.data, 1)\n",
        "  _, predicted_test_class = torch.max(pred_test.data, 1)\n",
        "  errors_train_f[epoch] = 100 - 100 * (predicted_train_class == y_train_f).float().sum() / len(y_train_f)\n",
        "  errors_test_f[epoch]= 100 - 100 * (predicted_test_class == y_test_f).float().sum() / len(y_test_f)\n",
        "  losses_train_f[epoch] = loss_function(pred_train, y_train_f).item()\n",
        "  losses_test_f[epoch]= loss_function(pred_test, y_test_f).item()\n",
        "  print(f'Epoch {epoch:5d}, train loss {losses_train_f[epoch]:.6f}, train error {errors_train_f[epoch]:3.2f},  test loss {losses_test_f[epoch]:.6f}, test error {errors_test_f[epoch]:3.2f}')\n",
        "\n",
        "  # tell scheduler to consider updating learning rate\n",
        "  scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yI-l6kA_EH9G"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkkAAAHFCAYAAADmGm0KAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNPklEQVR4nO3deXwTdf7H8Xd6pQdtKS30ECjlBsuNshwKilQEUUCXVVFhWRUEkXohLCjHKlV08YAVF12RVRF/ruKi6yIgiigqCILcqJwCtZxtaaGF9vv7YzaxoSmU0jZJ+3o+HnlkMjOZfJJJH3n3+/3OjM0YYwQAAAAXfp4uAAAAwBsRkgAAANwgJAEAALhBSAIAAHCDkAQAAOAGIQkAAMANQhIAAIAbhCQAAAA3CEkAAABuEJJQJdhstlLdPv/884t6ncmTJ8tms5VP0f9zrnqHDh1arq9V3mbOnKnmzZvLbrcrKSlJU6ZM0enTp0v13NOnT2vKlClq0KCB7Ha7mjdvrpkzZ7pdd+fOnRo4cKBq1qypGjVqqFevXlq3bl2Zaq6s74ok5ebmavLkyaXe1u7du89Z0+TJky+6poqwb98+DRgwQA0bNlRYWJgiIyPVrl07zZo1S2fOnHFZ9+2339aVV16p2NhY2e12JSQkqF+/flq1alWpXmvo0KFuP5vmzZu7Xf9ivqNAgKcLAMrD119/7fL4L3/5iz777DMtX77cZX7Lli0v6nXuuusu9e7d+6K24c7NN9+shx56qNj82rVrl/trlZcnn3xSjz32mMaNG6eUlBStWbNGEydO1P79+zVnzpzzPn/kyJF644039Je//EWXXXaZPvnkE40ZM0bZ2dn685//7Fzv0KFDuuKKKxQVFaXXXntNwcHBSktLU48ePbRmzRo1a9bsguqurO+KZIWkKVOmSJJ69OhR6ueNHj1at912W7H5devWveiaKkJOTo4iIiL02GOPqX79+srPz9fHH3+s0aNHa/369Xr11Ved6x45ckRdu3bVmDFjFBMTo4MHD2rGjBm68sor9emnn6p79+7nfb2QkJBi+yskJKTYehf7HQVkgCpoyJAhJiws7Lzr5eTkVEI15ybJjBo1qkzPLan+wsJCk5ubezFlmdzcXFNYWOh22eHDh01wcLC55557XOY/+eSTxmazmc2bN59z25s2bTI2m81MmzbNZf7dd99tQkJCzJEjR5zzHnnkERMYGGh2797tnJeZmWliYmLMoEGDLvRtFVPa70pZHDp0yEgykyZNKtX6u3btMpLMM888U6bXK+n7cObMGXPq1KkybfN82z6XQYMGmYCAgPO+9vHjx01gYKC54447zrvN0u6vi/2OAsYYQ3cbqo0ePXooOTlZX3zxhbp06aLQ0FANGzZMkvTOO+8oJSVF8fHxCgkJUYsWLTRu3Djl5OS4bMNdd1uDBg10/fXXa/HixWrfvr1CQkLUvHlzvfbaa+Va/9ChQ1WjRg1t3LhRKSkpCg8PV8+ePSVZXUj33XefXn75ZbVo0UJ2u13z5s2TJH355Zfq2bOnwsPDFRoaqi5duug///mPy7Zff/112Ww2LVmyRMOGDVPt2rUVGhqqvLw8t7UsXrxYp06d0h//+EeX+X/84x9ljNEHH3xwzvfywQcfyBjj9vknT57U4sWLnfMWLlyoq6++WomJic55ERERGjhwoD788MNi3TnlIT8/X0888YSzm6Z27dr64x//qEOHDrmst3z5cvXo0UPR0dEKCQlR/fr1ddNNNyk3N1e7d+92tgROmTKl3LtQS/o+O7rspk+frieeeEJJSUmy2+367LPPJEmLFi1S586dFRoaqvDwcPXq1atY65rje75u3TrdfPPNioqKUqNGjS64xtq1a8vPz0/+/v7nXC88PFzBwcEKCCi/zo2L/Y4CEt1tqGYOHjyo22+/XWPHjtW0adPk52f9n/Djjz+qT58+Sk1NVVhYmLZt26ann35aq1evLtas786GDRv00EMPady4cYqNjdWrr76qP/3pT2rcuLGuvPLK8z7fGOP2x97f398llOXn5+uGG27Q8OHDNW7cOJfnfPDBB1q5cqUef/xxxcXFqU6dOlqxYoV69eql1q1b6x//+Ifsdrteeukl9evXT2+//bb+8Ic/uLzesGHD1LdvX73xxhvKyclRYGCg23o3bdokSWrVqpXL/Pj4eMXExDiXl2TTpk2qXbu24uLiXOa3bt3aZfsnT57Uzz//rAEDBhTbRuvWrXXy5Ent3LlTTZs2PefrXYjCwkLdeOONWrlypcaOHasuXbpoz549mjRpknr06KHvvvtOISEh2r17t/r27asrrrhCr732mmrWrKn9+/dr8eLFys/PV3x8vBYvXqzevXvrT3/6k+666y5JpetCLSwsdPt9ODtElPR9lqQXX3xRTZs21bPPPquIiAg1adJE8+fP1+DBg5WSkqK3335beXl5mj59unr06KFPP/1U3bp1c9n+wIEDdcstt2jEiBHF/mFwxxijgoICZWdna8mSJXr99df10EMPuQ0/BQUFKiws1P79+5WWliZjjEaNGnXe15Cs70VcXJwOHTqk+Ph49e/fX1OnTlWtWrWc61zsdxSQRHcbqiZ3TfLdu3c3ksynn356zucWFhaa06dPmxUrVhhJZsOGDc5lkyZNMmf/2SQmJprg4GCzZ88e57yTJ0+aWrVqmeHDh5+3Vkkl3t544w2X9yTJvPbaa263ERkZaY4ePeoy/3e/+52pU6eOyc7Ods47c+aMSU5ONnXr1nV2p82dO9dIMnfeeed56zXG6haz2+1ulzVt2tSkpKSc8/m9evUyzZo1c7ssKCjI2UWyf/9+I8mkpaUVW2/+/PlGklm1alWpai7J2d+Vt99+20gy7733nst6a9asMZLMSy+9ZIwx5l//+peRZNavX1/itsva3VbSbeXKlc51S/o+O7bRqFEjk5+f75xfUFBgEhISTKtWrUxBQYFzfnZ2tqlTp47p0qWLc57je/7444+Xqm6HtLQ0Z602m81MmDChxHWbNWvmXDc+Pt58+eWXpXqNGTNmmBkzZpglS5aYJUuWmAkTJpjQ0FDTvHlzl+/5xX5HAWOMoSUJ1UpUVJSuvvrqYvN37typiRMnavny5crIyJAxxrls69atzhaOkrRt21b169d3Pg4ODlbTpk21Z8+eUtU1aNAgPfLII8XmN2zYsNi8m266ye02rr76akVFRTkf5+Tk6Ntvv9W9996rGjVqOOf7+/vrjjvu0KOPPqrt27e7HBVU0rbdOddRfqU5AvBCnn+xr3UhPvroI9WsWVP9+vVzac1p27at4uLi9Pnnn+vee+9V27ZtFRQUpHvuuUcjR47UFVdc4XZ/lcWYMWN0++23F5t/9hFcJX2fJemGG25waQncvn27Dhw4oNTUVJcWpxo1auimm27S3//+d+Xm5io0NNS57EK+D5LVJXzNNdfo6NGjWr58uZ555hllZma6PWrxvffeU05Ojvbu3auXX35Z1113nRYtWnTeAe4PPPCAy+NevXqpXbt2uvnmm/XKK6+4LK/M7w2qJkISqpX4+Phi806cOKErrrhCwcHBeuKJJ9S0aVOFhoZq3759GjhwoE6ePHne7UZHRxebZ7fbS/VcyeqC6dix43nXCw0NVUREhNtlZ7+3Y8eOyRjj9j0nJCRIso40Otc2ShIdHa1Tp04V+1GVpKNHj6pDhw7nff769euLzc/JyVF+fr6z2yQqKko2m61YnY7XkeTSxVIefv31Vx0/flxBQUFulx8+fFiS1KhRIy1btkzTp0/XqFGjlJOTo4YNG+r+++/XmDFjLqqGunXrlur7cK79dfYyx2dY0vehsLBQx44dc9mfpf0+OMTFxTm7UFNSUhQVFaVx48Zp2LBhateuncu6l156qSTp8ssvV//+/dWuXTuNGTNGGzZsuKDXlKQBAwYoLCxM33zzjXPexX5HAYmQhGrG3X+Py5cv14EDB/T555+7HH58/PjxSqysdC7kP+OoqCj5+fnp4MGDxdY9cOCAJCkmJqbU2y/KMc5j48aN6tSpk3N+enq6Dh8+rOTk5PM+f8GCBUpPT3cZl7Rx40ZJcj4/JCREjRs3ds4vauPGjQoJCSm31huHmJgYRUdHuwweLyo8PNw5fcUVV+iKK65QQUGBvvvuO82cOVOpqamKjY3VLbfcUq51uXMh3wdHkC/p++Dn5+fSEnm+7ZfG5ZdfLknasWNHsZBUVEBAgNq3b6//+7//K/NrGWNcWsgu9jsKSJxMEnD+ENjtdpf5f//73z1RTrkJCwtTp06d9P7777u0aBUWFurNN99U3bp1yzzguXfv3goODtbrr7/uMt9xlFz//v3P+fwbb7xRNpvNeQRe0eeHhIS4nItqwIABWr58ufbt2+ecl52drffff1833HBDuR4RJUnXX3+9jhw5ooKCAnXs2LHYzd15mfz9/dWpUyf97W9/kyTniS4d36nStihWpGbNmumSSy7R/PnzXbqTc3Jy9N577zmPeCtPjiPqGjdufM71Tp06pW+++ea865XkX//6l3Jzc/W73/3OOe9iv6OAREsSoC5duigqKkojRozQpEmTFBgYqLfeeqtMzf5l9euvv7p0FThERERc1EkN09LS1KtXL1111VV6+OGHFRQUpJdeekmbNm3S22+/XeaWglq1amnixIl67LHHVKtWLeeJ+iZPnqy77rrLpeZ//vOfGjZsmF577TXdeeedkqyulj/96U+aNGmS/P39ddlll2nJkiWaM2eOnnjiCZcutIcfflhvvPGG+vbtq6lTp8put+upp57SqVOnip2BeujQoZo3b5527dqlBg0alOm93XLLLXrrrbfUp08fjRkzRpdffrkCAwP1yy+/6LPPPtONN96oAQMG6OWXX9by5cvVt29f1a9fX6dOnXKe9uGaa66RZLU6JSYm6t///rd69uypWrVqKSYm5ry17d271+33oXbt2mU6FF+S/Pz8NH36dA0ePFjXX3+9hg8frry8PD3zzDM6fvy4nnrqqTJtV5ImTZqkX3/9VVdeeaUuueQSHT9+XIsXL9Yrr7yi3//+9y5dW126dNENN9ygFi1aKDIyUrt379bs2bP1888/a+HChS7bdYSmn376SZK0Z88e3XbbbbrlllvUuHFj2Ww2rVixQs8//7wuvfRS5xGE0oV9R4ESeXbcOFAxSjq67dJLL3W7/qpVq0znzp1NaGioqV27trnrrrvMunXrjCQzd+5c53olHd3Wt2/fYtvs3r276d69+3lr1TmOZurates531PRbZR0QsqVK1eaq6++2oSFhZmQkBDzu9/9znz44Ycu6ziObluzZs156y3qhRdeME2bNjVBQUGmfv36ZtKkSS5HVBXddtHP0Rhj8vPzzaRJk0z9+vVNUFCQadq0qXnxxRfdvs5PP/1k+vfvbyIiIkxoaKjp2bOnWbt2bbH1brrpJhMSEmKOHTtW6vfg7nM9ffq0efbZZ02bNm1McHCwqVGjhmnevLkZPny4+fHHH40xxnz99ddmwIABJjEx0djtdhMdHW26d+9uFi1a5LKtZcuWmXbt2hm73W4kmSFDhpRYy/mObhs8eLBz3ZK+z+c7IeUHH3xgOnXqZIKDg01YWJjp2bOn+eqrr1zWcXzPDx06dM7PzmHRokXmmmuuMbGxsSYgIMDUqFHDXH755ebFF180p0+fdln3oYceMm3atDGRkZEmICDAxMXFmQEDBhSrwRjrbysxMdH5+OjRo2bAgAGmQYMGJiQkxAQFBZkmTZqYsWPHmuPHj7utrTTfUaAkNmOKtLsCgA+Li4vTHXfcoWeeecbTpQCoAghJAKqEzZs3q3Pnztq5c2exAekAUBaEJAAAADc4ug0AAMANj4akL774Qv369VNCQoJsNluxCw4aYzR58mQlJCQoJCREPXr00ObNm13WycvL0+jRoxUTE6OwsDDdcMMN+uWXXyrxXQAAgKrIoyEpJydHbdq00axZs9wunz59umbMmKFZs2ZpzZo1iouLU69evZSdne1cJzU1VQsXLtSCBQv05Zdf6sSJE7r++utVUFBQWW8DAABUQV4zJslms2nhwoXOE3wZY5SQkKDU1FQ9+uijkqxWo9jYWD399NMaPny4MjMzVbt2bb3xxhvOq5kfOHBA9erV08cff6xrr73WU28HAAD4OK89meSuXbuUnp6ulJQU5zy73a7u3btr1apVGj58uNauXavTp0+7rJOQkKDk5GStWrWqxJCUl5envLw85+PCwkIdPXpU0dHRXPQQAAAfYYxRdna2EhISXC5LU168NiSlp6dLkmJjY13mx8bGOq+snp6erqCgoGLXG4qNjXU+3520tDRNmTKlnCsGAACesG/fPtWtW7fct+u1Icnh7JYdY8x5W3vOt8748eP14IMPOh9nZmaqfv362rdvX4lXWAcAAN4lKytL9erVc7nwdHny2pDkuDJ4enq64uPjnfMzMjKcrUtxcXHKz8/XsWPHXFqTMjIy1KVLlxK3bbfbi13MVLKuk0VIAgDAt1TUUBmvPU9SUlKS4uLitHTpUue8/Px8rVixwhmAOnTooMDAQJd1Dh48qE2bNp0zJAEAAJyPR1uSTpw44by6s2QN1l6/fr1q1aql+vXrKzU1VdOmTVOTJk3UpEkTTZs2TaGhobrtttskSZGRkfrTn/6khx56SNHR0apVq5YefvhhtWrVynkVbgAAgLLwaEj67rvvdNVVVzkfO8YJDRkyRK+//rrGjh2rkydPauTIkTp27Jg6deqkJUuWuPQ9PvfccwoICNCgQYN08uRJ9ezZU6+//rr8/f0r/f0AAICqw2vOk+RJWVlZioyMVGZmJmOSAADlqqCgQKdPn/Z0GT4pMDDwnI0eFf377bUDtwEA8GXGGKWnp+v48eOeLsWn1axZU3FxcR45jyEhCQCACuAISHXq1FFoaCgnK75Axhjl5uYqIyNDklyOdK8shCQAAMpZQUGBMyBFR0d7uhyfFRISIsk6tU+dOnUqfbyx154CAAAAX+UYgxQaGurhSnyf4zP0xLguQhIAABWELraL58nPkJAEAADgBiEJAABUiAYNGuj555/3dBllxsBtAADg1KNHD7Vt27Zcws2aNWsUFhZ28UV5CCEJAACUmjFGBQUFCgg4f4SoXbt2JVRUcehuAwAAkqShQ4dqxYoVeuGFF2Sz2WSz2fT666/LZrPpk08+UceOHWW327Vy5Ur9/PPPuvHGGxUbG6saNWrosssu07Jly1y2d3Z3m81m06uvvqoBAwYoNDRUTZo00aJFiyr5XZYeIQkAgMpgjJST45lbKa9A9sILL6hz5866++67dfDgQR08eFD16tWTJI0dO1ZpaWnaunWrWrdurRMnTqhPnz5atmyZvv/+e1177bXq16+f9u7de87XmDJligYNGqQffvhBffr00eDBg3X06NGL/ngrAt1tAABUhtxcqUYNz7z2iRNSKcYGRUZGKigoSKGhoYqLi5Mkbdu2TZI0depU9erVy7ludHS02rRp43z8xBNPaOHChVq0aJHuu+++El9j6NChuvXWWyVJ06ZN08yZM7V69Wr17t27TG+tItGSBAAAzqtjx44uj3NycjR27Fi1bNlSNWvWVI0aNbRt27bztiS1bt3aOR0WFqbw8HDnpUe8DS1JAABUhtBQq0XHU699kc4+Su2RRx7RJ598omeffVaNGzdWSEiIbr75ZuXn559zO4GBgS6PbTabCgsLL7q+ikBIAgCgMthspery8rSgoCAVFBScd72VK1dq6NChGjBggCTpxIkT2r17dwVXV7nobgMAAE4NGjTQt99+q927d+vw4cMltvI0btxY77//vtavX68NGzbotttu89oWobIiJAEAAKeHH35Y/v7+atmypWrXrl3iGKPnnntOUVFR6tKli/r166drr71W7du3r+RqK5bNmFIeF1iFZWVlKTIyUpmZmYqIiPB0OQAAH3fq1Cnt2rVLSUlJCg4O9nQ5Pu1cn2VF/37TkgQAAOAGIQkAAMANQhIAAIAbhCQAAAA3CEkAAABuEJIAAADcICQBAAC4QUgCAABwg5AEAADgBiEJAADADUISAABw6tGjh1JTU8tte0OHDlX//v3LbXuViZAEAADgBiEJAABIslp9VqxYoRdeeEE2m002m027d+/Wli1b1KdPH9WoUUOxsbG64447dPjwYefz/vWvf6lVq1YKCQlRdHS0rrnmGuXk5Gjy5MmaN2+e/v3vfzu39/nnn3vuDV6gAE8XAABAdWCMlJvrmdcODZVstvOv98ILL2jHjh1KTk7W1KlTJUkFBQXq3r277r77bs2YMUMnT57Uo48+qkGDBmn58uU6ePCgbr31Vk2fPl0DBgxQdna2Vq5cKWOMHn74YW3dulVZWVmaO3euJKlWrVoV+VbLFSEJAIBKkJsr1ajhmdc+cUIKCzv/epGRkQoKClJoaKji4uIkSY8//rjat2+vadOmOdd77bXXVK9ePe3YsUMnTpzQmTNnNHDgQCUmJkqSWrVq5Vw3JCREeXl5zu35EkISAAAo0dq1a/XZZ5+phpuE9/PPPyslJUU9e/ZUq1atdO211yolJUU333yzoqKiPFBt+SIkAQBQCUJDrRYdT712WRUWFqpfv356+umniy2Lj4+Xv7+/li5dqlWrVmnJkiWaOXOmJkyYoG+//VZJSUkXUbXnEZIAAKgENlvpurw8LSgoSAUFBc7H7du313vvvacGDRooIMB9bLDZbOratau6du2qxx9/XImJiVq4cKEefPDBYtvzJRzdBgAAnBo0aKBvv/1Wu3fv1uHDhzVq1CgdPXpUt956q1avXq2dO3dqyZIlGjZsmAoKCvTtt99q2rRp+u6777R37169//77OnTokFq0aOHc3g8//KDt27fr8OHDOn36tIffYekRkgAAgNPDDz8sf39/tWzZUrVr11Z+fr6++uorFRQU6Nprr1VycrLGjBmjyMhI+fn5KSIiQl988YX69Omjpk2bauLEifrrX/+q6667TpJ09913q1mzZurYsaNq166tr776ysPvsPRsxhjj6SI8LSsrS5GRkcrMzFRERISnywEA+LhTp05p165dSkpKUnBwsKfL8Wnn+iwr+vebliQAAAA3CEkAAABuEJIAAADcICQBAAC4QUgCAKCCcGzUxfPkZ0hIAgCgnAUGBkqScj11RdsqxPEZOj7TysQZtwEAKGf+/v6qWbOmMjIyJEmhoaGy2Wwersq3GGOUm5urjIwM1axZU/7+/pVeAyEJAIAK4LjqvSMooWxq1qzp/CwrGyEJAIAKYLPZFB8frzp16vjUpTi8SWBgoEdakBwISQAAVCB/f3+P/tCj7Bi4DQAA4AYhCQAAwA1CEgAAgBuEJAAAADcISQAAAG4QkgAAANwgJAEAALhBSAIAAHCDkAQAAOAGIQkAAMANQhIAAIAbhCQAAAA3CEkAAABuEJIAAADc8OqQdObMGU2cOFFJSUkKCQlRw4YNNXXqVBUWFjrXMcZo8uTJSkhIUEhIiHr06KHNmzd7sGoAAFAVeHVIevrpp/Xyyy9r1qxZ2rp1q6ZPn65nnnlGM2fOdK4zffp0zZgxQ7NmzdKaNWsUFxenXr16KTs724OVAwAAX+fVIenrr7/WjTfeqL59+6pBgwa6+eablZKSou+++06S1Yr0/PPPa8KECRo4cKCSk5M1b9485ebmav78+R6uHgAA+DKvDkndunXTp59+qh07dkiSNmzYoC+//FJ9+vSRJO3atUvp6elKSUlxPsdut6t79+5atWpVidvNy8tTVlaWyw0AAKCoAE8XcC6PPvqoMjMz1bx5c/n7+6ugoEBPPvmkbr31VklSenq6JCk2NtblebGxsdqzZ0+J201LS9OUKVMqrnAAAODzvLol6Z133tGbb76p+fPna926dZo3b56effZZzZs3z2U9m83m8tgYU2xeUePHj1dmZqbztm/fvgqpHwAA+C6vbkl65JFHNG7cON1yyy2SpFatWmnPnj1KS0vTkCFDFBcXJ8lqUYqPj3c+LyMjo1jrUlF2u112u71iiwcAAD7Nq1uScnNz5efnWqK/v7/zFABJSUmKi4vT0qVLncvz8/O1YsUKdenSpVJrBQAAVYtXtyT169dPTz75pOrXr69LL71U33//vWbMmKFhw4ZJsrrZUlNTNW3aNDVp0kRNmjTRtGnTFBoaqttuu83D1QMAAF/m1SFp5syZeuyxxzRy5EhlZGQoISFBw4cP1+OPP+5cZ+zYsTp58qRGjhypY8eOqVOnTlqyZInCw8M9WDkAAPB1NmOM8XQRnpaVlaXIyEhlZmYqIiLC0+UAAIBSqOjfb68ekwQAAOAphCQAAAA3CEkAAABuEJIAAADcICQBAAC4QUgCAABwg5AEAADgBiEJAADADUISAACAG4QkAAAANwhJAAAAbhCSAAAA3CAkAQAAuEFIAgAAcIOQBAAA4AYhCQAAwA1CEgAAgBuEJAAAADcISQAAAG4QkgAAANwgJAEAALhBSAIAAHCDkAQAAOAGIQkAAMANQhIAAIAbhCQAAAA3CEkAAABuEJIAAADcICQBAAC4QUgqwhhPVwAAALwFIamIPXs8XQEAAPAWhKQiNm70dAUAAMBbEJKKICQBAAAHQlIRGzZ4ugIAAOAtCElF0JIEAAAcCElF7N8vHTni6SoAAIA3ICSdZf16T1cAAAC8ASHpLIQkAAAgEZKKISQBAACJkFTM9997ugIAAOANCEln2bZNOnnS01UAAABPIyQVUauWVFAgbdrk6UoAAICnEZKKaNPGumdcEgAAICQV0aqVdU9IAgAAhKQiWre27hm8DQAACElFOELSDz9YY5MAAED1RUgqonFjKSREysmRfv7Z09UAAABPIiQV4W8rpMsNAABIIiS52rdPbdtakwzeBgCgeiMkFbV9u9q1syYJSQAAVG+EpKK2bXO2JNHdBgBA9UZIKmrbNrVqJfn5Sb/+KqWne7ogAADgKYSkorZtU2io1KyZ9ZDWJAAAqi9CUlE7dkjGMHgbAAAQklxkZ0sHDjB4GwAAEJKK2bKFwdsAAICQVMyWLWrZ0prcuVM6fdqz5QAAAM8gJJ1tyxbFx0vBwdb12/bu9XRBAADAEwhJZ9uyRX5+UsOG1kOu4QYAQPVESDrb5s2SMWrUyHpISAIAoHoiJJ3t2DEpI4OQBABANUdIKiopybrfsoWQBABANUdIKspxqu2tWwlJAABUc14fkvbv36/bb79d0dHRCg0NVdu2bbV27VrncmOMJk+erISEBIWEhKhHjx7avHlz2V6seXPrvkhL0s6dkjEX+SYAAIDP8eqQdOzYMXXt2lWBgYH673//qy1btuivf/2ratas6Vxn+vTpmjFjhmbNmqU1a9YoLi5OvXr1UnZ29oW/YJGQ1KCBdaHbnBzrYrcAAKB6CfB0Aefy9NNPq169epo7d65zXoMGDZzTxhg9//zzmjBhggYOHChJmjdvnmJjYzV//nwNHz78wl7Q0d22ZYuCgqR69aQ9e6wut7i4i303AADAl3h1S9KiRYvUsWNH/f73v1edOnXUrl07vfLKK87lu3btUnp6ulJSUpzz7Ha7unfvrlWrVpW43by8PGVlZbncJElNm1r3v/4qHTnCuCQAAKoxrw5JO3fu1OzZs9WkSRN98sknGjFihO6//37985//lCSlp6dLkmJjY12eFxsb61zmTlpamiIjI523evXqWQtq1JASE63pIoO3f/qpfN8XAADwfl4dkgoLC9W+fXtNmzZN7dq10/Dhw3X33Xdr9uzZLuvZbDaXx8aYYvOKGj9+vDIzM523ffv2/bawRQvrntMAAABQrXl1SIqPj1dLx9Vm/6dFixba+78LqsX9b6DQ2a1GGRkZxVqXirLb7YqIiHC5OTlej5AEAEC15tUhqWvXrtq+fbvLvB07dijxf11iSUlJiouL09KlS53L8/PztWLFCnXp0qVsL0pIAgAA8vKj2x544AF16dJF06ZN06BBg7R69WrNmTNHc+bMkWR1s6WmpmratGlq0qSJmjRpomnTpik0NFS33XZb2V7UEZKKjEk6fFjKypKKNjgBAICqzatD0mWXXaaFCxdq/Pjxmjp1qpKSkvT8889r8ODBznXGjh2rkydPauTIkTp27Jg6deqkJUuWKDw8vGwv6hiT9MsvilCWateO0KFDVmtSu3bl8KYAAIBPsBnD+aSzsrIUGRmpzMxMa3zSJZdIBw5I33yjzqmd9M030rvvSjff7OlKAQCAQ7Hf73Lm1WOSPIZxSQAAVHuEJHcISQAAVHuEJHc4VxIAANUeIckdWpIAAKj2CEnuOC6iu3+/GjW0xrXv2yfl53uuJAAAULkISe7870zeOn1asQFHFBYmFRZKu3d7tCoAAFCJCEnuBAVJMTGSJFv6QTVsaM2myw0AgOqDkFSS+Hjr/sAB57ikn37yXDkAAKByEZJKkpBg3R88yOBtAACqIUJSSdy0JBGSAACoPghJJXGEJFqSAAColghJJXHT3bZzp3WUGwAAqPoISSUp0t1Wv77k7y/l5VnXvQUAAFUfIakkRbrbAgOlxETrIV1uAABUD4SkkhTpbpMxjEsCAKCaISSVxHHW7bw86dgxQhIAANXMBYekM2fOKCAgQJs2baqIerxHcLBUq5Y1zRFuAABUOxcckgICApSYmKiCgoKKqMe7cNZtAACqrTJ1t02cOFHjx4/X0aNHy7se71Jk8HZysjW5caOUm+u5kgAAQOUIKMuTXnzxRf30009KSEhQYmKiwsLCXJavW7euXIrzuCKDtxs3lurWlX75RfrqK6lXL8+WBgAAKlaZQlL//v3LuQwvVaS7zWaTevaU5s2Tli0jJAEAUNWVKSRNmjSpvOvwTkW62yTpmmuskPTppx6sCQAAVIoyhSSHtWvXauvWrbLZbGrZsqXatWtXXnV5h6LnSpJ09dXWw3XrpKNHfzv4DQAAVD1lCkkZGRm65ZZb9Pnnn6tmzZoyxigzM1NXXXWVFixYoNq1a5d3nZ5RpLtNsjJTy5bSli3SZ59JN93kwdoAAECFKtPRbaNHj1ZWVpY2b96so0eP6tixY9q0aZOysrJ0//33l3eNnnPWWbcla1ySRJcbAABVXZlC0uLFizV79my1aNHCOa9ly5b629/+pv/+97/lVpzHOVqSTp6UsrIkEZIAAKguyhSSCgsLFRgYWGx+YGCgCgsLL7oorxESIkVGWtP/63Lr0UPy85N27JD27fNcaQAAoGKVKSRdffXVGjNmjA78LzhI0v79+/XAAw+op6Oppao4a/B2ZKR02WXWLFqTAACousoUkmbNmqXs7Gw1aNBAjRo1UuPGjZWUlKTs7GzNnDmzvGv0rLMGb0t0uQEAUB2U6ei2evXqad26dVq6dKm2bdsmY4xatmypa665przr87yzzpUkWedLmjbNOqmkMZLN5qHaAABAhbngkHTmzBkFBwdr/fr16tWrl3pV9VNPn9XdJkmdO0vBwVJ6urR1q3VaAAAAULVccHdbQECAEhMTVVBQUBH1eB833W3BwVK3btY0XW4AAFRNZRqTNHHiRI0fP15Hjx4t73q8j5uWJMnqcpOsLjcAAFD1lGlM0osvvqiffvpJCQkJSkxMVFhYmMvydevWlUtxXsHNmCTpt8Hbn38unTkjBVzUBV4AAIC3KdNPe//+/cu5DC/mprtNktq1k6KipGPHpLVrpU6dPFAbAACoMGUauC1Jw4YNU7169cq9IK/jCEk5OVJ2thQeLkny95euukp6/31rXBIhCQCAqqVMA7efffbZ6jNwu0YNZzA6uzXJ0eX2z38WWwQAAHxcmQZu9+zZU59//nk5l+LFShiXNHCgFB0tbd8udeworV7tgdoAAECFKNOYpOuuu07jx4/Xpk2b1KFDh2IDt2+44YZyKc5rJCRYF2s7KyTFxUnffivdcIO0ZYt05ZXSq69Kt9/uoToBAEC5KVNIuvfeeyVJM2bMKLbMZrNVva64EgZvS1KjRtLXX1vB6MMPpTvukH74QXrwQenIkd9u2dlSSooVrAAAgPcrU0gqLCws7zq8WwndbQ4REdIHH0iPPWZdruSZZ6zb2fr1kxYtqrgyAQBA+bmgMUl9+vRRZmam8/GTTz6p48ePOx8fOXJELaviNTpKOKFkUX5+0pNPSm+/bZ0awGaTatWSmjSR2ra11vnyS+tabwAAwPtdUEj65JNPlJeX53z89NNPu5x1+8yZM9q+fXv5VectztHddrZbbpEyMqTTp61uth07pG++kQIDrXMq7dlTwbUCAIBycUEhyZzVDHL24yqrFC1JRQUEWOdRcrDbpeRka7oqnYwcAICqrEynAKh2zjMmqTTatbPuCUkAAPiGCwpJNptNNput2LwqzxGSsrKsM2+XQfv21v3335dTTQAAoEJd0NFtxhgNHTpUdrtdknTq1CmNGDHCeZ6kouOVqpTwcCkszApIBw9KjRtf8CYcIYmWJAAAfMMFhaQhQ4a4PL7dzVkT77zzzouryBvZbFZr0k8/WYO3yxCSWre2joBLT7dylqNxCgAAeKcLCklz586tqDq8nyMklXFcUliY1Ly5dWbudeukvn3LuT4AAFCuGLhdWhd4hJs7jsHbjEsCAMD7EZJK6wLOlVQSxiUBAOA7CEmlVQ4tSYQkAAB8ByGptMrhXEmOy5Ps2WOdjRsAAHgvQlJpOULS/v1l3kTNmlKjRtb0+vUXXREAAKhAhKTSqlfPut+376KuUsuZtwEA8A2EpNKqX9+6z8mRilzU90IxLgkAAN9ASCqt4GApNtaa3rOnzJshJAEA4BsISRciMdG6v4iQ5Ohu+/FHKTu7HGoCAAAVgpB0IcohJNWpI9Wtaw1r2rChnOoCAADljpB0IcohJEl0uQEA4AsISReiQQPrfvfui9oMR7gBAOD9CEkXopxbks6+htuBA9LUqdKOHRe1eQAAUA4ISReinEPS5s3SqVPW9OLFUps20qRJUu/e0okTF/USAADgIvlUSEpLS5PNZlNqaqpznjFGkydPVkJCgkJCQtSjRw9t3ry5YgpwhKSjRy8qxVxyiVS7tlRQYHW5jRsnXXeddPiwtXzXLunRR8uhXgAAUGY+E5LWrFmjOXPmqHXr1i7zp0+frhkzZmjWrFlas2aN4uLi1KtXL2VXxPH1ERHWtUWki2pNstl+a03q1096+mlreuRI6aOPrOmXXpKWL3f//MJC68i406fLXAIAADgPnwhJJ06c0ODBg/XKK68oKirKOd8Yo+eff14TJkzQwIEDlZycrHnz5ik3N1fz58+vmGLKqcvNMXj76FEre737rvS3v0l9+0r33mstGzas+LmUjh+Xrr/eulhuWtpFlQAAAM7BJ0LSqFGj1LdvX11zzTUu83ft2qX09HSlpKQ459ntdnXv3l2rVq0qcXt5eXnKyspyuZWa4wi3iwxJvXtb9x07WgO4b775t2XTp1svs2eP9Mgjv83fsUP63e+k//7Xevzvf19UCQAA4BwCPF3A+SxYsEDr1q3TmjVrii1LT0+XJMU6LhfyP7GxsdpzjhCTlpamKVOmlK0gR0vSRZ4GoHt3KT3dGpvkd1ZUrVFDmjtXuuoq6e9/lwYOtE4++Yc/SJmZUkKCdSTc999bLUuOHkAAAFB+vLolad++fRozZozefPNNBQcHl7iezWZzeWyMKTavqPHjxyszM9N527dvX+mLKqfuNsm6FNzZAcmhRw9p9Ghr+tZbpT59rIDUpYs12LtpUys4ffHFRZcBAADc8OqQtHbtWmVkZKhDhw4KCAhQQECAVqxYoRdffFEBAQHOFiRHi5JDRkZGsdaloux2uyIiIlxupVaOIel80tKkRo2scUuFhdYYpeXLrXB11VXWOp99VuFlAABQLXl1SOrZs6c2btyo9evXO28dO3bU4MGDtX79ejVs2FBxcXFaunSp8zn5+flasWKFunTpUjFFVWJICguTFiywWo9mzZJefVWy261lhCQAACqWV49JCg8PV3Jyssu8sLAwRUdHO+enpqZq2rRpatKkiZo0aaJp06YpNDRUt912W8UU5QhJBw9KeXm/pZYK0rGj9NVXxef36GHdb9ggHTkiRUdXaBkAAFQ7Xt2SVBpjx45VamqqRo4cqY4dO2r//v1asmSJwsPDK+YFY2Kk0FBr+kLGMpWz2FipZUtresUKj5UBAECVZTPGGE8X4WlZWVmKjIxUZmZm6cYntWwpbd0qLV0qnXVagso0apR10snRo6UXX/RYGQAAeMQF/35fIJ9vSfKIShyXdC6MSwIAoOIQksrCS0KSY1zSpk3SoUMeLQUAgCqHkFQWXhKSYmKkVq2s6c8/92gpAABUOYSksvCSkCTR5QYAQEUhJJUFIQkAgCqPkFQWjovc/vKLdOaMR0u58krJZpO2bbNO3VRejJFOnSq/7QEA4GsISWURHy8FBloB6cABj5ZSq5bUpo01XR7nSzp1SnrlFenSS6WICOsyKAAAVEeEpLLw85Pq1bOmq0iXW0aGNHmyVL++dM891mmgTp+WHn3UalUCAKC6ISSVVRUalzRzphWOpkyxTiWQmGhdXDc0VPruO+m//y2/WgEA8BWEpLLyopB05ZVW49aPP0r791/Yc2fPlu6/37oM3eWXS++8I/30kzRunDRypLXOlCm0JgEAqh9CUll5UUiKjJTat7emn35a2rmzdM+bN++3IDR+vPTNN9KgQVLA/y57/PDDUkiItHq19Mkn5V83AADejJBUVo4j3LwgJElSSop1P3Om1KiRNfD60UellSvdH4D3zjvSsGHW9P33S08+aR0lV1RsrHTvvdY0rUkAgOqGC9yqjBfI++wz6eqrpaZNpe3bK7bAUsjJkebMkRYtsoJRQcFvy2rVkq67Trr+eql3b2v5wIFWeLrrLut5Zwckh/R0KSnJOurtk09+C2PnUlBgrR8WVvI62dnSsmXWujfdVPLrAwBQkoq+wC0hSWX8kHfutJpsgoOl3Fyv+pU/ftwKNB9+KH38sXTs2G/L/P2tUs+ckQYPtrrc/P3Pvb0HH5See07q3Fn66qvib/XoUaur7uuvrdu330onTkgtW1rPcdxCQqT//Meq6/PPpfx86/l33y299NJv3XwAAJQGIakSlOlDzs+3ApIxVnNLbGzFFllGZ85YAebDD6WPPpK2bLHmDxxodbmVJpgUbU1askTq1csKYu+8Y4Wsr78uW20NGkh790qFhVK/ftKCBdYRdSUxRsrMlI4csW5BQVKTJudusXIoLLRqPnzYCnWJidbprgAAvouQVAnK/CHXrWsdTvbtt9ahYT5g507rHEgpKdb5MEsrNVV64QWpbVupWTPpgw+sI+Icmja1Wou6dLHu69SxBnw7WpdWr7bW79bN6va7/nprO//+t3TrrVYA+93vrDAXE2NtMydHeu896Y03pA0brHBTtBvR4ZJLrNdv1swKWY4QVfR27JgVlBz8/KRrrpGGDpX697daucrq0CHrxJt2e9m3cSEKC62weOxY8XFiZ85Y84u+98zM0o0nCwqyGkebNrXCZ3h4xdRfHRkjZWVZQT0sTIqKct+C69i3WVnW51+zpvVddbfe8ePW7ex96+dnbT8y0n0Dd0GB9beUlXXx7wvwhEaNfpsmJFWCMn/IXbtKq1ZJ//d/0u9/X3EFeoEDB6SGDV2DUXKyNGSIdNttUkLCuZ9/5ox1ckp3YeSrr6yWpGPHrB/op56yxlb9619Wt93ZwsKk6Girl/Pw4Qt7H+Hh1o/HL7/8Ni8iwjqqr3lz13Bx9Kj1I9WsmVVX06bWH+eePVbwW7XKuv/lFysgdejg2r0YH+/+R8oY6ddfraFsu3ZZn0tRBQXWj5+jjsOHi9dVNPBVlPh4a5/Xrm193o5bSIhVQ9Ga3O2n6s4RZBz7rOgBFDab9d2KjrbuT5yw9vOxY67/CDgCT3S09b11tKSeHfrd8fe3xiNGR0s1avxWS9Hud8DX+Pu7/i0RkipBmT/kwYOl+fOlZ56xjpev4v76V2vsUN++VgtMu3blNxRr61ZrUPneva7zGzWygtj111utU9HRVi+nw9Gj0o4dv93y8lx/0B23mBjrByMoyHrezz9L//ynddu9u3zew9mCgn77kYqJsX7k9u+36szOvvjth4QUb40o+qMaE/Pbj7C71oiz5eRY58jascM6AzvKn93u+o/Gxa7n7jtw5kzprrsYGlq67wXgTfz9rcDvQEiqBGX+kP/8Z+vU1PfdZx17j4uyf790443WSTEHDbKCWJcuFTsmvrBQ+uILazzUiROuwSoqyvrv3hHAtm+3Qlx09G/dip07Sx07WuO2HC1LX38tbdx47v/0/fysMVmNGhUfh1W0leFct4rs3jt+3NoPu3YV77o8efK38OcIY+HhXnXsgteIjHQNrMHBVsuhoyXu8GHrsw4P/20dx77Nzy/eberYXnS0tQ9K+g6cOuXaEnnixG/h2fHcC+luB7wVIakSlPlD/vvfpREjrGaODz+suAKrEUew8Nb/cPPzrR+X8wWCkyet1hhHV4ujmyMuzuq+a9iw8sYwAUBVVdEhiYOuL4Zj9NjSpdKbb0q33+7ZeqoAbw1HDo7uuvMJCbGOoHOcmB0A4Hu8/CfJy111lTXiOC9PuuMO6YEH3J/eGgAA+BxC0sXw97eOhZ840Xr8/PPWsfWHDnmyKgAAUA4ISRfLz0/6y1+sE/rUqGFdrqRjR2vkLgAA8FmEpPIycKB1auvGja1DoIYP93RFAADgIhCSytOll1pXbZWss3BzSlsAAHwWIam8JSZax3cXFlonzgEAAD6JkFQRune37les8GwdAACgzAhJFeHKK637L77wbB0AAKDMCEkVwdGStGaNdRVWAADgcwhJFaFBA6luXesiTd984+lqAABAGRCSKoLNxrgkAAB8HCGpojAuCQAAn0ZIqiiOkPTNN9a13QAAgE8hJFWUZs2kOnWkU6esAdwAAMCnEJIqis1GlxsAAD6MkFSRGLwNAIDPIiRVJEdL0ldfSWfOeLYWAABwQQhJFSk5WYqKknJypHXrPF0NAAC4AISkiuTnJ11xhTXNuCQAAHwKIamiObrcGJcEAIBPISRVNMfg7ZUrpYICz9YCAABKjZBU0dq2lcLDpcxMaeNGT1cDAABKiZBU0QICpK5drWnGJQEA4DMISZWBcUkAAPgcQlJlcBzh9tVXkjGerQUAAJQKIakydOwoBQVJv/4q/fyzp6sBAAClQEiqDMHBVlCSrNYkAADg9QhJlcUxeJuQBACATyAkVRZCEgAAPoWQVFm6dLHut2yRjh71bC0AAOC8CEmVpXZtqVkza3rVKs/WAgAAzouQVJnocgMAwGcQkioTIQkAAJ9BSKpM3bpZ96tXS3l5nq0FAACcEyGpMjVpYo1NysuT1q3zdDUAAOAcCEmVyWb77Sg3utwAAPBqhKTKxrgkAAB8AiGpsjnGJXGxWwAAvBohqbK1by/Z7dKhQ9KPP3q6GgAAUAJCUmWz26XLLrOm6XIDAMBrEZI8gXFJAAB4PUKSJxQdlwQAALySV4ektLQ0XXbZZQoPD1edOnXUv39/bd++3WUdY4wmT56shIQEhYSEqEePHtq8ebOHKi4lx2kAtm2TDh/2bC0AAMAtrw5JK1as0KhRo/TNN99o6dKlOnPmjFJSUpSTk+NcZ/r06ZoxY4ZmzZqlNWvWKC4uTr169VJ2drYHKz+PWrWkFi2saS52CwCAV/LqkLR48WINHTpUl156qdq0aaO5c+dq7969Wrt2rSSrFen555/XhAkTNHDgQCUnJ2vevHnKzc3V/PnzPVz9eTAuCQAAr+bVIelsmZmZkqRatWpJknbt2qX09HSlpKQ417Hb7erevbtWnaOFJi8vT1lZWS63SnfFFdb9smWV/9oAAOC8fCYkGWP04IMPqlu3bkpOTpYkpaenS5JiY2Nd1o2NjXUucyctLU2RkZHOW7169Squ8JL07m1dpmTdOumXXyr/9QEAwDn5TEi677779MMPP+jtt98utsxms7k8NsYUm1fU+PHjlZmZ6bzt27ev3Os9rzp1pM6dremPPqr81wcAAOfkEyFp9OjRWrRokT777DPVrVvXOT8uLk6SirUaZWRkFGtdKsputysiIsLl5hE33GDdL1rkmdcHAAAl8uqQZIzRfffdp/fff1/Lly9XUlKSy/KkpCTFxcVp6dKlznn5+flasWKFujgOs/dmjpD06afSiROerQUAALjw6pA0atQovfnmm5o/f77Cw8OVnp6u9PR0nTx5UpLVzZaamqpp06Zp4cKF2rRpk4YOHarQ0FDddtttHq6+FJo3lxo3lvLzpSVLPF0NAAAowqtD0uzZs5WZmakePXooPj7eeXvnnXec64wdO1apqakaOXKkOnbsqP3792vJkiUKDw/3YOWlZLP91pr07397thYAAODCZowxni7C07KyshQZGanMzMzKH5+0YoXUo4cUHS2lp0sBAZX7+gAA+KiK/v326pakaqFrVykqSjpyRPr6a09XAwAA/oeQ5GkBAVLfvtY0R7kBAOA1CEnegFMBAADgdQhJ3qB3bykoSNqxQ9q+3dPVAAAAEZK8Q3i4dNVV1jStSQAAeAVCkregyw0AAK9CSPIW/fpZ96tWSYcOebYWAABASPIa9epJ7dpJhYXSxx97uhoAAKo9QpI3cXS5/eMfEuf4BADAowhJ3uSuu6TgYGnlSumjjzxdDQAA1RohyZvUrSulplrTjz4qnTnj0XIAAKjOCEneZtw46zpuW7dKc+d6uhoAAKotQpK3iYyUHn/cmn78cSknx7P1AABQTRGSvNGIEVLDhlJ6ujRjhqerAQCgWiIkeaOgICktzZqePl369VfP1gMAQDVESPJWv/+9dNll0okT0pQpnq4GAIBqh5DkrWw26ZlnrOk5c7jwLQAAlYyQ5M26d7cuV1JQIA0ZIp065emKAACoNghJ3u6556SoKOnbb6VhwzgTNwAAlYSQ5O0aNZLee08KCJDeflt68klPVwQAQLVASPIFV10lvfSSNf3YY9K773q2HgAAqgFCkq+4++7fLlkyZIj03XceLQcAgKqOkORLnn1W6tNHOnlSuuEGaf9+T1cEAECVRUjyJf7+1rik5GTp4EHrRJMAAKBCEJJ8TUSENH68Nb12rWdrAQCgCiMk+aLWra37H37glAAAAFQQQpIvatbMur5bdra0Z4+nqwEAoEoiJPmiwECpZUtr+ocfPFsLAABVFCHJVxXtcgMAAOWOkOSrHCFpwwbP1gEAQBVFSPJVtCQBAFChCEm+yhGSfvxRys31bC0AAFRBhCRfFRsr1aljnQJg82ZPVwMAQJVDSPJldLkBAFBhCEm+jJAEAECFIST5MkISAAAVhpDky4qeBoDLkwAAUK4ISb6sZUvJ3186dkzav9/T1QAAUKUQknyZ3S41b25N0+UGAEC5IiT5OsYlAQBQIQhJvo6QBABAhSAk+TpCEgAAFYKQ5OscIWnbNikvz7O1AABQhRCSfN0ll0hRUVJBgbR1q6erAQCgyiAk+TqbzfV8SQAAoFwQkqoCxiUBAFDuCElVASEJAIByR0iqCtq0se4JSQAAlBtCUlVw6aXW2KSMDOnXXz1dDQAAVQIhqSoIDZWaNLGmaU0CAKBcEJKqCse4pDVrPFsHAABVBCGpqujRw7qfPl3at8+jpQAAUBUQkqqK4cOlyy+XMjOloUOlwkJPVwQAgE8jJFUVAQHSm29a45OWL5defNHTFQEA4NMISVVJkybSjBnW9Lhx0qZNnq0HAAAfRkiqau65R+rb17rY7e23c9FbAADKiJBU1dhs0quvSjEx1rXcJk3ydEUAAPgkQlJVFBcnvfKKNT19uvTXv0q5uZ6tCQAAH0NIqqr695fuuksyRnr4YalBA+npp6WsLE9XBgCATyAkVWWzZ0t//7uUlCQdOmQN5m7QQJoyRcrO9nR1AAB4NUJSVRYQYA3k3r5dmjdPatZMOnZMmjzZmp4/32ppAgAAxRCSqoPAQOnOO6XNm6V33pEaN5YOHpQGD7bO1L1xo6crBADA61SZkPTSSy8pKSlJwcHB6tChg1auXOnpkryPv780aJAVip54QgoJkb74QmrXThozRtq61dMVAgDgNapESHrnnXeUmpqqCRMm6Pvvv9cVV1yh6667Tnv37vV0ad4pOFiaMMEKRTfdJBUUWGfobtlSat5cGj9eWr2aS5sAAKo1mzG+PyilU6dOat++vWbPnu2c16JFC/Xv319paWnnfX5WVpYiIyOVmZmpiIiIiizVOy1dKj33nLRsmXT69G/zExKkbt2kzp2lLl2ktm2loCCPlQkAQFEV/fsdUO5brGT5+flau3atxo0b5zI/JSVFq1at8lBVPqZXL+uWlSV9/LG0cKF1f+CA9H//Z90kqwWqVSupVi0pMlKKiLBuNWpY454CA63B4o7ps+f5+1snuwQAoCxsNmnAgEp7OZ8PSYcPH1ZBQYFiY2Nd5sfGxio9Pd3tc/Ly8pRX5HIdmZmZkqxEWu316WPdTp2yutyK3o4dk9as8XSFAIDqys/P+i36H8fvdkV1ivl8SHKwndVCYYwpNs8hLS1NU6ZMKTa/Xr16FVIbAAAoB4WFVk/GWY4cOaJIN/Mvls+HpJiYGPn7+xdrNcrIyCjWuuQwfvx4Pfjgg87Hx48fV2Jiovbu3VshHzJKLysrS/Xq1dO+ffuq5/gwL8K+8C7sD+/BvvAemZmZql+/vmrVqlUh2/f5kBQUFKQOHTpo6dKlGlCkn3Lp0qW68cYb3T7HbrfLbrcXmx8ZGckX3ktERESwL7wE+8K7sD+8B/vCe/j5VczB+j4fkiTpwQcf1B133KGOHTuqc+fOmjNnjvbu3asRI0Z4ujQAAOCjqkRI+sMf/qAjR45o6tSpOnjwoJKTk/Xxxx8rMTHR06UBAAAfVSVCkiSNHDlSI0eOLNNz7Xa7Jk2a5LYLDpWLfeE92Bfehf3hPdgX3qOi90WVOJkkAABAeasSlyUBAAAob4QkAAAANwhJAAAAbhCSAAAA3Kj2Iemll15SUlKSgoOD1aFDB61cudLTJVV5aWlpuuyyyxQeHq46deqof//+2r59u8s6xhhNnjxZCQkJCgkJUY8ePbR582YPVVx9pKWlyWazKTU11TmPfVG59u/fr9tvv13R0dEKDQ1V27ZttXbtWudy9kflOHPmjCZOnKikpCSFhISoYcOGmjp1qgoLC53rsC8qxhdffKF+/fopISFBNptNH3zwgcvy0nzueXl5Gj16tGJiYhQWFqYbbrhBv/zyy4UXY6qxBQsWmMDAQPPKK6+YLVu2mDFjxpiwsDCzZ88eT5dWpV177bVm7ty5ZtOmTWb9+vWmb9++pn79+ubEiRPOdZ566ikTHh5u3nvvPbNx40bzhz/8wcTHx5usrCwPVl61rV692jRo0MC0bt3ajBkzxjmffVF5jh49ahITE83QoUPNt99+a3bt2mWWLVtmfvrpJ+c67I/K8cQTT5jo6Gjz0UcfmV27dpl3333X1KhRwzz//PPOddgXFePjjz82EyZMMO+9956RZBYuXOiyvDSf+4gRI8wll1xili5datatW2euuuoq06ZNG3PmzJkLqqVah6TLL7/cjBgxwmVe8+bNzbhx4zxUUfWUkZFhJJkVK1YYY4wpLCw0cXFx5qmnnnKuc+rUKRMZGWlefvllT5VZpWVnZ5smTZqYpUuXmu7duztDEvuicj366KOmW7duJS5nf1Sevn37mmHDhrnMGzhwoLn99tuNMeyLynJ2SCrN5378+HETGBhoFixY4Fxn//79xs/PzyxevPiCXr/adrfl5+dr7dq1SklJcZmfkpKiVatWeaiq6ikzM1OSnBco3LVrl9LT0132jd1uV/fu3dk3FWTUqFHq27evrrnmGpf57IvKtWjRInXs2FG///3vVadOHbVr106vvPKKczn7o/J069ZNn376qXbs2CFJ2rBhg7788kv16dNHEvvCU0rzua9du1anT592WSchIUHJyckXvG+qzBm3L9Thw4dVUFCg2NhYl/mxsbFKT0/3UFXVjzFGDz74oLp166bk5GRJcn7+7vbNnj17Kr3Gqm7BggVat26d1qxZU2wZ+6Jy7dy5U7Nnz9aDDz6oP//5z1q9erXuv/9+2e123XnnneyPSvToo48qMzNTzZs3l7+/vwoKCvTkk0/q1ltvlcTfhqeU5nNPT09XUFCQoqKiiq1zob/v1TYkOdhsNpfHxphi81Bx7rvvPv3www/68ssviy1j31S8ffv2acyYMVqyZImCg4NLXI99UTkKCwvVsWNHTZs2TZLUrl07bd68WbNnz9add97pXI/9UfHeeecdvfnmm5o/f74uvfRSrV+/XqmpqUpISNCQIUOc67EvPKMsn3tZ9k217W6LiYmRv79/sVSZkZFRLKGiYowePVqLFi3SZ599prp16zrnx8XFSRL7phKsXbtWGRkZ6tChgwICAhQQEKAVK1boxRdfVEBAgPPzZl9Ujvj4eLVs2dJlXosWLbR3715J/G1UpkceeUTjxo3TLbfcolatWumOO+7QAw88oLS0NEnsC08pzeceFxen/Px8HTt2rMR1SqvahqSgoCB16NBBS5cudZm/dOlSdenSxUNVVQ/GGN133316//33tXz5ciUlJbksT0pKUlxcnMu+yc/P14oVK9g35axnz57auHGj1q9f77x17NhRgwcP1vr169WwYUP2RSXq2rVrsdNh7NixQ4mJiZL426hMubm58vNz/Yn09/d3ngKAfeEZpfncO3TooMDAQJd1Dh48qE2bNl34vinTcPMqwnEKgH/84x9my5YtJjU11YSFhZndu3d7urQq7d577zWRkZHm888/NwcPHnTecnNznes89dRTJjIy0rz//vtm48aN5tZbb+XQ2kpS9Og2Y9gXlWn16tUmICDAPPnkk+bHH380b731lgkNDTVvvvmmcx32R+UYMmSIueSSS5ynAHj//fdNTEyMGTt2rHMd9kXFyM7ONt9//735/vvvjSQzY8YM8/333ztPz1Oaz33EiBGmbt26ZtmyZWbdunXm6quv5hQAZfG3v/3NJCYmmqCgINO+fXvnYeioOJLc3ubOnetcp7Cw0EyaNMnExcUZu91urrzySrNx40bPFV2NnB2S2BeV68MPPzTJycnGbreb5s2bmzlz5rgsZ39UjqysLDNmzBhTv359ExwcbBo2bGgmTJhg8vLynOuwLyrGZ5995vY3YsiQIcaY0n3uJ0+eNPfdd5+pVauWCQkJMddff73Zu3fvBddiM8aYMrd7AQAAVFHVdkwSAADAuRCSAAAA3CAkAQAAuEFIAgAAcIOQBAAA4AYhCQAAwA1CEgAAgBuEJABww2az6YMPPvB0GQA8iJAEwOsMHTpUNput2K13796eLg1ANRLg6QIAwJ3evXtr7ty5LvPsdruHqgFQHdGSBMAr2e12xcXFudyioqIkWV1hs2fP1nXXXaeQkBAlJSXp3XffdXn+xo0bdfXVVyskJETR0dG65557dOLECZd1XnvtNV166aWy2+2Kj4/Xfffd57L88OHDGjBggEJDQ9WkSRMtWrSoYt80AK9CSALgkx577DHddNNN2rBhg26//Xbdeuut2rp1qyQpNzdXvXv3VlRUlNasWaN3331Xy5YtcwlBs2fP1qhRo3TPPfdo48aNWrRokRo3buzyGlOmTNGgQYP0ww8/qE+fPho8eLCOHj1aqe8TgAdd/PV6AaB8DRkyxPj7+5uwsDCX29SpU40xxkgyI0aMcHlOp06dzL333muMMWbOnDkmKirKnDhxwrn8P//5j/Hz8zPp6enGGGMSEhLMhAkTSqxBkpk4caLz8YkTJ4zNZjP//e9/y+19AvBujEkC4JWuuuoqzZ4922VerVq1nNOdO3d2Wda5c2etX79ekrR161a1adNGYWFhzuVdu3ZVYWGhtm/fLpvNpgMHDqhnz57nrKF169bO6bCwMIWHhysjI6OsbwmAjyEkAfBKYWFhxbq/zsdms0mSjDHOaXfrhISElGp7gYGBxZ5bWFh4QTUB8F2MSQLgk7755ptij5s3by5JatmypdavX6+cnBzn8q+++kp+fn5q2rSpwsPD1aBBA3366aeVWjMA30JLEgCvlJeXp/T0dJd5AQEBiomJkSS9++676tixo7p166a33npLq1ev1j/+8Q9J0uDBgzVp0iQNGTJEkydP1qFDhzR69Gjdcccdio2NlSRNnjxZI0aMUJ06dXTdddcpOztbX331lUaPHl25bxSA1yIkAfBKixcvVnx8vMu8Zs2aadu2bZKsI88WLFigkSNHKi4uTm+99ZZatmwpSQoNDdUnn3yiMWPG6LLLLlNoaKhuuukmzZgxw7mtIUOG6NSpU3ruuef08MMPKyYmRjfffHPlvUEAXs9mjDGeLgIALoTNZtPChQvVv39/T5cCoApjTBIAAIAbhCQAAAA3GJMEwOcwSgBAZaAlCQAAwA1CEgAAgBuEJAAAADcISQAAAG4QkgAAANwgJAEAALhBSAIAAHCDkAQAAOAGIQkAAMCN/westSQJOXsppAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min Train Error 0.00, Min Test Error 33.10\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min Train Loss 0.01, Min Test Loss 1.07\n"
          ]
        }
      ],
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(errors_train_f,'r-',label='train')\n",
        "ax.plot(errors_test_f,'b-',label='test')\n",
        "ax.set_ylim(0,100); ax.set_xlim(0,n_epoch)\n",
        "ax.set_xlabel('Epoch'); ax.set_ylabel('Error')\n",
        "ax.set_title('Train Error %3.2f, Test Error %3.2f'%(errors_train_f[-1],errors_test_f[-1]))\n",
        "ax.legend()\n",
        "plt.show()\n",
        "print(f'Min Train Error %3.2f, Min Test Error %3.2f'%(min(errors_train_f),min(errors_test_f)))\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(losses_train_f,'r-',label='train')\n",
        "ax.plot(losses_test_f,'b-',label='test')\n",
        "ax.set_xlim(0,n_epoch)\n",
        "ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
        "ax.set_title('Train Loss %3.2f, Test Loss %3.2f'%(losses_train_f[-1],losses_test_f[-1]))\n",
        "ax.legend()\n",
        "plt.show()\n",
        "print(f'Min Train Loss %3.2f, Min Test Loss %3.2f'%(min(losses_train_f),min(losses_test_f)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning\n",
        "\n",
        "Grid search to find best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.01, 0.00): min train loss 0.007896, min test loss 1.02\n",
            "(0.01, 0.30): min train loss 0.006849, min test loss 1.02\n",
            "(0.01, 0.60): min train loss 0.006895, min test loss 1.04\n",
            "(0.01, 0.90): min train loss 0.006166, min test loss 1.07\n",
            "(0.01, 1.00): min train loss 0.007303, min test loss 1.07\n",
            "(0.03, 0.00): min train loss 0.007734, min test loss 1.09\n",
            "(0.03, 0.30): min train loss 0.006903, min test loss 1.07\n",
            "(0.03, 0.60): min train loss 0.005568, min test loss 1.06\n",
            "(0.03, 0.90): min train loss 0.007069, min test loss 0.99\n",
            "(0.03, 1.00): min train loss 0.007291, min test loss 1.05\n",
            "(0.05, 0.00): min train loss 0.006816, min test loss 1.02\n",
            "(0.05, 0.30): min train loss 0.006247, min test loss 1.02\n",
            "(0.05, 0.60): min train loss 0.006629, min test loss 1.03\n",
            "(0.05, 0.90): min train loss 0.006882, min test loss 1.08\n",
            "(0.05, 1.00): min train loss 0.005564, min test loss 1.03\n",
            "(0.07, 0.00): min train loss 0.006144, min test loss 1.07\n",
            "(0.07, 0.30): min train loss 0.007713, min test loss 1.03\n",
            "(0.07, 0.60): min train loss 0.007026, min test loss 1.06\n",
            "(0.07, 0.90): min train loss 0.006031, min test loss 0.97\n",
            "(0.07, 1.00): min train loss 0.005820, min test loss 1.07\n",
            "(0.09, 0.00): min train loss 0.006985, min test loss 1.06\n",
            "(0.09, 0.30): min train loss 0.006024, min test loss 1.09\n",
            "(0.09, 0.60): min train loss 0.005612, min test loss 1.06\n",
            "(0.09, 0.90): min train loss 0.006152, min test loss 1.05\n",
            "(0.09, 1.00): min train loss 0.006179, min test loss 1.03\n",
            "best params: (0.05, 1)\n"
          ]
        }
      ],
      "source": [
        "# choose cross entropy loss function (equation 5.24)\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "min_train_loss = 10000\n",
        "\n",
        "for learning_rate in [0.01, 0.03, 0.05, 0.07, 0.09]:\n",
        "  for momentum in [0, 0.3, 0.6, 0.9, 1]:\n",
        "\n",
        "    model = copy.deepcopy(model_f)\n",
        "\n",
        "    # construct SGD optimizer and initialize learning rate and momentum\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9)\n",
        "\n",
        "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "    x_train = torch.tensor(data['x'].astype('float32'))\n",
        "    y_train = torch.tensor(data['y'].transpose().astype('int64'))\n",
        "    x_test = torch.tensor(data['x_test'].astype('float32'))\n",
        "    y_test = torch.tensor(data['y_test'].astype('int64'))\n",
        "\n",
        "    # load the data into a class that creates the batches\n",
        "    data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
        "\n",
        "    # Initialize model weights\n",
        "    model.apply(weights_init)\n",
        "\n",
        "    # loop over the dataset n_epoch times\n",
        "    n_epoch = 50\n",
        "    # store the loss and the % correct at each epoch\n",
        "    losses_train = np.zeros((n_epoch))\n",
        "    errors_train = np.zeros((n_epoch))\n",
        "    losses_test = np.zeros((n_epoch))\n",
        "    errors_test = np.zeros((n_epoch))\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "      # loop over batches\n",
        "      for i, batch in enumerate(data_loader):\n",
        "        # retrieve inputs and labels for this batch\n",
        "        x_batch, y_batch = batch\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass -- calculate model output\n",
        "        pred = model(x_batch)\n",
        "        # compute the loss\n",
        "        loss = loss_function(pred, y_batch)\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # SGD update\n",
        "        optimizer.step()\n",
        "\n",
        "      # Run whole dataset to get statistics -- normally wouldn't do this\n",
        "      pred_train = model(x_train_f)\n",
        "      pred_test = model(x_test_f)\n",
        "      _, predicted_train_class = torch.max(pred_train.data, 1)\n",
        "      _, predicted_test_class = torch.max(pred_test.data, 1)\n",
        "      errors_train[epoch] = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)\n",
        "      errors_test[epoch]= 100 - 100 * (predicted_test_class == y_test).float().sum() / len(y_test)\n",
        "      losses_train[epoch] = loss_function(pred_train, y_train).item()\n",
        "      losses_test[epoch]= loss_function(pred_test, y_test).item()\n",
        "      # print(f'Epoch {epoch:5d}, train loss {losses_train[epoch]:.6f}, train error {errors_train[epoch]:3.2f},  test loss {losses_test[epoch]:.6f}, test error {errors_test[epoch]:3.2f}')\n",
        "\n",
        "      # tell scheduler to consider updating learning rate\n",
        "      scheduler.step()\n",
        "    \n",
        "    print(f'({learning_rate:.2f}, {momentum:.2f}): min train loss {min(losses_train):.6f}, min test loss {min(losses_test):.2f}')\n",
        "\n",
        "    if min(losses_train) < min_train_loss:\n",
        "      best_params = (learning_rate, momentum)\n",
        "      min_train_loss = min(losses_train)\n",
        "\n",
        "print('best params: ' + str(best_params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: intuitively it would make more sense to train for a minimised test loss, however the task asked to train for a minimal training loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results\n",
        "* I implemented a basic grid search looping through learning rates and momentums to find the best training loss. I found all learning rate and momentum values investigated resulted in similar training losses with no clear improvements.\n",
        "\n",
        "* The total number of parameters is calculated by summing input dimension * output dimension + bias for each layer in the model, which gives $(40 \\cdot 285 + 285) + (285 \\cdot 135 + 135) + (135 \\cdot 60 + 60) + (60 \\cdot 10 + 10) = 59065$ parameters.\n",
        "\n",
        "* Throughout training, the training loss decreases with more epochs but the test loss has a minimum and increases afterwards due to over-fitting. The training loss converges to zero after some time. The test loss also converges to some value due to the weights no longer updating due to the the parameters being optimised on the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convolutional neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original hyperparameters\n",
        "Momentum = 0.9, Learning Rate = 0.08"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv1d(1, 15, kernel_size=(3,), stride=(2,))\n",
              "  (1): ReLU()\n",
              "  (2): Conv1d(15, 15, kernel_size=(3,), stride=(2,))\n",
              "  (3): ReLU()\n",
              "  (4): Conv1d(15, 15, kernel_size=(3,), stride=(2,))\n",
              "  (5): Flatten(start_dim=1, end_dim=-1)\n",
              "  (6): Linear(in_features=60, out_features=10, bias=True)\n",
              "  (7): Softmax(dim=1)\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_c = torch.nn.Sequential(\n",
        "    torch.nn.Conv1d(\n",
        "      in_channels=1,\n",
        "      out_channels=15,\n",
        "      kernel_size=3,\n",
        "      stride=2,\n",
        "      padding=0\n",
        "    ),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Conv1d(\n",
        "      in_channels=15,\n",
        "      out_channels=15,\n",
        "      kernel_size=3,\n",
        "      stride=2,\n",
        "      padding=0\n",
        "    ),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Conv1d(\n",
        "      in_channels=15,\n",
        "      out_channels=15,\n",
        "      kernel_size=3,\n",
        "      stride=2,\n",
        "      padding=0\n",
        "    ),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(60, 10),\n",
        "    torch.nn.Softmax(dim=1))\n",
        "   \n",
        "\n",
        "def weights_init(layer_in):\n",
        "  if isinstance(layer_in, nn.Linear):\n",
        "    nn.init.kaiming_uniform_(layer_in.weight)\n",
        "    layer_in.bias.data.fill_(0.0)\n",
        "\n",
        "model_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch     0, train loss 2.287551, train error 82.88,  val loss 2.286578, percent error 82.70\n",
            "Epoch     1, train loss 2.196783, train error 78.45,  val loss 2.206283, percent error 78.60\n",
            "Epoch     2, train loss 2.160195, train error 73.43,  val loss 2.181346, percent error 75.60\n",
            "Epoch     3, train loss 2.118956, train error 64.28,  val loss 2.145323, percent error 68.80\n",
            "Epoch     4, train loss 2.070565, train error 60.62,  val loss 2.097260, percent error 63.50\n",
            "Epoch     5, train loss 2.051888, train error 59.45,  val loss 2.087574, percent error 64.60\n",
            "Epoch     6, train loss 2.010463, train error 53.33,  val loss 2.054225, percent error 58.70\n",
            "Epoch     7, train loss 2.009117, train error 54.72,  val loss 2.044924, percent error 58.90\n",
            "Epoch     8, train loss 2.002418, train error 53.90,  val loss 2.038661, percent error 57.10\n",
            "Epoch     9, train loss 1.976913, train error 51.62,  val loss 2.024733, percent error 57.30\n",
            "Epoch    10, train loss 1.946164, train error 47.53,  val loss 1.988551, percent error 53.00\n",
            "Epoch    11, train loss 1.946326, train error 48.08,  val loss 1.998818, percent error 54.70\n",
            "Epoch    12, train loss 1.949443, train error 48.42,  val loss 1.993759, percent error 53.60\n",
            "Epoch    13, train loss 1.915043, train error 45.15,  val loss 1.956256, percent error 49.80\n",
            "Epoch    14, train loss 1.900379, train error 42.95,  val loss 1.943202, percent error 47.90\n",
            "Epoch    15, train loss 1.896636, train error 43.10,  val loss 1.950503, percent error 49.20\n",
            "Epoch    16, train loss 1.867262, train error 39.55,  val loss 1.907328, percent error 44.20\n",
            "Epoch    17, train loss 1.860078, train error 39.47,  val loss 1.914636, percent error 45.50\n",
            "Epoch    18, train loss 1.839434, train error 37.33,  val loss 1.880051, percent error 41.20\n",
            "Epoch    19, train loss 1.809185, train error 33.82,  val loss 1.854529, percent error 39.10\n",
            "Epoch    20, train loss 1.801151, train error 32.65,  val loss 1.850841, percent error 38.80\n",
            "Epoch    21, train loss 1.791620, train error 31.85,  val loss 1.841564, percent error 37.70\n",
            "Epoch    22, train loss 1.792063, train error 32.22,  val loss 1.842234, percent error 38.40\n",
            "Epoch    23, train loss 1.781275, train error 31.00,  val loss 1.835236, percent error 37.00\n",
            "Epoch    24, train loss 1.782220, train error 31.32,  val loss 1.834042, percent error 37.50\n",
            "Epoch    25, train loss 1.770261, train error 29.88,  val loss 1.824465, percent error 36.40\n",
            "Epoch    26, train loss 1.774597, train error 30.62,  val loss 1.826741, percent error 36.40\n",
            "Epoch    27, train loss 1.756509, train error 28.32,  val loss 1.815500, percent error 35.10\n",
            "Epoch    28, train loss 1.767444, train error 29.50,  val loss 1.815311, percent error 35.10\n",
            "Epoch    29, train loss 1.749581, train error 27.70,  val loss 1.799654, percent error 32.40\n",
            "Epoch    30, train loss 1.750341, train error 27.88,  val loss 1.797111, percent error 32.80\n",
            "Epoch    31, train loss 1.745821, train error 27.80,  val loss 1.796428, percent error 33.40\n",
            "Epoch    32, train loss 1.739856, train error 26.80,  val loss 1.790068, percent error 32.70\n",
            "Epoch    33, train loss 1.725317, train error 25.25,  val loss 1.784202, percent error 31.70\n",
            "Epoch    34, train loss 1.728382, train error 25.85,  val loss 1.792756, percent error 32.70\n",
            "Epoch    35, train loss 1.719238, train error 24.75,  val loss 1.778950, percent error 31.60\n",
            "Epoch    36, train loss 1.725512, train error 25.68,  val loss 1.781085, percent error 31.60\n",
            "Epoch    37, train loss 1.714466, train error 24.32,  val loss 1.776306, percent error 31.10\n",
            "Epoch    38, train loss 1.702731, train error 22.97,  val loss 1.765461, percent error 29.80\n",
            "Epoch    39, train loss 1.703683, train error 23.05,  val loss 1.766496, percent error 30.50\n",
            "Epoch    40, train loss 1.703173, train error 23.20,  val loss 1.765301, percent error 30.60\n",
            "Epoch    41, train loss 1.697747, train error 22.55,  val loss 1.761075, percent error 29.40\n",
            "Epoch    42, train loss 1.691412, train error 21.93,  val loss 1.759503, percent error 29.60\n",
            "Epoch    43, train loss 1.688579, train error 21.62,  val loss 1.759565, percent error 30.00\n",
            "Epoch    44, train loss 1.687748, train error 21.40,  val loss 1.759837, percent error 29.50\n",
            "Epoch    45, train loss 1.687230, train error 21.30,  val loss 1.757762, percent error 29.50\n",
            "Epoch    46, train loss 1.684188, train error 20.95,  val loss 1.757615, percent error 29.00\n",
            "Epoch    47, train loss 1.683143, train error 21.10,  val loss 1.752807, percent error 28.50\n",
            "Epoch    48, train loss 1.680747, train error 20.62,  val loss 1.752905, percent error 28.70\n",
            "Epoch    49, train loss 1.689071, train error 21.32,  val loss 1.760459, percent error 29.30\n",
            "Epoch    50, train loss 1.680304, train error 20.55,  val loss 1.752237, percent error 28.40\n",
            "Epoch    51, train loss 1.677281, train error 20.45,  val loss 1.752066, percent error 28.50\n",
            "Epoch    52, train loss 1.679913, train error 20.70,  val loss 1.756281, percent error 29.10\n",
            "Epoch    53, train loss 1.677766, train error 20.68,  val loss 1.751847, percent error 28.70\n",
            "Epoch    54, train loss 1.677098, train error 20.40,  val loss 1.752682, percent error 28.90\n",
            "Epoch    55, train loss 1.677683, train error 20.68,  val loss 1.751984, percent error 28.60\n",
            "Epoch    56, train loss 1.671911, train error 19.93,  val loss 1.750357, percent error 29.00\n",
            "Epoch    57, train loss 1.668515, train error 19.65,  val loss 1.745509, percent error 28.40\n",
            "Epoch    58, train loss 1.667621, train error 19.40,  val loss 1.748223, percent error 28.50\n",
            "Epoch    59, train loss 1.668528, train error 19.50,  val loss 1.749205, percent error 28.70\n",
            "Epoch    60, train loss 1.664839, train error 19.22,  val loss 1.746014, percent error 27.90\n",
            "Epoch    61, train loss 1.663914, train error 19.00,  val loss 1.743825, percent error 27.70\n",
            "Epoch    62, train loss 1.663070, train error 18.97,  val loss 1.744410, percent error 28.10\n",
            "Epoch    63, train loss 1.662033, train error 18.88,  val loss 1.744546, percent error 28.30\n",
            "Epoch    64, train loss 1.661345, train error 18.80,  val loss 1.742810, percent error 27.40\n",
            "Epoch    65, train loss 1.661949, train error 18.82,  val loss 1.744606, percent error 27.40\n",
            "Epoch    66, train loss 1.661548, train error 18.88,  val loss 1.743311, percent error 27.80\n",
            "Epoch    67, train loss 1.660350, train error 18.68,  val loss 1.742163, percent error 27.60\n",
            "Epoch    68, train loss 1.658839, train error 18.53,  val loss 1.741093, percent error 27.10\n",
            "Epoch    69, train loss 1.658909, train error 18.55,  val loss 1.741247, percent error 27.60\n",
            "Epoch    70, train loss 1.658329, train error 18.47,  val loss 1.740017, percent error 27.60\n",
            "Epoch    71, train loss 1.659832, train error 18.60,  val loss 1.745939, percent error 27.70\n",
            "Epoch    72, train loss 1.657758, train error 18.43,  val loss 1.744564, percent error 27.70\n",
            "Epoch    73, train loss 1.657143, train error 18.45,  val loss 1.742505, percent error 27.60\n",
            "Epoch    74, train loss 1.656440, train error 18.32,  val loss 1.740350, percent error 27.10\n",
            "Epoch    75, train loss 1.656727, train error 18.35,  val loss 1.743206, percent error 27.60\n",
            "Epoch    76, train loss 1.655327, train error 18.20,  val loss 1.740398, percent error 26.60\n",
            "Epoch    77, train loss 1.658333, train error 18.57,  val loss 1.742539, percent error 27.90\n",
            "Epoch    78, train loss 1.653840, train error 18.15,  val loss 1.736779, percent error 26.60\n",
            "Epoch    79, train loss 1.652877, train error 18.12,  val loss 1.738298, percent error 27.00\n",
            "Epoch    80, train loss 1.652792, train error 18.05,  val loss 1.740387, percent error 26.80\n",
            "Epoch    81, train loss 1.652177, train error 18.07,  val loss 1.740140, percent error 26.90\n",
            "Epoch    82, train loss 1.651725, train error 17.97,  val loss 1.738398, percent error 27.10\n",
            "Epoch    83, train loss 1.651603, train error 17.95,  val loss 1.738816, percent error 26.70\n",
            "Epoch    84, train loss 1.651564, train error 17.93,  val loss 1.739939, percent error 27.00\n",
            "Epoch    85, train loss 1.651276, train error 17.88,  val loss 1.738160, percent error 27.40\n",
            "Epoch    86, train loss 1.650913, train error 17.82,  val loss 1.737795, percent error 27.00\n",
            "Epoch    87, train loss 1.650494, train error 17.85,  val loss 1.737871, percent error 26.80\n",
            "Epoch    88, train loss 1.650507, train error 17.88,  val loss 1.738885, percent error 27.40\n",
            "Epoch    89, train loss 1.650150, train error 17.85,  val loss 1.739060, percent error 27.40\n",
            "Epoch    90, train loss 1.649703, train error 17.85,  val loss 1.738691, percent error 27.20\n",
            "Epoch    91, train loss 1.649941, train error 17.78,  val loss 1.739407, percent error 27.00\n",
            "Epoch    92, train loss 1.649761, train error 17.75,  val loss 1.738044, percent error 27.10\n",
            "Epoch    93, train loss 1.648936, train error 17.72,  val loss 1.738092, percent error 27.00\n",
            "Epoch    94, train loss 1.648831, train error 17.78,  val loss 1.740325, percent error 27.70\n",
            "Epoch    95, train loss 1.649202, train error 17.68,  val loss 1.736951, percent error 26.90\n",
            "Epoch    96, train loss 1.648241, train error 17.75,  val loss 1.737595, percent error 27.10\n",
            "Epoch    97, train loss 1.647883, train error 17.68,  val loss 1.737857, percent error 27.10\n",
            "Epoch    98, train loss 1.648332, train error 17.65,  val loss 1.739668, percent error 27.80\n",
            "Epoch    99, train loss 1.647940, train error 17.60,  val loss 1.738225, percent error 27.10\n",
            "Epoch   100, train loss 1.647199, train error 17.53,  val loss 1.738270, percent error 27.40\n",
            "Epoch   101, train loss 1.647161, train error 17.55,  val loss 1.738209, percent error 27.20\n",
            "Epoch   102, train loss 1.647115, train error 17.55,  val loss 1.738425, percent error 27.40\n",
            "Epoch   103, train loss 1.647094, train error 17.50,  val loss 1.738252, percent error 27.10\n",
            "Epoch   104, train loss 1.646861, train error 17.55,  val loss 1.739489, percent error 27.40\n",
            "Epoch   105, train loss 1.646647, train error 17.50,  val loss 1.738210, percent error 27.40\n",
            "Epoch   106, train loss 1.646837, train error 17.55,  val loss 1.738155, percent error 27.40\n",
            "Epoch   107, train loss 1.646484, train error 17.47,  val loss 1.737640, percent error 27.20\n",
            "Epoch   108, train loss 1.646502, train error 17.53,  val loss 1.739207, percent error 27.40\n",
            "Epoch   109, train loss 1.646322, train error 17.50,  val loss 1.738686, percent error 27.40\n",
            "Epoch   110, train loss 1.646156, train error 17.45,  val loss 1.737798, percent error 27.20\n",
            "Epoch   111, train loss 1.646118, train error 17.43,  val loss 1.737599, percent error 26.80\n",
            "Epoch   112, train loss 1.646006, train error 17.43,  val loss 1.738330, percent error 27.30\n",
            "Epoch   113, train loss 1.645930, train error 17.50,  val loss 1.738658, percent error 27.30\n",
            "Epoch   114, train loss 1.645733, train error 17.43,  val loss 1.738380, percent error 27.30\n",
            "Epoch   115, train loss 1.645720, train error 17.45,  val loss 1.739441, percent error 27.40\n",
            "Epoch   116, train loss 1.645599, train error 17.47,  val loss 1.738113, percent error 27.10\n",
            "Epoch   117, train loss 1.645439, train error 17.43,  val loss 1.738205, percent error 27.30\n",
            "Epoch   118, train loss 1.645355, train error 17.40,  val loss 1.738597, percent error 27.40\n",
            "Epoch   119, train loss 1.645415, train error 17.40,  val loss 1.738549, percent error 27.50\n",
            "Epoch   120, train loss 1.645104, train error 17.45,  val loss 1.738026, percent error 27.20\n",
            "Epoch   121, train loss 1.645055, train error 17.45,  val loss 1.738175, percent error 27.30\n",
            "Epoch   122, train loss 1.645014, train error 17.40,  val loss 1.738198, percent error 27.20\n",
            "Epoch   123, train loss 1.644935, train error 17.40,  val loss 1.738305, percent error 27.20\n",
            "Epoch   124, train loss 1.644879, train error 17.40,  val loss 1.738279, percent error 27.30\n",
            "Epoch   125, train loss 1.644830, train error 17.45,  val loss 1.737985, percent error 27.30\n",
            "Epoch   126, train loss 1.644795, train error 17.38,  val loss 1.738368, percent error 27.50\n",
            "Epoch   127, train loss 1.644766, train error 17.43,  val loss 1.738256, percent error 27.30\n",
            "Epoch   128, train loss 1.644685, train error 17.35,  val loss 1.738924, percent error 27.50\n",
            "Epoch   129, train loss 1.644608, train error 17.40,  val loss 1.738595, percent error 27.30\n",
            "Epoch   130, train loss 1.644579, train error 17.30,  val loss 1.738529, percent error 27.50\n",
            "Epoch   131, train loss 1.644580, train error 17.30,  val loss 1.738499, percent error 27.30\n",
            "Epoch   132, train loss 1.644495, train error 17.35,  val loss 1.738267, percent error 27.50\n",
            "Epoch   133, train loss 1.644388, train error 17.32,  val loss 1.738730, percent error 27.40\n",
            "Epoch   134, train loss 1.644335, train error 17.38,  val loss 1.738687, percent error 27.50\n",
            "Epoch   135, train loss 1.644300, train error 17.28,  val loss 1.738438, percent error 27.40\n",
            "Epoch   136, train loss 1.644216, train error 17.32,  val loss 1.738842, percent error 27.60\n",
            "Epoch   137, train loss 1.644241, train error 17.28,  val loss 1.738532, percent error 27.50\n",
            "Epoch   138, train loss 1.644103, train error 17.32,  val loss 1.738710, percent error 27.60\n",
            "Epoch   139, train loss 1.644067, train error 17.30,  val loss 1.738815, percent error 27.60\n",
            "Epoch   140, train loss 1.644016, train error 17.28,  val loss 1.738798, percent error 27.50\n",
            "Epoch   141, train loss 1.643979, train error 17.30,  val loss 1.738775, percent error 27.50\n",
            "Epoch   142, train loss 1.643956, train error 17.30,  val loss 1.738731, percent error 27.50\n",
            "Epoch   143, train loss 1.643928, train error 17.22,  val loss 1.739093, percent error 27.60\n",
            "Epoch   144, train loss 1.643905, train error 17.30,  val loss 1.738469, percent error 27.50\n",
            "Epoch   145, train loss 1.643873, train error 17.25,  val loss 1.739114, percent error 27.50\n",
            "Epoch   146, train loss 1.643845, train error 17.22,  val loss 1.738737, percent error 27.50\n",
            "Epoch   147, train loss 1.643821, train error 17.28,  val loss 1.739058, percent error 27.70\n",
            "Epoch   148, train loss 1.643806, train error 17.22,  val loss 1.738803, percent error 27.60\n",
            "Epoch   149, train loss 1.643757, train error 17.22,  val loss 1.738867, percent error 27.40\n",
            "Epoch   150, train loss 1.643729, train error 17.22,  val loss 1.738876, percent error 27.60\n",
            "Epoch   151, train loss 1.643702, train error 17.22,  val loss 1.738727, percent error 27.50\n",
            "Epoch   152, train loss 1.643672, train error 17.20,  val loss 1.738968, percent error 27.50\n",
            "Epoch   153, train loss 1.643646, train error 17.22,  val loss 1.738764, percent error 27.50\n",
            "Epoch   154, train loss 1.643624, train error 17.22,  val loss 1.738735, percent error 27.50\n",
            "Epoch   155, train loss 1.643588, train error 17.20,  val loss 1.738798, percent error 27.60\n",
            "Epoch   156, train loss 1.643563, train error 17.20,  val loss 1.738937, percent error 27.60\n",
            "Epoch   157, train loss 1.643544, train error 17.20,  val loss 1.738977, percent error 27.60\n",
            "Epoch   158, train loss 1.643508, train error 17.18,  val loss 1.738941, percent error 27.50\n",
            "Epoch   159, train loss 1.643480, train error 17.18,  val loss 1.738910, percent error 27.50\n",
            "Epoch   160, train loss 1.643463, train error 17.18,  val loss 1.739063, percent error 27.70\n",
            "Epoch   161, train loss 1.643447, train error 17.18,  val loss 1.739092, percent error 27.60\n",
            "Epoch   162, train loss 1.643432, train error 17.20,  val loss 1.738896, percent error 27.60\n",
            "Epoch   163, train loss 1.643419, train error 17.18,  val loss 1.738799, percent error 27.50\n",
            "Epoch   164, train loss 1.643404, train error 17.15,  val loss 1.739028, percent error 27.60\n",
            "Epoch   165, train loss 1.643390, train error 17.15,  val loss 1.738994, percent error 27.70\n",
            "Epoch   166, train loss 1.643378, train error 17.15,  val loss 1.739005, percent error 27.70\n",
            "Epoch   167, train loss 1.643362, train error 17.15,  val loss 1.739002, percent error 27.60\n",
            "Epoch   168, train loss 1.643349, train error 17.15,  val loss 1.739076, percent error 27.60\n",
            "Epoch   169, train loss 1.643334, train error 17.18,  val loss 1.738990, percent error 27.70\n",
            "Epoch   170, train loss 1.643320, train error 17.15,  val loss 1.739010, percent error 27.50\n",
            "Epoch   171, train loss 1.643306, train error 17.15,  val loss 1.739177, percent error 27.60\n",
            "Epoch   172, train loss 1.643292, train error 17.15,  val loss 1.738964, percent error 27.60\n",
            "Epoch   173, train loss 1.643279, train error 17.15,  val loss 1.739033, percent error 27.50\n",
            "Epoch   174, train loss 1.643264, train error 17.18,  val loss 1.738991, percent error 27.70\n",
            "Epoch   175, train loss 1.643250, train error 17.15,  val loss 1.739156, percent error 27.60\n",
            "Epoch   176, train loss 1.643236, train error 17.15,  val loss 1.739210, percent error 27.60\n",
            "Epoch   177, train loss 1.643225, train error 17.15,  val loss 1.738954, percent error 27.60\n",
            "Epoch   178, train loss 1.643207, train error 17.15,  val loss 1.738910, percent error 27.60\n",
            "Epoch   179, train loss 1.643193, train error 17.15,  val loss 1.739105, percent error 27.70\n",
            "Epoch   180, train loss 1.643185, train error 17.15,  val loss 1.739140, percent error 27.60\n",
            "Epoch   181, train loss 1.643177, train error 17.15,  val loss 1.739112, percent error 27.60\n",
            "Epoch   182, train loss 1.643171, train error 17.15,  val loss 1.739033, percent error 27.70\n",
            "Epoch   183, train loss 1.643164, train error 17.15,  val loss 1.739082, percent error 27.70\n",
            "Epoch   184, train loss 1.643157, train error 17.15,  val loss 1.739112, percent error 27.70\n",
            "Epoch   185, train loss 1.643151, train error 17.15,  val loss 1.739073, percent error 27.70\n",
            "Epoch   186, train loss 1.643142, train error 17.15,  val loss 1.739060, percent error 27.60\n",
            "Epoch   187, train loss 1.643135, train error 17.15,  val loss 1.739114, percent error 27.60\n",
            "Epoch   188, train loss 1.643128, train error 17.15,  val loss 1.739079, percent error 27.60\n",
            "Epoch   189, train loss 1.643121, train error 17.15,  val loss 1.739115, percent error 27.60\n",
            "Epoch   190, train loss 1.643114, train error 17.15,  val loss 1.739018, percent error 27.70\n",
            "Epoch   191, train loss 1.643107, train error 17.15,  val loss 1.739075, percent error 27.70\n",
            "Epoch   192, train loss 1.643101, train error 17.15,  val loss 1.739202, percent error 27.60\n",
            "Epoch   193, train loss 1.643093, train error 17.15,  val loss 1.739131, percent error 27.60\n",
            "Epoch   194, train loss 1.643085, train error 17.15,  val loss 1.739097, percent error 27.60\n",
            "Epoch   195, train loss 1.643080, train error 17.15,  val loss 1.739070, percent error 27.70\n",
            "Epoch   196, train loss 1.643072, train error 17.15,  val loss 1.739066, percent error 27.60\n",
            "Epoch   197, train loss 1.643065, train error 17.15,  val loss 1.739122, percent error 27.60\n",
            "Epoch   198, train loss 1.643058, train error 17.15,  val loss 1.739100, percent error 27.60\n",
            "Epoch   199, train loss 1.643051, train error 17.15,  val loss 1.739124, percent error 27.60\n",
            "Epoch   200, train loss 1.643047, train error 17.15,  val loss 1.739124, percent error 27.60\n",
            "Epoch   201, train loss 1.643044, train error 17.15,  val loss 1.739077, percent error 27.70\n",
            "Epoch   202, train loss 1.643040, train error 17.15,  val loss 1.739134, percent error 27.60\n",
            "Epoch   203, train loss 1.643036, train error 17.15,  val loss 1.739156, percent error 27.60\n",
            "Epoch   204, train loss 1.643033, train error 17.15,  val loss 1.739110, percent error 27.60\n",
            "Epoch   205, train loss 1.643029, train error 17.15,  val loss 1.739136, percent error 27.60\n",
            "Epoch   206, train loss 1.643026, train error 17.15,  val loss 1.739106, percent error 27.60\n",
            "Epoch   207, train loss 1.643022, train error 17.15,  val loss 1.739068, percent error 27.60\n",
            "Epoch   208, train loss 1.643019, train error 17.15,  val loss 1.739097, percent error 27.60\n",
            "Epoch   209, train loss 1.643015, train error 17.15,  val loss 1.739094, percent error 27.60\n",
            "Epoch   210, train loss 1.643012, train error 17.15,  val loss 1.739152, percent error 27.60\n",
            "Epoch   211, train loss 1.643008, train error 17.15,  val loss 1.739136, percent error 27.60\n",
            "Epoch   212, train loss 1.643005, train error 17.15,  val loss 1.739141, percent error 27.60\n",
            "Epoch   213, train loss 1.643001, train error 17.15,  val loss 1.739109, percent error 27.60\n",
            "Epoch   214, train loss 1.642998, train error 17.15,  val loss 1.739114, percent error 27.60\n",
            "Epoch   215, train loss 1.642994, train error 17.15,  val loss 1.739070, percent error 27.60\n",
            "Epoch   216, train loss 1.642990, train error 17.12,  val loss 1.739144, percent error 27.60\n",
            "Epoch   217, train loss 1.642987, train error 17.15,  val loss 1.739126, percent error 27.60\n",
            "Epoch   218, train loss 1.642983, train error 17.12,  val loss 1.739142, percent error 27.60\n",
            "Epoch   219, train loss 1.642980, train error 17.15,  val loss 1.739079, percent error 27.60\n",
            "Epoch   220, train loss 1.642978, train error 17.15,  val loss 1.739094, percent error 27.60\n",
            "Epoch   221, train loss 1.642976, train error 17.15,  val loss 1.739121, percent error 27.60\n",
            "Epoch   222, train loss 1.642974, train error 17.15,  val loss 1.739097, percent error 27.60\n",
            "Epoch   223, train loss 1.642972, train error 17.15,  val loss 1.739108, percent error 27.60\n",
            "Epoch   224, train loss 1.642971, train error 17.15,  val loss 1.739103, percent error 27.60\n",
            "Epoch   225, train loss 1.642969, train error 17.15,  val loss 1.739125, percent error 27.60\n",
            "Epoch   226, train loss 1.642967, train error 17.15,  val loss 1.739109, percent error 27.60\n",
            "Epoch   227, train loss 1.642965, train error 17.12,  val loss 1.739137, percent error 27.60\n",
            "Epoch   228, train loss 1.642964, train error 17.15,  val loss 1.739119, percent error 27.60\n",
            "Epoch   229, train loss 1.642962, train error 17.15,  val loss 1.739099, percent error 27.60\n",
            "Epoch   230, train loss 1.642960, train error 17.15,  val loss 1.739104, percent error 27.60\n",
            "Epoch   231, train loss 1.642959, train error 17.15,  val loss 1.739110, percent error 27.60\n",
            "Epoch   232, train loss 1.642957, train error 17.12,  val loss 1.739121, percent error 27.60\n",
            "Epoch   233, train loss 1.642955, train error 17.15,  val loss 1.739110, percent error 27.60\n",
            "Epoch   234, train loss 1.642953, train error 17.12,  val loss 1.739104, percent error 27.60\n",
            "Epoch   235, train loss 1.642951, train error 17.12,  val loss 1.739120, percent error 27.60\n",
            "Epoch   236, train loss 1.642950, train error 17.12,  val loss 1.739115, percent error 27.60\n",
            "Epoch   237, train loss 1.642948, train error 17.12,  val loss 1.739133, percent error 27.60\n",
            "Epoch   238, train loss 1.642946, train error 17.12,  val loss 1.739109, percent error 27.60\n",
            "Epoch   239, train loss 1.642944, train error 17.10,  val loss 1.739113, percent error 27.60\n",
            "Epoch   240, train loss 1.642943, train error 17.10,  val loss 1.739120, percent error 27.60\n",
            "Epoch   241, train loss 1.642943, train error 17.12,  val loss 1.739125, percent error 27.60\n",
            "Epoch   242, train loss 1.642942, train error 17.10,  val loss 1.739109, percent error 27.60\n",
            "Epoch   243, train loss 1.642941, train error 17.10,  val loss 1.739117, percent error 27.60\n",
            "Epoch   244, train loss 1.642940, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   245, train loss 1.642939, train error 17.10,  val loss 1.739118, percent error 27.60\n",
            "Epoch   246, train loss 1.642938, train error 17.10,  val loss 1.739130, percent error 27.60\n",
            "Epoch   247, train loss 1.642937, train error 17.10,  val loss 1.739123, percent error 27.60\n",
            "Epoch   248, train loss 1.642936, train error 17.10,  val loss 1.739118, percent error 27.60\n",
            "Epoch   249, train loss 1.642935, train error 17.10,  val loss 1.739127, percent error 27.60\n",
            "Epoch   250, train loss 1.642935, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   251, train loss 1.642934, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   252, train loss 1.642933, train error 17.10,  val loss 1.739116, percent error 27.60\n",
            "Epoch   253, train loss 1.642932, train error 17.10,  val loss 1.739114, percent error 27.60\n",
            "Epoch   254, train loss 1.642931, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   255, train loss 1.642930, train error 17.10,  val loss 1.739119, percent error 27.60\n",
            "Epoch   256, train loss 1.642929, train error 17.10,  val loss 1.739119, percent error 27.60\n",
            "Epoch   257, train loss 1.642928, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   258, train loss 1.642928, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   259, train loss 1.642927, train error 17.10,  val loss 1.739124, percent error 27.60\n",
            "Epoch   260, train loss 1.642926, train error 17.10,  val loss 1.739122, percent error 27.60\n",
            "Epoch   261, train loss 1.642926, train error 17.10,  val loss 1.739124, percent error 27.60\n",
            "Epoch   262, train loss 1.642925, train error 17.10,  val loss 1.739123, percent error 27.60\n",
            "Epoch   263, train loss 1.642925, train error 17.10,  val loss 1.739122, percent error 27.60\n",
            "Epoch   264, train loss 1.642924, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   265, train loss 1.642924, train error 17.10,  val loss 1.739121, percent error 27.60\n",
            "Epoch   266, train loss 1.642923, train error 17.10,  val loss 1.739127, percent error 27.60\n",
            "Epoch   267, train loss 1.642923, train error 17.10,  val loss 1.739122, percent error 27.60\n",
            "Epoch   268, train loss 1.642923, train error 17.10,  val loss 1.739122, percent error 27.60\n",
            "Epoch   269, train loss 1.642922, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   270, train loss 1.642922, train error 17.10,  val loss 1.739123, percent error 27.60\n",
            "Epoch   271, train loss 1.642921, train error 17.10,  val loss 1.739128, percent error 27.60\n",
            "Epoch   272, train loss 1.642921, train error 17.10,  val loss 1.739124, percent error 27.60\n",
            "Epoch   273, train loss 1.642920, train error 17.10,  val loss 1.739122, percent error 27.60\n",
            "Epoch   274, train loss 1.642920, train error 17.10,  val loss 1.739120, percent error 27.60\n",
            "Epoch   275, train loss 1.642920, train error 17.10,  val loss 1.739123, percent error 27.60\n",
            "Epoch   276, train loss 1.642919, train error 17.10,  val loss 1.739124, percent error 27.60\n",
            "Epoch   277, train loss 1.642919, train error 17.10,  val loss 1.739123, percent error 27.60\n",
            "Epoch   278, train loss 1.642918, train error 17.10,  val loss 1.739120, percent error 27.60\n",
            "Epoch   279, train loss 1.642918, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   280, train loss 1.642918, train error 17.10,  val loss 1.739127, percent error 27.60\n",
            "Epoch   281, train loss 1.642917, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   282, train loss 1.642917, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   283, train loss 1.642917, train error 17.10,  val loss 1.739127, percent error 27.60\n",
            "Epoch   284, train loss 1.642917, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   285, train loss 1.642917, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   286, train loss 1.642916, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   287, train loss 1.642916, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   288, train loss 1.642916, train error 17.10,  val loss 1.739124, percent error 27.60\n",
            "Epoch   289, train loss 1.642915, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   290, train loss 1.642915, train error 17.10,  val loss 1.739127, percent error 27.60\n",
            "Epoch   291, train loss 1.642915, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   292, train loss 1.642915, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   293, train loss 1.642915, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   294, train loss 1.642914, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   295, train loss 1.642914, train error 17.10,  val loss 1.739128, percent error 27.60\n",
            "Epoch   296, train loss 1.642914, train error 17.10,  val loss 1.739129, percent error 27.60\n",
            "Epoch   297, train loss 1.642914, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   298, train loss 1.642914, train error 17.10,  val loss 1.739128, percent error 27.60\n",
            "Epoch   299, train loss 1.642913, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   300, train loss 1.642913, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   301, train loss 1.642913, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   302, train loss 1.642913, train error 17.10,  val loss 1.739127, percent error 27.60\n",
            "Epoch   303, train loss 1.642913, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   304, train loss 1.642913, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   305, train loss 1.642913, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   306, train loss 1.642913, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   307, train loss 1.642913, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   308, train loss 1.642912, train error 17.10,  val loss 1.739127, percent error 27.60\n",
            "Epoch   309, train loss 1.642912, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   310, train loss 1.642912, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   311, train loss 1.642912, train error 17.10,  val loss 1.739127, percent error 27.60\n",
            "Epoch   312, train loss 1.642912, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   313, train loss 1.642912, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   314, train loss 1.642912, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   315, train loss 1.642912, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   316, train loss 1.642912, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   317, train loss 1.642912, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   318, train loss 1.642912, train error 17.10,  val loss 1.739127, percent error 27.60\n",
            "Epoch   319, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   320, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   321, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   322, train loss 1.642911, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   323, train loss 1.642912, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   324, train loss 1.642911, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   325, train loss 1.642911, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   326, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   327, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   328, train loss 1.642911, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   329, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   330, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   331, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   332, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   333, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   334, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   335, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   336, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   337, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   338, train loss 1.642911, train error 17.10,  val loss 1.739125, percent error 27.60\n",
            "Epoch   339, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   340, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   341, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   342, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   343, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   344, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   345, train loss 1.642911, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   346, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   347, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   348, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   349, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   350, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   351, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   352, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   353, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   354, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   355, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   356, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   357, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   358, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   359, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   360, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   361, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   362, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   363, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   364, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   365, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   366, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   367, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   368, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   369, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   370, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   371, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   372, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   373, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   374, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   375, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   376, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   377, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   378, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   379, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   380, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   381, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   382, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   383, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   384, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   385, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   386, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   387, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   388, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   389, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   390, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   391, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   392, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   393, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   394, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   395, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   396, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   397, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   398, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   399, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   400, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   401, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   402, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   403, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   404, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   405, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   406, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   407, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   408, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   409, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   410, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   411, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   412, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   413, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   414, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   415, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   416, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   417, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   418, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   419, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   420, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   421, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   422, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   423, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   424, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   425, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   426, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   427, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   428, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   429, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   430, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   431, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   432, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   433, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   434, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   435, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   436, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   437, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   438, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   439, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   440, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   441, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   442, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   443, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   444, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   445, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   446, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   447, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   448, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   449, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   450, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   451, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   452, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   453, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   454, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   455, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   456, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   457, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   458, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   459, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   460, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   461, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   462, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   463, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   464, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   465, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   466, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   467, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   468, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   469, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   470, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   471, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   472, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   473, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   474, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   475, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   476, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   477, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   478, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   479, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   480, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   481, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   482, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   483, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   484, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   485, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   486, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   487, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   488, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   489, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   490, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   491, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   492, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   493, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   494, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   495, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   496, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   497, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   498, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   499, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   500, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   501, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   502, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   503, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   504, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   505, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   506, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   507, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   508, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   509, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   510, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   511, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   512, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   513, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   514, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   515, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   516, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   517, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   518, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   519, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   520, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   521, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   522, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   523, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   524, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   525, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   526, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   527, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   528, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   529, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   530, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   531, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   532, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   533, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   534, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   535, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   536, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   537, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   538, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   539, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   540, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   541, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   542, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   543, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   544, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   545, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   546, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   547, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   548, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   549, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   550, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   551, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   552, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   553, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   554, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   555, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   556, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   557, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   558, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   559, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   560, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   561, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   562, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   563, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   564, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   565, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   566, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   567, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   568, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   569, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   570, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   571, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   572, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   573, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   574, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   575, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   576, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   577, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   578, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   579, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   580, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   581, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   582, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   583, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   584, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   585, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   586, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   587, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   588, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   589, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   590, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   591, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   592, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   593, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   594, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   595, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   596, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   597, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   598, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   599, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   600, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   601, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   602, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   603, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   604, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   605, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   606, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   607, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   608, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   609, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   610, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   611, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   612, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   613, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   614, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   615, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   616, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   617, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   618, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   619, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   620, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   621, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   622, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   623, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   624, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   625, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   626, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   627, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   628, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   629, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   630, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   631, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   632, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   633, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   634, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   635, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   636, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   637, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   638, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   639, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   640, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   641, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   642, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   643, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   644, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   645, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   646, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   647, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   648, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   649, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   650, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   651, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   652, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   653, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   654, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   655, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   656, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   657, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   658, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   659, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   660, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   661, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   662, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   663, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   664, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   665, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   666, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   667, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   668, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   669, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   670, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   671, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   672, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   673, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   674, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   675, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   676, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   677, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   678, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   679, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   680, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   681, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   682, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   683, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   684, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   685, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   686, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   687, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   688, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   689, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   690, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   691, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   692, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   693, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   694, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   695, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   696, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   697, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   698, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   699, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   700, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   701, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   702, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   703, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   704, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   705, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   706, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   707, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   708, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   709, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   710, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   711, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   712, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   713, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   714, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   715, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   716, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   717, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   718, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   719, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   720, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   721, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   722, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   723, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   724, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   725, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   726, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   727, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   728, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   729, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   730, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   731, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   732, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   733, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   734, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   735, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   736, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   737, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   738, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   739, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   740, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   741, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   742, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   743, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   744, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   745, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   746, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   747, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   748, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   749, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   750, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   751, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   752, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   753, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   754, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   755, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   756, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   757, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   758, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   759, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   760, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   761, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   762, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   763, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   764, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   765, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   766, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   767, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   768, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   769, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   770, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   771, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   772, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   773, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   774, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   775, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   776, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   777, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   778, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   779, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   780, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   781, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   782, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   783, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   784, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   785, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   786, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   787, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   788, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   789, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   790, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   791, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   792, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   793, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   794, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   795, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   796, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   797, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   798, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   799, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   800, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   801, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   802, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   803, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   804, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   805, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   806, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   807, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   808, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   809, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   810, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   811, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   812, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   813, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   814, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   815, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   816, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   817, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   818, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   819, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   820, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   821, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   822, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   823, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   824, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   825, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   826, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   827, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   828, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   829, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   830, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   831, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   832, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   833, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   834, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   835, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   836, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   837, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   838, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   839, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   840, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   841, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   842, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   843, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   844, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   845, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   846, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   847, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   848, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   849, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   850, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   851, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   852, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   853, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   854, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   855, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   856, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   857, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   858, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   859, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   860, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   861, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   862, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   863, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   864, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   865, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   866, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   867, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   868, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   869, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   870, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   871, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   872, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   873, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   874, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   875, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   876, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   877, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   878, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   879, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   880, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   881, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   882, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   883, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   884, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   885, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   886, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   887, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   888, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   889, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   890, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   891, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   892, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   893, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   894, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   895, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   896, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   897, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   898, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   899, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   900, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   901, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   902, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   903, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   904, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   905, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   906, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   907, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   908, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   909, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   910, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   911, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   912, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   913, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   914, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   915, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   916, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   917, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   918, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   919, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   920, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   921, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   922, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   923, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   924, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   925, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   926, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   927, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   928, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   929, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   930, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   931, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   932, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   933, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   934, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   935, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   936, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   937, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   938, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   939, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   940, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   941, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   942, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   943, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   944, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   945, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   946, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   947, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   948, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   949, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   950, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   951, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   952, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   953, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   954, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   955, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   956, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   957, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   958, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   959, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   960, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   961, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   962, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   963, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   964, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   965, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   966, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   967, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   968, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   969, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   970, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   971, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   972, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   973, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   974, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   975, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   976, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   977, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   978, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   979, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   980, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   981, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   982, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   983, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   984, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   985, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   986, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   987, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   988, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   989, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   990, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   991, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   992, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   993, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   994, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   995, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   996, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   997, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   998, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch   999, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1000, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1001, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1002, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1003, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1004, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1005, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1006, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1007, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1008, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1009, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1010, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1011, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1012, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1013, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1014, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1015, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1016, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1017, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1018, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1019, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1020, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1021, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1022, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1023, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1024, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1025, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1026, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1027, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1028, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1029, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1030, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1031, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1032, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1033, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1034, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1035, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1036, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1037, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1038, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1039, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1040, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1041, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1042, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1043, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1044, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1045, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1046, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1047, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1048, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1049, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1050, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1051, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1052, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1053, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1054, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1055, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1056, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1057, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1058, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1059, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1060, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1061, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1062, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1063, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1064, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1065, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1066, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1067, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1068, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1069, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1070, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1071, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1072, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1073, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1074, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1075, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1076, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1077, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1078, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1079, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1080, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1081, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1082, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1083, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1084, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1085, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1086, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1087, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1088, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1089, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1090, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1091, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1092, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1093, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1094, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1095, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1096, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1097, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1098, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1099, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1100, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1101, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1102, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1103, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1104, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1105, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1106, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1107, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1108, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1109, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1110, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1111, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1112, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1113, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1114, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1115, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1116, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1117, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1118, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1119, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1120, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1121, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1122, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1123, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1124, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1125, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1126, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1127, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1128, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1129, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1130, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1131, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1132, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1133, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1134, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1135, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1136, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1137, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1138, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1139, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1140, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1141, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1142, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1143, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1144, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1145, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1146, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1147, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1148, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1149, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1150, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1151, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1152, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1153, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1154, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1155, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1156, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1157, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1158, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1159, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1160, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1161, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1162, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1163, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1164, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1165, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1166, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1167, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1168, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1169, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1170, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1171, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1172, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1173, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1174, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1175, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1176, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1177, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1178, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1179, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1180, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1181, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1182, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1183, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1184, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1185, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1186, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1187, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1188, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1189, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1190, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1191, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1192, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1193, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1194, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1195, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1196, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1197, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1198, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1199, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1200, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1201, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1202, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1203, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1204, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1205, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1206, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1207, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1208, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1209, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1210, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1211, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1212, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1213, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1214, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1215, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1216, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1217, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1218, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1219, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1220, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1221, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1222, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1223, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1224, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1225, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1226, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1227, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1228, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1229, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1230, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1231, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1232, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1233, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1234, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1235, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1236, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1237, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1238, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1239, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1240, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1241, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1242, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1243, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1244, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1245, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1246, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1247, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1248, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1249, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1250, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1251, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1252, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1253, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1254, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1255, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1256, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1257, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1258, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1259, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1260, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1261, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1262, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1263, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1264, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1265, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1266, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1267, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1268, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1269, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1270, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1271, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1272, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1273, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1274, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1275, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1276, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1277, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1278, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1279, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1280, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1281, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1282, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1283, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1284, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1285, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1286, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1287, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1288, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1289, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1290, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1291, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1292, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1293, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1294, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1295, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1296, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1297, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1298, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1299, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1300, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1301, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1302, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1303, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1304, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1305, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1306, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1307, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1308, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1309, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1310, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1311, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1312, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1313, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1314, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1315, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1316, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1317, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1318, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1319, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1320, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1321, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1322, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1323, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1324, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1325, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1326, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1327, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1328, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1329, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1330, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1331, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1332, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1333, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1334, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1335, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1336, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1337, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1338, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1339, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1340, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1341, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1342, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1343, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1344, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1345, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1346, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1347, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1348, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1349, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1350, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1351, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1352, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1353, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1354, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1355, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1356, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1357, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1358, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1359, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1360, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1361, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1362, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1363, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1364, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1365, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1366, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1367, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1368, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1369, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1370, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1371, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1372, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1373, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1374, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1375, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1376, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1377, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1378, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1379, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1380, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1381, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1382, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1383, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1384, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1385, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1386, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1387, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1388, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1389, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1390, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1391, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1392, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1393, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1394, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1395, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1396, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1397, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1398, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1399, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1400, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1401, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1402, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1403, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1404, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1405, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1406, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1407, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1408, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1409, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1410, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1411, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1412, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1413, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1414, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1415, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1416, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1417, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1418, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1419, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1420, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1421, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1422, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1423, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1424, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1425, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1426, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1427, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1428, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1429, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1430, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1431, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1432, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1433, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1434, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1435, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1436, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1437, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1438, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1439, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1440, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1441, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1442, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1443, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1444, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1445, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1446, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1447, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1448, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1449, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1450, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1451, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1452, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1453, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1454, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1455, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1456, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1457, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1458, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1459, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1460, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1461, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1462, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1463, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1464, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1465, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1466, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1467, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1468, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1469, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1470, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1471, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1472, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1473, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1474, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1475, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1476, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1477, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1478, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1479, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1480, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1481, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1482, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1483, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1484, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1485, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1486, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1487, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1488, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1489, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1490, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1491, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1492, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1493, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1494, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1495, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1496, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1497, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1498, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1499, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1500, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1501, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1502, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1503, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1504, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1505, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1506, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1507, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1508, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1509, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1510, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1511, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1512, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1513, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1514, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1515, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1516, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1517, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1518, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1519, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1520, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1521, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1522, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1523, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1524, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1525, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1526, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1527, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1528, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1529, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1530, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1531, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1532, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1533, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1534, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1535, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1536, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1537, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1538, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1539, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1540, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1541, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1542, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1543, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1544, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1545, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1546, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1547, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1548, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1549, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1550, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1551, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1552, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1553, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1554, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1555, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1556, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1557, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1558, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1559, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1560, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1561, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1562, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1563, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1564, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1565, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1566, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1567, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1568, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1569, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1570, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1571, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1572, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1573, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1574, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1575, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1576, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1577, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1578, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1579, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1580, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1581, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1582, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1583, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1584, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1585, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1586, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1587, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1588, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1589, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1590, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1591, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1592, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1593, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1594, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1595, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1596, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1597, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1598, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1599, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1600, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1601, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1602, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1603, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1604, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1605, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1606, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1607, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1608, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1609, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1610, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1611, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1612, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1613, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1614, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1615, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1616, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1617, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1618, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1619, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1620, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1621, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1622, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1623, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1624, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1625, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1626, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1627, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1628, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1629, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1630, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1631, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1632, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1633, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1634, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1635, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1636, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1637, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1638, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1639, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1640, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1641, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1642, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1643, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1644, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1645, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1646, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1647, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1648, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1649, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1650, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1651, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1652, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1653, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1654, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1655, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1656, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1657, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1658, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1659, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1660, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1661, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1662, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1663, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1664, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1665, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1666, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1667, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1668, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1669, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1670, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1671, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1672, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1673, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1674, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1675, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1676, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1677, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1678, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1679, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1680, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1681, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1682, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1683, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1684, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1685, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1686, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1687, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1688, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1689, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1690, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1691, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1692, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1693, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1694, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1695, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1696, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1697, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1698, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1699, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1700, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1701, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1702, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1703, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1704, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1705, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1706, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1707, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1708, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1709, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1710, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1711, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1712, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1713, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1714, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1715, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1716, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1717, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1718, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1719, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1720, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1721, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1722, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1723, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1724, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1725, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1726, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1727, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1728, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1729, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1730, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1731, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1732, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1733, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1734, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1735, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1736, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1737, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1738, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1739, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1740, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1741, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1742, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1743, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1744, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1745, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1746, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1747, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1748, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1749, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1750, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1751, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1752, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1753, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1754, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1755, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1756, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1757, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1758, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1759, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1760, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1761, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1762, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1763, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1764, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1765, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1766, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1767, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1768, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1769, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1770, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1771, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1772, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1773, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1774, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1775, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1776, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1777, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1778, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1779, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1780, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1781, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1782, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1783, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1784, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1785, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1786, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1787, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1788, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1789, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1790, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1791, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1792, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1793, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1794, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1795, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1796, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1797, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1798, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1799, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1800, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1801, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1802, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1803, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1804, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1805, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1806, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1807, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1808, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1809, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1810, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1811, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1812, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1813, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1814, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1815, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1816, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1817, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1818, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1819, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1820, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1821, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1822, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1823, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1824, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1825, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1826, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1827, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1828, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1829, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1830, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1831, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1832, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1833, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1834, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1835, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1836, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1837, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1838, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1839, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1840, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1841, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1842, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1843, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1844, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1845, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1846, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1847, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1848, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1849, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1850, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1851, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1852, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1853, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1854, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1855, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1856, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1857, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1858, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1859, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1860, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1861, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1862, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1863, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1864, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1865, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1866, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1867, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1868, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1869, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1870, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1871, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1872, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1873, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1874, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1875, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1876, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1877, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1878, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1879, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1880, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1881, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1882, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1883, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1884, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1885, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1886, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1887, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1888, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1889, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1890, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1891, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1892, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1893, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1894, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1895, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1896, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1897, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1898, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1899, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1900, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1901, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1902, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1903, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1904, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1905, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1906, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1907, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1908, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1909, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1910, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1911, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1912, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1913, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1914, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1915, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1916, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1917, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1918, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1919, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1920, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1921, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1922, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1923, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1924, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1925, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1926, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1927, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1928, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1929, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1930, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1931, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1932, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1933, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1934, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1935, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1936, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1937, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1938, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1939, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1940, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1941, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1942, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1943, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1944, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1945, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1946, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1947, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1948, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1949, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1950, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1951, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1952, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1953, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1954, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1955, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1956, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1957, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1958, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1959, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1960, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1961, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1962, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1963, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1964, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1965, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1966, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1967, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1968, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1969, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1970, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1971, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1972, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1973, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1974, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1975, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1976, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1977, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1978, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1979, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1980, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1981, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1982, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1983, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1984, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1985, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1986, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1987, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1988, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1989, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1990, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1991, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1992, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1993, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1994, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1995, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1996, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1997, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1998, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  1999, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2000, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2001, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2002, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2003, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2004, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2005, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2006, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2007, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2008, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2009, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2010, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2011, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2012, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2013, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2014, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2015, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2016, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2017, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2018, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2019, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2020, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2021, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2022, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2023, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2024, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2025, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2026, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2027, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2028, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2029, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2030, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2031, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2032, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2033, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2034, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2035, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2036, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2037, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2038, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2039, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2040, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2041, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2042, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2043, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2044, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2045, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2046, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2047, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2048, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2049, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2050, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2051, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2052, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2053, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2054, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2055, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2056, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2057, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2058, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2059, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2060, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2061, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2062, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2063, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2064, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2065, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2066, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2067, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2068, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2069, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2070, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2071, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2072, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2073, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2074, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2075, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2076, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2077, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2078, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2079, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2080, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2081, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2082, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2083, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2084, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2085, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2086, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2087, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2088, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2089, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2090, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2091, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2092, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2093, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2094, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2095, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2096, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2097, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2098, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2099, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2100, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2101, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2102, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2103, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2104, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2105, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2106, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2107, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2108, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2109, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2110, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2111, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2112, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2113, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2114, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2115, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2116, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2117, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2118, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2119, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2120, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2121, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2122, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2123, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2124, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2125, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2126, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2127, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2128, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2129, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2130, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2131, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2132, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2133, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2134, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2135, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2136, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2137, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2138, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2139, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2140, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2141, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2142, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2143, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2144, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2145, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2146, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2147, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2148, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2149, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2150, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2151, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2152, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2153, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2154, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2155, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2156, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2157, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2158, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2159, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2160, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2161, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2162, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2163, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2164, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2165, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2166, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2167, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2168, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2169, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2170, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2171, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2172, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2173, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2174, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2175, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2176, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2177, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2178, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2179, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2180, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2181, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2182, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2183, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2184, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2185, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2186, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2187, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2188, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2189, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2190, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2191, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2192, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2193, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2194, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2195, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2196, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2197, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2198, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2199, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2200, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2201, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2202, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2203, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2204, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2205, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2206, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2207, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2208, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2209, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2210, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2211, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2212, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2213, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2214, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2215, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2216, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2217, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2218, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2219, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2220, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2221, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2222, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2223, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2224, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2225, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2226, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2227, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2228, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2229, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2230, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2231, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2232, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2233, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2234, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2235, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2236, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2237, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2238, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2239, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2240, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2241, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2242, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2243, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2244, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2245, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2246, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2247, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2248, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2249, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2250, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2251, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2252, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2253, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2254, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2255, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2256, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2257, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2258, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2259, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2260, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2261, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2262, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2263, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2264, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2265, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2266, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2267, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2268, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2269, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2270, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2271, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2272, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2273, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2274, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2275, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2276, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2277, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2278, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2279, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2280, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2281, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2282, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2283, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2284, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2285, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2286, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2287, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2288, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2289, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2290, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2291, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2292, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2293, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2294, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2295, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2296, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2297, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2298, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2299, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2300, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2301, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2302, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2303, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2304, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2305, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2306, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2307, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2308, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2309, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2310, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2311, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2312, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2313, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2314, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2315, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2316, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2317, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2318, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2319, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2320, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2321, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2322, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2323, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2324, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2325, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2326, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2327, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2328, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2329, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2330, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2331, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2332, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2333, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2334, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2335, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2336, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2337, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2338, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2339, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2340, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2341, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2342, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2343, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2344, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2345, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2346, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2347, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2348, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2349, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2350, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2351, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2352, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2353, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2354, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2355, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2356, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2357, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2358, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2359, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2360, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2361, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2362, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2363, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2364, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2365, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2366, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2367, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2368, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2369, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2370, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2371, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2372, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2373, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2374, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2375, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2376, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2377, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2378, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2379, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2380, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2381, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2382, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2383, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2384, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2385, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2386, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2387, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2388, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2389, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2390, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2391, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2392, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2393, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2394, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2395, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2396, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2397, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2398, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2399, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2400, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2401, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2402, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2403, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2404, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2405, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2406, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2407, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2408, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2409, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2410, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2411, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2412, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2413, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2414, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2415, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2416, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2417, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2418, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2419, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2420, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2421, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2422, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2423, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2424, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2425, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2426, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2427, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2428, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2429, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2430, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2431, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2432, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2433, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2434, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2435, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2436, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2437, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2438, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2439, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2440, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2441, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2442, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2443, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2444, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2445, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2446, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2447, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2448, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2449, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2450, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2451, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2452, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2453, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2454, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2455, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2456, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2457, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2458, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2459, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2460, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2461, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2462, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2463, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2464, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2465, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2466, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2467, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2468, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2469, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2470, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2471, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2472, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2473, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2474, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2475, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2476, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2477, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2478, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2479, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2480, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2481, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2482, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2483, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2484, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2485, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2486, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2487, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2488, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2489, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2490, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2491, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2492, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2493, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2494, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2495, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2496, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2497, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2498, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n",
            "Epoch  2499, train loss 1.642910, train error 17.10,  val loss 1.739126, percent error 27.60\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = copy.deepcopy(model_c)\n",
        "\n",
        "# choose cross entropy loss function (equation 5.24 in the loss notes)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "# construct SGD optimizer and initialize learning rate and momentum\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.08, momentum=0.9)\n",
        "# object that decreases learning rate by half every 20 epochs\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "# create 100 dummy data points and store in data loader class\n",
        "\n",
        "x_train_c = torch.tensor(data['x'].astype('float32'))\n",
        "y_train_c = torch.tensor(data['y'].transpose().astype('int64'))\n",
        "x_val_c= torch.tensor(data['x_test'].astype('float32'))\n",
        "y_val_c = torch.tensor(data['y_test'].astype('int64'))\n",
        "\n",
        "# load the data into a class that creates the batches\n",
        "data_loader = DataLoader(TensorDataset(x_train_c,y_train_c), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
        "\n",
        "# Initialize model weights\n",
        "model.apply(weights_init)\n",
        "\n",
        "# loop over the dataset n_epoch times\n",
        "n_epoch = 2500\n",
        "# store the loss and the % correct at each epoch\n",
        "losses_train_c = np.zeros((n_epoch))\n",
        "errors_train_c = np.zeros((n_epoch))\n",
        "losses_val_c = np.zeros((n_epoch))\n",
        "errors_val_c = np.zeros((n_epoch))\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # loop over batches\n",
        "  for i, batch in enumerate(data_loader):\n",
        "    # retrieve inputs and labels for this batch\n",
        "    x_batch, y_batch = batch\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass -- calculate model output\n",
        "    pred = model(x_batch[:,None,:])\n",
        "    # compute the loss\n",
        "    loss = loss_function(pred, y_batch)\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    # SGD update\n",
        "    optimizer.step()\n",
        "\n",
        "  # Run whole dataset to get statistics -- normally wouldn't do this\n",
        "  pred_train = model(x_train_c[:,None,:])\n",
        "  pred_val = model(x_val_c[:,None,:])\n",
        "  _, predicted_train_class = torch.max(pred_train.data, 1)\n",
        "  _, predicted_val_class = torch.max(pred_val.data, 1)\n",
        "  errors_train_c[epoch] = 100 - 100 * (predicted_train_class == y_train_c).float().sum() / len(y_train_c)\n",
        "  errors_val_c[epoch]= 100 - 100 * (predicted_val_class == y_val_c).float().sum() / len(y_val_c)\n",
        "  losses_train_c[epoch] = loss_function(pred_train, y_train_c).item()\n",
        "  losses_val_c[epoch]= loss_function(pred_val, y_val_c).item()\n",
        "  print(f'Epoch {epoch:5d}, train loss {losses_train_c[epoch]:.6f}, train error {errors_train_c[epoch]:3.2f},  val loss {losses_val_c[epoch]:.6f}, percent error {errors_val_c[epoch]:3.2f}')\n",
        "\n",
        "  # tell scheduler to consider updating learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(errors_train_c,'r-',label='train')\n",
        "ax.plot(errors_val_c,'b-',label='validation')\n",
        "ax.set_ylim(0,100); ax.set_xlim(0,n_epoch)\n",
        "ax.set_xlabel('Epoch'); ax.set_ylabel('Error')\n",
        "ax.set_title('Part I: Validation Result %3.2f'%(errors_val_c[-1]))\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch     0, train loss 2.262056, train error 85.35,  val loss 2.267649, percent error 85.50\n",
            "Epoch     1, train loss 2.186344, train error 77.05,  val loss 2.198497, percent error 78.50\n",
            "Epoch     2, train loss 2.143208, train error 69.50,  val loss 2.166749, percent error 73.30\n",
            "Epoch     3, train loss 2.101712, train error 64.28,  val loss 2.127970, percent error 67.00\n",
            "Epoch     4, train loss 2.055028, train error 58.00,  val loss 2.090035, percent error 62.50\n",
            "Epoch     5, train loss 2.036433, train error 58.22,  val loss 2.070204, percent error 62.70\n",
            "Epoch     6, train loss 2.020017, train error 54.50,  val loss 2.061423, percent error 59.80\n",
            "Epoch     7, train loss 2.002380, train error 53.35,  val loss 2.044403, percent error 58.10\n",
            "Epoch     8, train loss 1.982711, train error 51.45,  val loss 2.030048, percent error 57.60\n",
            "Epoch     9, train loss 1.982609, train error 51.72,  val loss 2.033916, percent error 57.60\n",
            "Epoch    10, train loss 1.992705, train error 53.70,  val loss 2.021433, percent error 56.80\n",
            "Epoch    11, train loss 1.971711, train error 51.10,  val loss 2.010044, percent error 55.20\n",
            "Epoch    12, train loss 1.940434, train error 46.92,  val loss 1.988610, percent error 52.40\n",
            "Epoch    13, train loss 1.911869, train error 44.50,  val loss 1.960932, percent error 50.40\n",
            "Epoch    14, train loss 1.913900, train error 45.00,  val loss 1.961386, percent error 50.30\n",
            "Epoch    15, train loss 1.892672, train error 42.17,  val loss 1.945842, percent error 48.80\n",
            "Epoch    16, train loss 1.879334, train error 41.15,  val loss 1.931168, percent error 47.40\n",
            "Epoch    17, train loss 1.882281, train error 41.75,  val loss 1.928948, percent error 46.50\n",
            "Epoch    18, train loss 1.886144, train error 41.70,  val loss 1.923954, percent error 46.00\n",
            "Epoch    19, train loss 1.881203, train error 41.35,  val loss 1.925649, percent error 46.90\n",
            "Epoch    20, train loss 1.834970, train error 36.22,  val loss 1.882644, percent error 42.20\n",
            "Epoch    21, train loss 1.821442, train error 35.30,  val loss 1.859840, percent error 39.60\n",
            "Epoch    22, train loss 1.816762, train error 33.72,  val loss 1.858231, percent error 38.90\n",
            "Epoch    23, train loss 1.812717, train error 33.65,  val loss 1.860213, percent error 39.20\n",
            "Epoch    24, train loss 1.810722, train error 33.93,  val loss 1.860742, percent error 39.80\n",
            "Epoch    25, train loss 1.798980, train error 32.60,  val loss 1.845110, percent error 38.60\n",
            "Epoch    26, train loss 1.799878, train error 32.57,  val loss 1.851242, percent error 38.60\n",
            "Epoch    27, train loss 1.786222, train error 31.28,  val loss 1.835393, percent error 37.30\n",
            "Epoch    28, train loss 1.800607, train error 33.22,  val loss 1.846509, percent error 38.30\n",
            "Epoch    29, train loss 1.779112, train error 30.62,  val loss 1.833736, percent error 36.70\n",
            "Epoch    30, train loss 1.775218, train error 30.25,  val loss 1.822892, percent error 35.70\n",
            "Epoch    31, train loss 1.769262, train error 29.57,  val loss 1.820267, percent error 35.80\n",
            "Epoch    32, train loss 1.761937, train error 28.88,  val loss 1.815399, percent error 35.00\n",
            "Epoch    33, train loss 1.762833, train error 29.32,  val loss 1.813287, percent error 34.70\n",
            "Epoch    34, train loss 1.764749, train error 29.18,  val loss 1.811396, percent error 35.10\n",
            "Epoch    35, train loss 1.757485, train error 28.80,  val loss 1.797920, percent error 33.20\n",
            "Epoch    36, train loss 1.748714, train error 27.47,  val loss 1.795518, percent error 33.50\n",
            "Epoch    37, train loss 1.746134, train error 27.50,  val loss 1.797324, percent error 33.50\n",
            "Epoch    38, train loss 1.742580, train error 27.25,  val loss 1.794683, percent error 33.10\n",
            "Epoch    39, train loss 1.741467, train error 27.03,  val loss 1.785730, percent error 32.50\n",
            "Epoch    40, train loss 1.730092, train error 25.72,  val loss 1.779043, percent error 31.40\n",
            "Epoch    41, train loss 1.728708, train error 25.75,  val loss 1.785150, percent error 32.30\n",
            "Epoch    42, train loss 1.728367, train error 25.70,  val loss 1.780047, percent error 31.80\n",
            "Epoch    43, train loss 1.725640, train error 25.30,  val loss 1.782180, percent error 32.00\n",
            "Epoch    44, train loss 1.728881, train error 25.65,  val loss 1.781241, percent error 31.90\n",
            "Epoch    45, train loss 1.724234, train error 25.12,  val loss 1.779508, percent error 31.70\n",
            "Epoch    46, train loss 1.720052, train error 24.72,  val loss 1.778863, percent error 32.30\n",
            "Epoch    47, train loss 1.721483, train error 24.97,  val loss 1.776828, percent error 31.20\n",
            "Epoch    48, train loss 1.718197, train error 24.65,  val loss 1.776130, percent error 31.40\n",
            "Epoch    49, train loss 1.721966, train error 25.35,  val loss 1.782169, percent error 32.40\n",
            "Epoch    50, train loss 1.717584, train error 24.43,  val loss 1.773448, percent error 31.10\n",
            "Epoch    51, train loss 1.714317, train error 24.18,  val loss 1.769823, percent error 31.00\n",
            "Epoch    52, train loss 1.716288, train error 24.62,  val loss 1.777597, percent error 31.60\n",
            "Epoch    53, train loss 1.715617, train error 24.65,  val loss 1.773291, percent error 30.80\n",
            "Epoch    54, train loss 1.709538, train error 23.70,  val loss 1.768280, percent error 30.50\n",
            "Epoch    55, train loss 1.708455, train error 23.72,  val loss 1.766608, percent error 30.30\n",
            "Epoch    56, train loss 1.711179, train error 23.93,  val loss 1.767644, percent error 30.30\n",
            "Epoch    57, train loss 1.705540, train error 23.38,  val loss 1.764341, percent error 30.30\n",
            "Epoch    58, train loss 1.703344, train error 23.25,  val loss 1.763321, percent error 29.90\n",
            "Epoch    59, train loss 1.704446, train error 23.07,  val loss 1.766063, percent error 30.10\n",
            "Epoch    60, train loss 1.700995, train error 23.10,  val loss 1.763798, percent error 30.20\n",
            "Epoch    61, train loss 1.699287, train error 22.75,  val loss 1.761893, percent error 29.80\n",
            "Epoch    62, train loss 1.699368, train error 22.72,  val loss 1.763423, percent error 29.80\n",
            "Epoch    63, train loss 1.698332, train error 22.65,  val loss 1.759718, percent error 29.40\n",
            "Epoch    64, train loss 1.698957, train error 22.78,  val loss 1.762483, percent error 29.80\n",
            "Epoch    65, train loss 1.697363, train error 22.60,  val loss 1.760980, percent error 30.00\n",
            "Epoch    66, train loss 1.696694, train error 22.50,  val loss 1.760898, percent error 29.60\n",
            "Epoch    67, train loss 1.696671, train error 22.35,  val loss 1.760018, percent error 30.00\n",
            "Epoch    68, train loss 1.696553, train error 22.45,  val loss 1.761728, percent error 29.50\n",
            "Epoch    69, train loss 1.695442, train error 22.35,  val loss 1.759704, percent error 29.60\n",
            "Epoch    70, train loss 1.694698, train error 22.32,  val loss 1.760065, percent error 30.00\n",
            "Epoch    71, train loss 1.694660, train error 22.32,  val loss 1.759364, percent error 29.80\n",
            "Epoch    72, train loss 1.693765, train error 22.20,  val loss 1.759847, percent error 29.70\n",
            "Epoch    73, train loss 1.693854, train error 22.20,  val loss 1.758361, percent error 29.70\n",
            "Epoch    74, train loss 1.693142, train error 22.25,  val loss 1.758830, percent error 29.70\n",
            "Epoch    75, train loss 1.692913, train error 22.07,  val loss 1.760252, percent error 29.90\n",
            "Epoch    76, train loss 1.692105, train error 22.18,  val loss 1.759174, percent error 29.70\n",
            "Epoch    77, train loss 1.691629, train error 21.97,  val loss 1.759092, percent error 29.90\n",
            "Epoch    78, train loss 1.691380, train error 21.95,  val loss 1.757571, percent error 29.70\n",
            "Epoch    79, train loss 1.691798, train error 22.15,  val loss 1.759728, percent error 30.00\n",
            "Epoch    80, train loss 1.690179, train error 21.85,  val loss 1.758495, percent error 29.40\n",
            "Epoch    81, train loss 1.689750, train error 21.80,  val loss 1.758675, percent error 29.70\n",
            "Epoch    82, train loss 1.689438, train error 21.78,  val loss 1.758543, percent error 29.60\n",
            "Epoch    83, train loss 1.689788, train error 21.82,  val loss 1.759610, percent error 30.00\n",
            "Epoch    84, train loss 1.688963, train error 21.75,  val loss 1.758254, percent error 29.70\n",
            "Epoch    85, train loss 1.688883, train error 21.70,  val loss 1.758701, percent error 29.60\n",
            "Epoch    86, train loss 1.688569, train error 21.62,  val loss 1.758809, percent error 29.50\n",
            "Epoch    87, train loss 1.688636, train error 21.72,  val loss 1.758829, percent error 29.40\n",
            "Epoch    88, train loss 1.688506, train error 21.72,  val loss 1.758447, percent error 29.50\n",
            "Epoch    89, train loss 1.688064, train error 21.65,  val loss 1.757943, percent error 29.50\n",
            "Epoch    90, train loss 1.687652, train error 21.57,  val loss 1.758352, percent error 29.70\n",
            "Epoch    91, train loss 1.687840, train error 21.60,  val loss 1.758110, percent error 29.70\n",
            "Epoch    92, train loss 1.687376, train error 21.45,  val loss 1.758477, percent error 29.40\n",
            "Epoch    93, train loss 1.687251, train error 21.62,  val loss 1.758436, percent error 29.80\n",
            "Epoch    94, train loss 1.687130, train error 21.60,  val loss 1.759101, percent error 30.00\n",
            "Epoch    95, train loss 1.686733, train error 21.53,  val loss 1.757758, percent error 29.80\n",
            "Epoch    96, train loss 1.686605, train error 21.50,  val loss 1.757824, percent error 29.50\n",
            "Epoch    97, train loss 1.686700, train error 21.62,  val loss 1.758740, percent error 29.80\n",
            "Epoch    98, train loss 1.686425, train error 21.55,  val loss 1.758131, percent error 29.50\n",
            "Epoch    99, train loss 1.686025, train error 21.45,  val loss 1.757944, percent error 29.40\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = copy.deepcopy(model_c)\n",
        "\n",
        "# choose cross entropy loss function (equation 5.24 in the loss notes)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "# construct SGD optimizer and initialize learning rate and momentum\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.08, momentum=0.9)\n",
        "# object that decreases learning rate by half every 20 epochs\n",
        "scheduler = StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "# create 100 dummy data points and store in data loader class\n",
        "x_train_c = torch.tensor(data['x'].astype('float32'))\n",
        "y_train_c = torch.tensor(data['y'].transpose().astype('int64'))\n",
        "x_val_c= torch.tensor(data['x_test'].astype('float32'))\n",
        "y_val_c = torch.tensor(data['y_test'].astype('int64'))\n",
        "\n",
        "# load the data into a class that creates the batches\n",
        "data_loader = DataLoader(TensorDataset(x_train_c,y_train_c), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
        "\n",
        "# Initialize model weights\n",
        "model.apply(weights_init)\n",
        "\n",
        "# loop over the dataset n_epoch times\n",
        "n_epoch = 100\n",
        "\n",
        "# store the loss and the % correct at each epoch\n",
        "losses_train_c = np.zeros((n_epoch))\n",
        "errors_train_c = np.zeros((n_epoch))\n",
        "losses_val_c = np.zeros((n_epoch))\n",
        "errors_val_c = np.zeros((n_epoch))\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  # loop over batches\n",
        "  for i, batch in enumerate(data_loader):\n",
        "    # retrieve inputs and labels for this batch\n",
        "    x_batch, y_batch = batch\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass -- calculate model output\n",
        "    pred = model(x_batch[:,None,:])\n",
        "    # compute the loss\n",
        "    loss = loss_function(pred, y_batch)\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    # SGD update\n",
        "    optimizer.step()\n",
        "\n",
        "  # Run whole dataset to get statistics -- normally wouldn't do this\n",
        "  pred_train = model(x_train_c[:,None,:])\n",
        "  pred_val = model(x_val_c[:,None,:])\n",
        "  _, predicted_train_class = torch.max(pred_train.data, 1)\n",
        "  _, predicted_val_class = torch.max(pred_val.data, 1)\n",
        "  errors_train_c[epoch] = 100 - 100 * (predicted_train_class == y_train_c).float().sum() / len(y_train_c)\n",
        "  errors_val_c[epoch]= 100 - 100 * (predicted_val_class == y_val_c).float().sum() / len(y_val_c)\n",
        "  losses_train_c[epoch] = loss_function(pred_train, y_train_c).item()\n",
        "  losses_val_c[epoch]= loss_function(pred_val, y_val_c).item()\n",
        "  print(f'Epoch {epoch:5d}, train loss {losses_train_c[epoch]:.6f}, train error {errors_train_c[epoch]:3.2f},  val loss {losses_val_c[epoch]:.6f}, percent error {errors_val_c[epoch]:3.2f}')\n",
        "\n",
        "  # tell scheduler to consider updating learning rate\n",
        "  scheduler.step()\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(errors_train_c,'r-',label='train')\n",
        "ax.plot(errors_val_c,'b-',label='validation')\n",
        "ax.set_ylim(0,100); ax.set_xlim(0,n_epoch)\n",
        "ax.set_xlabel('Epoch'); ax.set_ylabel('Error')\n",
        "ax.set_title('Part I: Validation Result %3.2f'%(errors_val_c[-1]))\n",
        "ax.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning\n",
        "\n",
        "Grid search to find best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0.04, 0.00): min train loss 2.237380, min test loss 2.24\n",
            "(0.04, 0.30): min train loss 2.196926, min test loss 2.21\n",
            "(0.04, 0.60): min train loss 2.111966, min test loss 2.13\n",
            "(0.04, 0.90): min train loss 1.941229, min test loss 1.98\n",
            "(0.04, 1.00): min train loss 2.154020, min test loss 2.15\n",
            "(0.06, 0.00): min train loss 2.176808, min test loss 2.19\n",
            "(0.06, 0.30): min train loss 2.152978, min test loss 2.17\n",
            "(0.06, 0.60): min train loss 2.059059, min test loss 2.08\n",
            "(0.06, 0.90): min train loss 1.912261, min test loss 1.97\n",
            "(0.06, 1.00): min train loss 2.240170, min test loss 2.24\n",
            "(0.08, 0.00): min train loss 2.175590, min test loss 2.19\n",
            "(0.08, 0.30): min train loss 2.102998, min test loss 2.13\n",
            "(0.08, 0.60): min train loss 1.996575, min test loss 2.04\n",
            "(0.08, 0.90): min train loss 1.796723, min test loss 1.86\n",
            "(0.08, 1.00): min train loss 2.235045, min test loss 2.24\n",
            "(0.10, 0.00): min train loss 2.139351, min test loss 2.16\n",
            "(0.10, 0.30): min train loss 2.043441, min test loss 2.07\n",
            "(0.10, 0.60): min train loss 2.005662, min test loss 2.05\n",
            "(0.10, 0.90): min train loss 1.758902, min test loss 1.79\n",
            "(0.10, 1.00): min train loss 2.224495, min test loss 2.24\n",
            "(0.12, 0.00): min train loss 2.101115, min test loss 2.13\n",
            "(0.12, 0.30): min train loss 2.026269, min test loss 2.06\n",
            "(0.12, 0.60): min train loss 1.974655, min test loss 2.01\n",
            "(0.12, 0.90): min train loss 1.750602, min test loss 1.80\n",
            "(0.12, 1.00): min train loss 2.132906, min test loss 2.14\n",
            "best params: (0.12, 0.9)\n"
          ]
        }
      ],
      "source": [
        "# choose cross entropy loss function (equation 5.24 in the loss notes)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "min_train_loss = 10000\n",
        "\n",
        "for learning_rate in [0.04, 0.06, 0.08, 0.1, 0.12]:\n",
        "  for momentum in [0, 0.3, 0.6, 0.9, 1]:\n",
        "\n",
        "    model = copy.deepcopy(model_c)\n",
        "    \n",
        "    # construct SGD optimizer and initialize learning rate and momentum\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    # object that decreases learning rate by half every 20 epochs\n",
        "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "    # create 100 dummy data points and store in data loader class\n",
        "    x_train = torch.tensor(data['x'].astype('float32'))\n",
        "    y_train = torch.tensor(data['y'].transpose().astype('int64'))\n",
        "    x_val= torch.tensor(data['x_test'].astype('float32'))\n",
        "    y_val = torch.tensor(data['y_test'].astype('int64'))\n",
        "\n",
        "    # load the data into a class that creates the batches\n",
        "    data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
        "\n",
        "    # Initialize model weights\n",
        "    model.apply(weights_init)\n",
        "\n",
        "    # loop over the dataset n_epoch times\n",
        "    n_epoch = 50\n",
        "    # store the loss and the % correct at each epoch\n",
        "    losses_train = np.zeros((n_epoch))\n",
        "    errors_train = np.zeros((n_epoch))\n",
        "    losses_val = np.zeros((n_epoch))\n",
        "    errors_val = np.zeros((n_epoch))\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "      # loop over batches\n",
        "      for i, batch in enumerate(data_loader):\n",
        "        # retrieve inputs and labels for this batch\n",
        "        x_batch, y_batch = batch\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass -- calculate model output\n",
        "        pred = model(x_batch[:,None,:])\n",
        "        # compute the loss\n",
        "        loss = loss_function(pred, y_batch)\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # SGD update\n",
        "        optimizer.step()\n",
        "\n",
        "      # Run whole dataset to get statistics -- normally wouldn't do this\n",
        "      pred_train = model(x_train[:,None,:])\n",
        "      pred_val = model(x_val[:,None,:])\n",
        "      _, predicted_train_class = torch.max(pred_train.data, 1)\n",
        "      _, predicted_val_class = torch.max(pred_val.data, 1)\n",
        "      errors_train[epoch] = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)\n",
        "      errors_val[epoch]= 100 - 100 * (predicted_val_class == y_val).float().sum() / len(y_val)\n",
        "      losses_train[epoch] = loss_function(pred_train, y_train).item()\n",
        "      losses_val[epoch]= loss_function(pred_val, y_val).item()\n",
        "      # print(f'Epoch {epoch:5d}, train loss {losses_train_c[epoch]:.6f}, train error {errors_train_c[epoch]:3.2f},  val loss {losses_val_c[epoch]:.6f}, percent error {errors_val_c[epoch]:3.2f}')\n",
        "\n",
        "      # tell scheduler to consider updating learning rate\n",
        "      scheduler.step()\n",
        "\n",
        "    print(f'({learning_rate:.2f}, {momentum:.2f}): min train loss {np.min(losses_train):.6f}, min test loss {np.min(losses_val):.2f}')\n",
        "\n",
        "    if min(losses_train) < min_train_loss:\n",
        "      best_params = (learning_rate, momentum)\n",
        "      min_train_loss = np.min(losses_train )\n",
        "\n",
        "print('best params: ' + str(best_params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2\n",
        "* I implemented another basic grid search looping through learning rates and momentums to find the best training loss. I found that all learning rates and momentum values investigated resulted in similar training losses with no clear improvements.\n",
        "\n",
        "* The total number of hidden units is calculated by multiplying the dimensions of each hidden layer and summing them, which gives $(19 \\cdot 15) + (9 \\cdot 15) + (4 \\cdot 15) = 480$\n",
        "\n",
        "* The total number of parameters is calculated by summing kernel width * number of input channels * number of output channels + bias for each convolutional layer in the model and summing input dimension * output dimension + bias for the output layer, which gives $(3 \\cdot 1 \\cdot 15 + 15) + (3 \\cdot 15 \\cdot 15 + 15) + (3 \\cdot 15 \\cdot 15 + 15) + (60 \\cdot 10 + 10) = 2050$ parameters.\n",
        "\n",
        "* Throughout training, the training and test losses decrease with more epochs. The training loss decreases further but does not approach zero so we do not over-fit. Another indicator for this is that the test loss is always decreasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing architectures\n",
        "### Question 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x1f1ece6b650>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x1000 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot the results\n",
        "fig, ax = plt.subplots(2,2,figsize=(12, 10))\n",
        "ax = ax.flatten()\n",
        "\n",
        "ax[0].plot(errors_train_c, 'r-', label='train')\n",
        "ax[0].plot(errors_val_c, 'b-', label='test')\n",
        "ax[0].set_ylim(0, 100)\n",
        "ax[0].set_xlim(0, 100)\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Error')\n",
        "ax[0].set_title(f'Convolutional')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(errors_train_f, 'r-', label='train')\n",
        "ax[1].plot(errors_test_f, 'b-', label='test')\n",
        "ax[1].set_ylim(0, 100)\n",
        "ax[1].set_xlim(0, 100)\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Error')\n",
        "ax[1].set_title(f'Fully-connected')\n",
        "ax[1].legend()\n",
        "\n",
        "ax[2].plot(losses_train_c, 'r-', label='train')\n",
        "ax[2].plot(losses_val_c, 'b-', label='test')\n",
        "ax[2].set_ylim(0, 3)\n",
        "ax[2].set_xlim(0, 100)\n",
        "ax[2].set_xlabel('Epoch')\n",
        "ax[2].set_ylabel('Loss')\n",
        "ax[2].set_title(f'Convolutional')\n",
        "ax[2].legend()\n",
        "\n",
        "ax[3].plot(losses_train_f, 'r-', label='train')\n",
        "ax[3].plot(losses_test_f, 'b-', label='test')\n",
        "ax[3].set_ylim(0, 3)\n",
        "ax[3].set_xlim(0, 100)\n",
        "ax[3].set_xlabel('Epoch')\n",
        "ax[3].set_ylabel('Loss')\n",
        "ax[3].set_title(f'Fully-connected')\n",
        "ax[3].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main differences between errors:\n",
        "* Fully-connected model converges faster than convolutional model because the weights are independent and don't need to be shared across dimensions.\n",
        "* Training error approaches zero for the fully-connected model, indicating overfitting, but not the convolutional model.\n",
        "* Test error approaches a lower threshold for the convolutional model than the fully-connected model, indicating a better performance on an unseen dataset. \n",
        "\n",
        "Main differences between losses:\n",
        "* Training and test losses for convolutional model both decrease whereas only the training loss decreases for the fully-connected model. The test loss initially decreases to a minima before increasing to a threshold again, indicating overfitting.\n",
        "* Training loss does not approach zero for the convolutional model because we output a probability distribution instead of a definitive label. This is also the reason why the losses for the convolutional network are higher than the fully-connected network.\n",
        "\n",
        "The convolutional neural network works better and it had lower test error. In general, CNNs have less parameters than a fully-connected neural network, which make them less likely to overfit to training data. Convolutional neural networks are better suited for dealing with images due to the convolutional layers. They also keep translational invariance and equivariance."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
